[
  {
    "slug": "agentic-software-development-a-custom-incident-handling-agent",
    "title": "Getting Started with Agentic Software Development: A Custom Incident Handling Agent",
    "excerpt": "Learn how to build a custom incident handling agent using LLMs and LangChain. This post introduces the principles of agentic software development and walks through a real-world use case of automating incident response with memory, log search, ticketing, and remediation.",
    "content": "Agentic software development is redefining how we build applications by leveraging **autonomous agents**â€”self-directed programs powered by large language models (LLMs) that can reason, plan, and act based on context.\n\nIn this blog, we'll walk through building a **custom incident handling agent**, a real-world example that showcases the power of agentic systems to monitor, diagnose, and react to incidents in production environments.\n\n---\n\n## ðŸ¤– What is Agentic Software Development?\n\nAgentic software treats LLMs not just as passive tools (e.g., summarizers), but as active **decision-making components**. These agents:\n\n- Perceive their environment (through tools like APIs)\n- Maintain memory and context\n- Use reasoning chains (e.g., ReAct or Chain-of-Thought)\n- Take actions autonomously (e.g., trigger alerts, write to databases, create Jira tickets)\n\n---\n\n## ðŸ§  Use Case: Custom Incident Handling Agent\n\n### ðŸŽ¯ Problem\nDevOps teams often face alert fatigue. A typical on-call engineer receive",
    "tags": [
      "Agentic Software",
      "LLM Agents",
      "Incident Management",
      "LangChain",
      "OpenAI",
      "Autonomous Agents"
    ],
    "author": "Abstract Algorithms",
    "date": "2025-06-24",
    "readingTime": "5 min read"
  },
  {
    "slug": "intro-to-langchain-and-langgraph",
    "title": "Introduction to LangChain and LangGraph: Building Agentic Workflows",
    "excerpt": "A practical introduction to LangChain and LangGraph, the leading frameworks for building agentic LLM-powered applications.",
    "content": "This post introduces LangChain and LangGraph, two powerful frameworks for building agentic software with LLMs.\n\n## What is LangChain?\n- An open-source framework for developing applications powered by LLMs and tools.\n- Provides abstractions for agents, tools, memory, and chains.\n\n## What is LangGraph?\n- A framework for building multi-agent and graph-based agentic workflows.\n- Enables complex, stateful, and collaborative agent systems.\n\n## Key Features\n- Tool integration, memory management, agent orchestration, graph workflows.\n- Extensible and production-ready.\n\n## Example Use Cases\n- Autonomous research agents\n- Workflow automation\n- Multi-agent chat systems\n\n---\n\n*See our other posts for foundational concepts and multi-agent patterns.*\n\n",
    "tags": [
      "LangChain",
      "LangGraph",
      "LLM Agents",
      "Workflows"
    ],
    "author": "Abstract Algorithms",
    "date": "2025-06-22",
    "readingTime": "5 min read"
  },
  {
    "slug": "intro-to-llms-and-agents",
    "title": "Introduction to LLMs and Agents: Foundations for Agentic Software",
    "excerpt": "A beginner-friendly introduction to Large Language Models (LLMs) and the concept of agents, setting the stage for agentic software development.",
    "content": "This post introduces the core concepts of Large Language Models (LLMs) and software agents, which are foundational to building agentic systems.\n\n## What is an LLM?\n- A Large Language Model (LLM) is a neural network trained on vast text data to generate and understand human language.\n- Examples: OpenAI GPT-4, Anthropic Claude, Google Gemini, Meta Llama.\n\n## What is an Agent?\n- An agent is a program that can perceive its environment, reason, and take actions to achieve goals.\n- In AI, agents can be rule-based, learning-based, or powered by LLMs.\n\n## Why Combine LLMs and Agents?\n- LLMs provide reasoning, context, and language understanding.\n- Agents provide autonomy, memory, and tool use.\n\n## Key Concepts\n- Perception, memory, planning, action, feedback loops.\n- Prompt engineering and tool integration.\n\n---\n\n*Next: See our post on multi-agent systems for more advanced agentic architectures.*\n\n",
    "tags": [
      "LLM",
      "Agents",
      "AI",
      "Foundations"
    ],
    "author": "Abstract Algorithms",
    "date": "2025-06-20",
    "readingTime": "5 min read"
  },
  {
    "slug": "little's-law",
    "title": "Little's Law: Understanding Queue Performance in Distributed Systems",
    "excerpt": "Master Little's Law to optimize system performance, predict throughput, and design scalable distributed systems with practical queuing theory.",
    "content": "# Little's Law: The Foundation of Queueing and System Performance\r\n\r\n## Introduction\r\n\r\nLittle's Law is a fundamental principle in queueing theory and system performance analysis. It provides a simple yet powerful relationship that governs how items flow through any stable systemâ€”whether it's customers in a bakery, requests in a web server, or tasks in a distributed pipeline.\r\n\r\nThis article will help you:\r\n- Understand the intuition and math behind Little's Law\r\n- Apply it to real-world engineering scenarios\r\n- Use it for capacity planning, performance optimization, and system design\r\n\r\n---\r\n\r\n## What is Little's Law?\r\n\r\nLittle's Law describes the relationship between:\r\n- **L**: Average number of items in the system (queue length)\r\n- **Î»**: Average arrival rate (items per unit time)\r\n- **W**: Average time an item spends in the system (wait + service)\r\n\r\nThe formula is:\r\n\r\n```\r\nL = Î» Ã— W\r\n```\r\n\r\nThis means: **The average number of items in a stable system equals the arrival rate times ",
    "tags": [
      "queueing-theory",
      "performance",
      "system-design",
      "mathematics",
      "distributed-systems",
      "scalability"
    ],
    "author": "Abstract Algorithms",
    "date": "2024-03-05",
    "readingTime": "5 min read"
  },
  {
    "slug": "llm-engineering-mastery-part-1",
    "title": "LLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models",
    "excerpt": "Part 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.",
    "content": "# LLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models\r\n\r\nWelcome to the **LLM Engineering Mastery** series! This focused 3-part series is designed for engineers who want to master Large Language Models from a practical, implementation-oriented perspective.\r\n\r\n## Series Overview\r\n\r\nThis series focuses on the **engineering perspective** of working with LLMs, emphasizing practical usage, integration, and optimization rather than theoretical underpinnings.\r\n\r\n### What We'll Cover in This 3-Part Series\r\n\r\n1. **Part 1: Understanding and Leveraging Foundation Models** (This part)\r\n   - Foundation model ecosystem and selection\r\n   - API integration patterns and best practices\r\n   - Performance optimization and cost management\r\n   - Understanding model capabilities and limitations\r\n\r\n2. **Part 2: Advanced Prompt Engineering and RAG Systems**\r\n   - Advanced prompting techniques and optimization\r\n   - Building production-ready RAG systems\r\n   - Context management and ",
    "tags": [
      "llm",
      "genai",
      "engineering",
      "foundation-models",
      "practical-ai"
    ],
    "author": "Abstract Algorithms",
    "date": "2024-01-27",
    "readingTime": "5 min read"
  },
  {
    "slug": "llm-engineering-mastery-part-2",
    "title": "LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems",
    "excerpt": "Part 2 of the LLM Engineering Mastery series: Master advanced prompt engineering techniques and build production-ready RAG systems for enhanced LLM applications.",
    "content": "# LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems\r\n\r\nBuilding on the foundation model integration from Part 1, we now dive deep into advanced prompt engineering techniques and Retrieval-Augmented Generation (RAG) systems that can dramatically enhance your LLM applications' capabilities and reliability.\r\n\r\n## Advanced Prompt Engineering Techniques\r\n\r\n### 1. Few-Shot Learning Patterns\r\n\r\nFew-shot prompting provides examples to guide the model's behavior and output format.\r\n\r\n```python\r\nclass FewShotPromptBuilder:\r\n    def __init__(self):\r\n        self.examples = {}\r\n    \r\n    def add_example(self, category: str, input_text: str, output_text: str):\r\n        \"\"\"Add an example for few-shot learning\"\"\"\r\n        if category not in self.examples:\r\n            self.examples[category] = []\r\n        \r\n        self.examples[category].append({\r\n            \"input\": input_text,\r\n            \"output\": output_text\r\n        })\r\n    \r\n    def build_prompt(self, category: st",
    "tags": [
      "llm",
      "prompt-engineering",
      "rag",
      "vector-databases",
      "retrieval"
    ],
    "author": "Abstract Algorithms",
    "date": "2024-02-03",
    "readingTime": "5 min read"
  },
  {
    "slug": "llm-engineering-mastery-part-3",
    "title": "LLM Engineering Mastery: Part 3 - Production Deployment and Scaling",
    "excerpt": "Part 3 of the LLM Engineering Mastery series: Master production deployment, scaling strategies, monitoring, and security for enterprise-grade LLM applications.",
    "content": "# LLM Engineering Mastery: Part 3 - Production Deployment and Scaling\r\n\r\nIn this final part of the LLM Engineering Mastery series, we'll cover everything you need to deploy, scale, and maintain LLM applications in production environments. From infrastructure patterns to monitoring and security, this guide provides the practical knowledge needed for enterprise-grade deployments.\r\n\r\n## Infrastructure Patterns for LLM Applications\r\n\r\n### 1. Microservices Architecture for LLM Systems\r\n\r\n```python\r\nfrom fastapi import FastAPI, HTTPException, Depends\r\nfrom pydantic import BaseModel\r\nfrom typing import List, Optional\r\nimport asyncio\r\nimport httpx\r\nfrom datetime import datetime\r\nimport logging\r\n\r\n# Data models\r\nclass ChatRequest(BaseModel):\r\n    messages: List[dict]\r\n    model: str = \"gpt-3.5-turbo\"\r\n    temperature: float = 0.7\r\n    max_tokens: int = 1000\r\n\r\nclass RAGRequest(BaseModel):\r\n    query: str\r\n    collection: str = \"default\"\r\n    top_k: int = 5\r\n\r\nclass ChatResponse(BaseModel):\r\n   ",
    "tags": [
      "llm",
      "production",
      "deployment",
      "scaling",
      "monitoring",
      "security"
    ],
    "author": "Abstract Algorithms",
    "date": "2024-02-10",
    "readingTime": "5 min read"
  },
  {
    "slug": "llm-engineering-mastery-series",
    "title": "LLM Engineering Mastery - Complete Series",
    "excerpt": "Complete LLM Engineering Mastery series with 3 parts covering Part 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.",
    "content": "# LLM Engineering Mastery\n\nPart 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.\n\n## Series Overview\n\nThis comprehensive 3-part series covers:\n\n### 1. LLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models\n\nPart 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.\n\n[Read Part 1 â†’](/posts/llm-engineering-mastery-part-1/)\n\n### 2. LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems\n\nPart 2 of the LLM Engineering Mastery series: Master advanced prompt engineering techniques and build production-ready RAG systems for enhanced LLM applications.\n\n[Read Part 2 â†’](/posts/llm-engineering-mastery-part-2/)\n\n### 3. LLM Engineering Mastery: Part 3 - Production Deployment and Scaling\n\nPart 3 of the",
    "tags": [
      "llm",
      "genai",
      "engineering",
      "foundation-models",
      "practical-ai"
    ],
    "author": "Abstract Algorithms",
    "date": "2024-01-27",
    "readingTime": "5 min read"
  },
  {
    "slug": "multi-agent-systems-in-practice",
    "title": "Multi-Agent Systems: Collaboration and Coordination in Agentic Software",
    "excerpt": "Explore how multiple agents can collaborate, communicate, and coordinate to solve complex problems in agentic software.",
    "content": "This post explores the principles and patterns of multi-agent systems, where multiple agents work together to achieve shared or distributed goals.\n\n## What is a Multi-Agent System?\n- A system with two or more agents that interact, cooperate, or compete.\n- Used in distributed AI, robotics, simulations, and modern LLM-powered applications.\n\n## Key Concepts\n- Communication protocols (messages, signals)\n- Coordination strategies (leader election, consensus)\n- Collaboration vs. competition\n\n## Example Use Cases\n- Automated trading bots\n- Distributed monitoring and alerting\n- Multi-agent chat assistants\n\n---\n\n*Next: Learn about LangChain and LangGraph for building agentic workflows.*\n\n",
    "tags": [
      "Multi-Agent",
      "Agents",
      "Collaboration",
      "Coordination"
    ],
    "author": "Abstract Algorithms",
    "date": "2025-06-21",
    "readingTime": "5 min read"
  },
  {
    "slug": "understanding-hash-tables-ultimate-guide",
    "title": "Understanding Hash Tables: The Ultimate Guide",
    "excerpt": "A comprehensive guide to hash tables, covering implementation details, collision resolution strategies, and performance analysis with practical examples.",
    "content": "Hash tables are one of the most fundamental and powerful data structures in computer science, offering average-case O(1) time complexity for basic operations. This comprehensive guide explores hash tables from the ground up.\r\n\r\n## What Are Hash Tables?\r\n\r\nA hash table (also known as a hash map) is a data structure that implements an associative array abstract data type, mapping keys to values. It uses a hash function to compute an index into an array of buckets or slots.\r\n\r\n![Hash Table Overview](/posts/understanding-hash-tables-ultimate-guide/assets/overview.png)\r\n\r\n### Key Components\r\n\r\n1. **Hash Function**: Converts keys into array indices\r\n2. **Buckets**: Array slots that store key-value pairs\r\n3. **Collision Resolution**: Strategy for handling multiple keys mapping to the same index\r\n\r\n![Hash Table Anatomy](/posts/understanding-hash-tables-ultimate-guide/assets/anatomy.png)\r\n\r\n## Hash Functions\r\n\r\nA good hash function should:\r\n- Be deterministic\r\n- Distribute keys uniformly\r\n- Be ",
    "tags": [
      "data-structures",
      "algorithms",
      "hash-tables",
      "performance"
    ],
    "author": "Abstract Algorithms",
    "date": "2024-01-15",
    "readingTime": "5 min read"
  }
]