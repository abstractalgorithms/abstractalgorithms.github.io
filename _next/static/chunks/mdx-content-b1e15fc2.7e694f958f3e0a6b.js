"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[9969],{3449:function(n,e,r){r.r(e),r.d(e,{default:function(){return o},frontmatter:function(){return a}});var t=r(7437),s=r(4229);let a=void 0;function i(n){let e={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{children:"Index Performance Monitoring Fundamentals"}),"\n",(0,t.jsx)(e.h3,{children:"Key Performance Metrics"}),"\n",(0,t.jsx)(e.h4,{children:"Query Performance Indicators"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Query Execution Time"}),": End-to-end query duration"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Index Seek vs Index Scan"}),": Seek is targeted, scan reads entire index"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Key Lookups"}),": Additional operations to fetch non-indexed columns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sort Operations"}),": Whether sorting uses indexes or requires explicit sorting"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Buffer Cache Hit Ratio"}),": Percentage of index pages found in memory"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{children:"Index Health Metrics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Index Fragmentation"}),": Physical disorder of index pages"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Index Usage Statistics"}),": How frequently indexes are accessed"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Index Size Growth"}),": Storage consumption over time"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Write Performance Impact"}),": Effect on INSERT/UPDATE/DELETE operations"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Database-Specific Monitoring Tools"}),"\n",(0,t.jsx)(e.h4,{children:"MySQL Performance Monitoring"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Enable Performance Schema\r\nSET GLOBAL performance_schema = ON;\r\n\r\n-- Monitor index usage\r\nSELECT \r\n    object_schema,\r\n    object_name,\r\n    index_name,\r\n    count_read,\r\n    count_write,\r\n    sum_timer_read/1000000000 AS read_time_seconds,\r\n    sum_timer_write/1000000000 AS write_time_seconds\r\nFROM performance_schema.table_io_waits_summary_by_index_usage\r\nWHERE object_schema = 'your_database'\r\nORDER BY count_read DESC;\r\n\r\n-- Check slow queries using indexes\r\nSELECT \r\n    digest_text,\r\n    count_star,\r\n    avg_timer_wait/1000000000 AS avg_time_seconds,\r\n    sum_rows_examined,\r\n    sum_rows_sent\r\nFROM performance_schema.events_statements_summary_by_digest\r\nWHERE digest_text LIKE '%your_table%'\r\nORDER BY avg_timer_wait DESC;\r\n\r\n-- Index effectiveness analysis\r\nSELECT \r\n    TABLE_SCHEMA,\r\n    TABLE_NAME,\r\n    INDEX_NAME,\r\n    CARDINALITY,\r\n    CARDINALITY / (SELECT table_rows FROM information_schema.tables \r\n                   WHERE table_schema = s.TABLE_SCHEMA \r\n                   AND table_name = s.TABLE_NAME) AS selectivity\r\nFROM information_schema.statistics s\r\nWHERE TABLE_SCHEMA = 'your_database'\r\nORDER BY selectivity DESC;\n"})}),"\n",(0,t.jsx)(e.h4,{children:"PostgreSQL Monitoring"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Index usage statistics\r\nSELECT \r\n    schemaname,\r\n    tablename,\r\n    indexname,\r\n    idx_scan,\r\n    idx_tup_read,\r\n    idx_tup_fetch,\r\n    idx_scan::float / GREATEST(seq_scan + idx_scan, 1) AS index_usage_ratio\r\nFROM pg_stat_user_indexes\r\nWHERE schemaname = 'public'\r\nORDER BY idx_scan DESC;\r\n\r\n-- Unused indexes (potential candidates for removal)\r\nSELECT \r\n    schemaname,\r\n    tablename,\r\n    indexname,\r\n    pg_size_pretty(pg_relation_size(indexrelid)) AS size\r\nFROM pg_stat_user_indexes\r\nWHERE idx_scan = 0 \r\n  AND schemaname = 'public'\r\nORDER BY pg_relation_size(indexrelid) DESC;\r\n\r\n-- Index bloat analysis\r\nSELECT \r\n    tablename,\r\n    indexname,\r\n    pg_size_pretty(pg_relation_size(indexrelid)) AS size,\r\n    CASE \r\n        WHEN indisunique THEN 'UNIQUE'\r\n        ELSE 'NON-UNIQUE'\r\n    END AS index_type,\r\n    n_tup_ins + n_tup_upd + n_tup_del AS total_writes,\r\n    idx_scan AS total_scans\r\nFROM pg_stat_user_indexes \r\nJOIN pg_stat_user_tables USING (schemaname, tablename)\r\nJOIN pg_index ON indexrelid = pg_index.indexrelid\r\nWHERE schemaname = 'public'\r\nORDER BY pg_relation_size(indexrelid) DESC;\r\n\r\n-- Buffer cache hit ratio for indexes\r\nSELECT \r\n    schemaname,\r\n    tablename,\r\n    indexname,\r\n    heap_blks_read,\r\n    heap_blks_hit,\r\n    CASE \r\n        WHEN heap_blks_hit + heap_blks_read = 0 THEN NULL\r\n        ELSE heap_blks_hit::float / (heap_blks_hit + heap_blks_read)\r\n    END AS hit_ratio\r\nFROM pg_statio_user_indexes\r\nWHERE schemaname = 'public'\r\nORDER BY hit_ratio ASC NULLS LAST;\n"})}),"\n",(0,t.jsx)(e.h4,{children:"SQL Server Monitoring"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Index usage statistics\r\nSELECT \r\n    OBJECT_NAME(i.object_id) AS table_name,\r\n    i.name AS index_name,\r\n    i.type_desc AS index_type,\r\n    dm_ius.user_seeks,\r\n    dm_ius.user_scans,\r\n    dm_ius.user_lookups,\r\n    dm_ius.user_updates,\r\n    dm_ius.user_seeks + dm_ius.user_scans + dm_ius.user_lookups AS total_reads,\r\n    CASE \r\n        WHEN dm_ius.user_updates > 0 \r\n        THEN (dm_ius.user_seeks + dm_ius.user_scans + dm_ius.user_lookups) / dm_ius.user_updates \r\n        ELSE NULL \r\n    END AS read_write_ratio\r\nFROM sys.indexes i\r\nLEFT JOIN sys.dm_db_index_usage_stats dm_ius \r\n    ON i.object_id = dm_ius.object_id AND i.index_id = dm_ius.index_id\r\nWHERE OBJECTPROPERTY(i.object_id, 'IsUserTable') = 1\r\nORDER BY total_reads DESC;\r\n\r\n-- Index fragmentation analysis\r\nSELECT \r\n    OBJECT_NAME(i.object_id) AS table_name,\r\n    i.name AS index_name,\r\n    s.avg_fragmentation_in_percent,\r\n    s.fragment_count,\r\n    s.page_count,\r\n    CASE \r\n        WHEN s.avg_fragmentation_in_percent < 5 THEN 'No action needed'\r\n        WHEN s.avg_fragmentation_in_percent < 30 THEN 'Reorganize'\r\n        ELSE 'Rebuild'\r\n    END AS recommended_action\r\nFROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'DETAILED') s\r\nJOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id\r\nWHERE s.page_count > 100  -- Only consider indexes with significant pages\r\nORDER BY s.avg_fragmentation_in_percent DESC;\r\n\r\n-- Missing index suggestions\r\nSELECT \r\n    mid.statement AS table_name,\r\n    mids.equality_columns,\r\n    mids.inequality_columns,\r\n    mids.included_columns,\r\n    migs.user_seeks,\r\n    migs.user_scans,\r\n    migs.avg_total_user_cost,\r\n    migs.avg_user_impact,\r\n    'CREATE INDEX idx_' + REPLACE(REPLACE(REPLACE(mid.statement, '[', ''), ']', ''), '.', '_') + \r\n    '_suggested ON ' + mid.statement + \r\n    ' (' + ISNULL(mids.equality_columns, '') + \r\n    CASE WHEN mids.inequality_columns IS NOT NULL \r\n         THEN CASE WHEN mids.equality_columns IS NOT NULL THEN ',' ELSE '' END + mids.inequality_columns \r\n         ELSE '' END + ')' +\r\n    CASE WHEN mids.included_columns IS NOT NULL \r\n         THEN ' INCLUDE (' + mids.included_columns + ')' \r\n         ELSE '' END AS create_statement\r\nFROM sys.dm_db_missing_index_groups mig\r\nJOIN sys.dm_db_missing_index_group_stats migs ON mig.index_group_handle = migs.group_handle\r\nJOIN sys.dm_db_missing_index_details mid ON mig.index_handle = mid.index_handle\r\nWHERE migs.avg_user_impact > 50  -- High impact suggestions only\r\nORDER BY migs.avg_user_impact DESC;\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Automated Index Maintenance"}),"\n",(0,t.jsx)(e.h3,{children:"SQL Server Automated Maintenance"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create maintenance plan for index optimization\r\n-- This script reorganizes or rebuilds indexes based on fragmentation level\r\n\r\nDECLARE @DatabaseName NVARCHAR(50) = 'YourDatabase'\r\nDECLARE @FragmentationThreshold FLOAT = 5.0\r\nDECLARE @RebuildThreshold FLOAT = 30.0\r\n\r\n-- Cursor to iterate through fragmented indexes\r\nDECLARE index_cursor CURSOR FOR\r\nSELECT \r\n    OBJECT_NAME(i.object_id) AS table_name,\r\n    i.name AS index_name,\r\n    s.avg_fragmentation_in_percent,\r\n    CASE \r\n        WHEN s.avg_fragmentation_in_percent >= @RebuildThreshold THEN 'REBUILD'\r\n        WHEN s.avg_fragmentation_in_percent >= @FragmentationThreshold THEN 'REORGANIZE'\r\n        ELSE 'NO_ACTION'\r\n    END AS action_type\r\nFROM sys.dm_db_index_physical_stats(DB_ID(@DatabaseName), NULL, NULL, NULL, 'DETAILED') s\r\nJOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id\r\nWHERE s.avg_fragmentation_in_percent >= @FragmentationThreshold\r\n  AND s.page_count > 100;\r\n\r\nDECLARE @TableName NVARCHAR(128), @IndexName NVARCHAR(128), @Fragmentation FLOAT, @Action NVARCHAR(20)\r\nDECLARE @SQL NVARCHAR(500)\r\n\r\nOPEN index_cursor\r\nFETCH NEXT FROM index_cursor INTO @TableName, @IndexName, @Fragmentation, @Action\r\n\r\nWHILE @@FETCH_STATUS = 0\r\nBEGIN\r\n    IF @Action = 'REBUILD'\r\n    BEGIN\r\n        SET @SQL = 'ALTER INDEX ' + @IndexName + ' ON ' + @TableName + ' REBUILD WITH (ONLINE = ON)'\r\n        PRINT 'Rebuilding: ' + @IndexName + ' (Fragmentation: ' + CAST(@Fragmentation AS VARCHAR(10)) + '%)'\r\n    END\r\n    ELSE IF @Action = 'REORGANIZE'\r\n    BEGIN\r\n        SET @SQL = 'ALTER INDEX ' + @IndexName + ' ON ' + @TableName + ' REORGANIZE'\r\n        PRINT 'Reorganizing: ' + @IndexName + ' (Fragmentation: ' + CAST(@Fragmentation AS VARCHAR(10)) + '%)'\r\n    END\r\n    \r\n    IF @SQL IS NOT NULL\r\n    BEGIN\r\n        EXEC sp_executesql @SQL\r\n        SET @SQL = NULL\r\n    END\r\n    \r\n    FETCH NEXT FROM index_cursor INTO @TableName, @IndexName, @Fragmentation, @Action\r\nEND\r\n\r\nCLOSE index_cursor\r\nDEALLOCATE index_cursor\n"})}),"\n",(0,t.jsx)(e.h3,{children:"PostgreSQL Automated Maintenance"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"#!/bin/bash\r\n# PostgreSQL index maintenance script\r\n\r\nDATABASE=\"your_database\"\r\nREINDEX_THRESHOLD=0.2  # 20% bloat threshold\r\n\r\n# Function to check index bloat and reindex if necessary\r\ncheck_and_reindex() {\r\n    psql -d $DATABASE -c \"\r\n    DO \\$\\$\r\n    DECLARE\r\n        idx_record RECORD;\r\n        bloat_query TEXT := '\r\n            SELECT \r\n                schemaname,\r\n                tablename,\r\n                indexname,\r\n                CASE \r\n                    WHEN pg_relation_size(indexrelid) = 0 THEN 0\r\n                    ELSE (pg_relation_size(indexrelid)::float / \r\n                          GREATEST(pg_relation_size(c.oid), 1))\r\n                END AS bloat_ratio\r\n            FROM pg_stat_user_indexes ui\r\n            JOIN pg_class c ON c.relname = ui.tablename\r\n            WHERE schemaname = ''public''\r\n            AND pg_relation_size(indexrelid) > 1000000';  -- Only large indexes\r\n    BEGIN\r\n        FOR idx_record IN EXECUTE bloat_query LOOP\r\n            IF idx_record.bloat_ratio > $REINDEX_THRESHOLD THEN\r\n                RAISE NOTICE 'Reindexing %.% (bloat ratio: %)', \r\n                    idx_record.indexname, idx_record.tablename, idx_record.bloat_ratio;\r\n                EXECUTE 'REINDEX INDEX CONCURRENTLY ' || idx_record.indexname;\r\n            END IF;\r\n        END LOOP;\r\n    END\r\n    \\$\\$;\r\n    \"\r\n}\r\n\r\n# Update table statistics\r\npsql -d $DATABASE -c \"\r\nSELECT 'ANALYZE ' || schemaname || '.' || tablename || ';'\r\nFROM pg_stat_user_tables \r\nWHERE schemaname = 'public'\r\n\" | grep ANALYZE | psql -d $DATABASE\r\n\r\n# Check and reindex bloated indexes\r\ncheck_and_reindex\r\n\r\necho \"Index maintenance completed\"\n"})}),"\n",(0,t.jsx)(e.h3,{children:"MySQL Automated Maintenance"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- MySQL maintenance procedure\r\nDELIMITER //\r\nCREATE PROCEDURE OptimizeIndexes()\r\nBEGIN\r\n    DECLARE done INT DEFAULT FALSE;\r\n    DECLARE table_name VARCHAR(255);\r\n    DECLARE table_cursor CURSOR FOR \r\n        SELECT TABLE_NAME \r\n        FROM information_schema.TABLES \r\n        WHERE TABLE_SCHEMA = DATABASE() \r\n        AND TABLE_TYPE = 'BASE TABLE';\r\n    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;\r\n\r\n    OPEN table_cursor;\r\n    \r\n    table_loop: LOOP\r\n        FETCH table_cursor INTO table_name;\r\n        IF done THEN\r\n            LEAVE table_loop;\r\n        END IF;\r\n        \r\n        -- Optimize table (rebuilds indexes)\r\n        SET @sql = CONCAT('OPTIMIZE TABLE ', table_name);\r\n        PREPARE stmt FROM @sql;\r\n        EXECUTE stmt;\r\n        DEALLOCATE PREPARE stmt;\r\n        \r\n        -- Analyze table (updates statistics)\r\n        SET @sql = CONCAT('ANALYZE TABLE ', table_name);\r\n        PREPARE stmt FROM @sql;\r\n        EXECUTE stmt;\r\n        DEALLOCATE PREPARE stmt;\r\n        \r\n    END LOOP;\r\n    \r\n    CLOSE table_cursor;\r\nEND //\r\nDELIMITER ;\r\n\r\n-- Schedule the procedure to run weekly\r\n-- CREATE EVENT weekly_index_maintenance\r\n-- ON SCHEDULE EVERY 1 WEEK\r\n-- STARTS '2024-01-01 02:00:00'\r\n-- DO CALL OptimizeIndexes();\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Real-Time Performance Monitoring"}),"\n",(0,t.jsx)(e.h3,{children:"Setting Up Monitoring Dashboards"}),"\n",(0,t.jsx)(e.h4,{children:"PostgreSQL with pg_stat_statements"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Enable pg_stat_statements extension\r\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\r\n\r\n-- Configure postgresql.conf\r\n-- shared_preload_libraries = 'pg_stat_statements'\r\n-- pg_stat_statements.track = all\r\n\r\n-- Query to identify slow queries using indexes\r\nSELECT \r\n    query,\r\n    calls,\r\n    total_time / calls AS avg_time,\r\n    rows / calls AS avg_rows,\r\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\r\nFROM pg_stat_statements \r\nWHERE query LIKE '%your_table%'\r\nORDER BY total_time DESC\r\nLIMIT 10;\r\n\r\n-- Reset statistics\r\nSELECT pg_stat_statements_reset();\n"})}),"\n",(0,t.jsx)(e.h4,{children:"MySQL Performance Schema Monitoring"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Monitor index usage patterns\r\nSELECT \r\n    object_schema,\r\n    object_name,\r\n    index_name,\r\n    count_read,\r\n    count_write,\r\n    sum_timer_read / count_read / 1000000000 AS avg_read_time_seconds,\r\n    sum_timer_write / count_write / 1000000000 AS avg_write_time_seconds\r\nFROM performance_schema.table_io_waits_summary_by_index_usage\r\nWHERE count_read > 0 OR count_write > 0\r\nORDER BY count_read DESC;\r\n\r\n-- Monitor statement performance\r\nSELECT \r\n    DIGEST_TEXT,\r\n    COUNT_STAR,\r\n    AVG_TIMER_WAIT / 1000000000 AS avg_time_seconds,\r\n    SUM_ROWS_EXAMINED / COUNT_STAR AS avg_rows_examined,\r\n    SUM_ROWS_SENT / COUNT_STAR AS avg_rows_sent\r\nFROM performance_schema.events_statements_summary_by_digest\r\nWHERE DIGEST_TEXT IS NOT NULL\r\nORDER BY AVG_TIMER_WAIT DESC\r\nLIMIT 10;\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Application-Level Monitoring"}),"\n",(0,t.jsx)(e.h4,{children:"Python Application Monitoring"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import time\r\nimport logging\r\nfrom functools import wraps\r\n\r\n# Query performance decorator\r\ndef monitor_query_performance(query_name):\r\n    def decorator(func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            start_time = time.time()\r\n            result = func(*args, **kwargs)\r\n            end_time = time.time()\r\n            \r\n            execution_time = end_time - start_time\r\n            \r\n            # Log slow queries\r\n            if execution_time > 1.0:  # Queries taking more than 1 second\r\n                logging.warning(f"Slow query detected: {query_name} took {execution_time:.2f} seconds")\r\n            \r\n            # Store metrics for analysis\r\n            store_performance_metric(query_name, execution_time, len(result) if result else 0)\r\n            \r\n            return result\r\n        return wrapper\r\n    return decorator\r\n\r\n# Usage example\r\n@monitor_query_performance("get_user_orders")\r\ndef get_user_orders(user_id, start_date, end_date):\r\n    query = """\r\n    SELECT * FROM orders \r\n    WHERE customer_id = %s \r\n      AND order_date BETWEEN %s AND %s\r\n    """\r\n    # Execute query and return results\r\n    return execute_query(query, (user_id, start_date, end_date))\r\n\r\n# Performance metrics storage\r\ndef store_performance_metric(query_name, execution_time, result_count):\r\n    # Store in time-series database, logging system, or monitoring service\r\n    metrics = {\r\n        \'query_name\': query_name,\r\n        \'execution_time\': execution_time,\r\n        \'result_count\': result_count,\r\n        \'timestamp\': time.time()\r\n    }\r\n    # Send to monitoring system (e.g., Prometheus, DataDog, etc.)\n'})}),"\n",(0,t.jsx)(e.h4,{children:"Node.js Application Monitoring"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"const queryMonitor = (queryName) => {\r\n    return (target, propertyName, descriptor) => {\r\n        const method = descriptor.value;\r\n        \r\n        descriptor.value = async function(...args) {\r\n            const startTime = Date.now();\r\n            \r\n            try {\r\n                const result = await method.apply(this, args);\r\n                const endTime = Date.now();\r\n                const duration = endTime - startTime;\r\n                \r\n                // Log performance metrics\r\n                console.log(`Query ${queryName}: ${duration}ms`);\r\n                \r\n                // Send to monitoring service\r\n                if (duration > 1000) {\r\n                    console.warn(`Slow query detected: ${queryName} took ${duration}ms`);\r\n                }\r\n                \r\n                return result;\r\n            } catch (error) {\r\n                console.error(`Query ${queryName} failed:`, error);\r\n                throw error;\r\n            }\r\n        };\r\n    };\r\n};\r\n\r\n// Usage\r\nclass OrderService {\r\n    @queryMonitor('getUserOrders')\r\n    async getUserOrders(userId, startDate, endDate) {\r\n        const query = `\r\n            SELECT * FROM orders \r\n            WHERE customer_id = ? \r\n              AND order_date BETWEEN ? AND ?\r\n        `;\r\n        return await this.db.query(query, [userId, startDate, endDate]);\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Index Performance Alerts"}),"\n",(0,t.jsx)(e.h3,{children:"Setting Up Automated Alerts"}),"\n",(0,t.jsx)(e.h4,{children:"PostgreSQL Alert Script"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:'#!/bin/bash\r\n# PostgreSQL performance alert script\r\n\r\nDATABASE="your_database"\r\nSLOW_QUERY_THRESHOLD=5.0  # seconds\r\nUNUSED_INDEX_SIZE_THRESHOLD=100  # MB\r\n\r\n# Check for slow queries\r\nSLOW_QUERIES=$(psql -d $DATABASE -t -c "\r\nSELECT COUNT(*) \r\nFROM pg_stat_statements \r\nWHERE mean_time > $SLOW_QUERY_THRESHOLD * 1000  -- Convert to milliseconds\r\n")\r\n\r\nif [ "$SLOW_QUERIES" -gt 0 ]; then\r\n    echo "Alert: $SLOW_QUERIES slow queries detected"\r\n    # Send alert (email, Slack, etc.)\r\nfi\r\n\r\n# Check for large unused indexes\r\nUNUSED_INDEXES=$(psql -d $DATABASE -t -c "\r\nSELECT COUNT(*) \r\nFROM pg_stat_user_indexes \r\nWHERE idx_scan = 0 \r\n  AND pg_relation_size(indexrelid) > $UNUSED_INDEX_SIZE_THRESHOLD * 1024 * 1024\r\n")\r\n\r\nif [ "$UNUSED_INDEXES" -gt 0 ]; then\r\n    echo "Alert: $UNUSED_INDEXES large unused indexes detected"\r\n    # Send alert\r\nfi\n'})}),"\n",(0,t.jsx)(e.h4,{children:"SQL Server Alert Setup"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create alert for high fragmentation\r\nEXEC msdb.dbo.sp_add_alert\r\n    @name = N'High Index Fragmentation',\r\n    @message_id = 50001,\r\n    @severity = 16,\r\n    @enabled = 1;\r\n\r\n-- Create custom alert job\r\nEXEC msdb.dbo.sp_add_job\r\n    @job_name = N'Index Health Check';\r\n\r\nEXEC msdb.dbo.sp_add_jobstep\r\n    @job_name = N'Index Health Check',\r\n    @step_name = N'Check Fragmentation',\r\n    @command = N'\r\n    IF EXISTS (\r\n        SELECT 1 \r\n        FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, ''DETAILED'')\r\n        WHERE avg_fragmentation_in_percent > 30 AND page_count > 1000\r\n    )\r\n    BEGIN\r\n        RAISERROR(''High index fragmentation detected'', 16, 1)\r\n    END';\r\n\r\nEXEC msdb.dbo.sp_add_schedule\r\n    @schedule_name = N'Daily Check',\r\n    @freq_type = 4,  -- Daily\r\n    @freq_interval = 1;\r\n\r\nEXEC msdb.dbo.sp_attach_schedule\r\n    @job_name = N'Index Health Check',\r\n    @schedule_name = N'Daily Check';\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Best Practices for Production Monitoring"}),"\n",(0,t.jsx)(e.h3,{children:"Monitoring Strategy"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Baseline Performance"}),": Establish performance baselines before implementing changes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Continuous Monitoring"}),": Set up automated monitoring for key metrics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Proactive Alerts"}),": Configure alerts for performance degradation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Regular Reviews"}),": Schedule periodic index usage reviews"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Key Metrics to Track"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Query Performance"}),": Execution time, rows examined vs. returned"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Index Usage"}),": Seek/scan ratios, usage frequency"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resource Utilization"}),": CPU, memory, I/O impact"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fragmentation Levels"}),": Physical disorder of index pages"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Maintenance Windows"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Schedule Regular Maintenance"}),": Plan index rebuilds during low-activity periods"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Test Changes"}),": Always test index changes in staging environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Monitor After Changes"}),": Watch performance closely after index modifications"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Rollback Plans"}),": Have procedures to quickly revert problematic changes"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"In Part 6, we'll explore advanced indexing techniques including partitioned indexes, columnar indexes, and specialized indexing strategies for big data and analytics workloads."})]})}function o(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}={...(0,s.a)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(i,{...n})}):i(n)}},5582:function(n,e,r){r.r(e),r.d(e,{default:function(){return o},frontmatter:function(){return a}});var t=r(7437),s=r(4229);let a=void 0;function i(n){let e={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.a)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{children:"Advanced Indexing Techniques"}),"\n",(0,t.jsx)(e.h3,{children:"Partitioned Indexes"}),"\n",(0,t.jsx)(e.p,{children:"Partitioned indexes split large indexes across multiple physical structures, improving performance and manageability for very large tables."}),"\n",(0,t.jsx)(e.h4,{children:"PostgreSQL Table Partitioning with Indexes"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create partitioned table by date range\r\nCREATE TABLE sales_partitioned (\r\n    id BIGSERIAL,\r\n    customer_id INT,\r\n    sale_date DATE,\r\n    amount DECIMAL(10,2),\r\n    product_id INT,\r\n    region VARCHAR(50)\r\n) PARTITION BY RANGE (sale_date);\r\n\r\n-- Create partitions for different date ranges\r\nCREATE TABLE sales_2023 PARTITION OF sales_partitioned\r\n    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');\r\n\r\nCREATE TABLE sales_2024 PARTITION OF sales_partitioned\r\n    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');\r\n\r\n-- Create indexes on each partition\r\nCREATE INDEX idx_sales_2023_customer ON sales_2023(customer_id, sale_date);\r\nCREATE INDEX idx_sales_2024_customer ON sales_2024(customer_id, sale_date);\r\n\r\n-- Create global index across all partitions\r\nCREATE INDEX idx_sales_partitioned_customer ON sales_partitioned(customer_id);\r\n\r\n-- Queries automatically use partition pruning\r\nEXPLAIN (ANALYZE, BUFFERS) \r\nSELECT * FROM sales_partitioned \r\nWHERE sale_date BETWEEN '2024-01-01' AND '2024-01-31'\r\n  AND customer_id = 1000;\n"})}),"\n",(0,t.jsx)(e.h4,{children:"SQL Server Partitioned Indexes"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create partition function and scheme\r\nCREATE PARTITION FUNCTION sales_date_function (DATE)\r\nAS RANGE RIGHT FOR VALUES (\r\n    '2023-01-01', '2023-04-01', '2023-07-01', '2023-10-01',\r\n    '2024-01-01', '2024-04-01', '2024-07-01', '2024-10-01'\r\n);\r\n\r\nCREATE PARTITION SCHEME sales_date_scheme\r\nAS PARTITION sales_date_function\r\nTO (\r\n    [Partition1], [Partition2], [Partition3], [Partition4],\r\n    [Partition5], [Partition6], [Partition7], [Partition8]\r\n);\r\n\r\n-- Create partitioned table\r\nCREATE TABLE sales_partitioned (\r\n    id BIGINT IDENTITY(1,1),\r\n    customer_id INT,\r\n    sale_date DATE,\r\n    amount DECIMAL(10,2),\r\n    product_id INT,\r\n    region VARCHAR(50),\r\n    CONSTRAINT PK_sales_partitioned PRIMARY KEY (id, sale_date)\r\n) ON sales_date_scheme(sale_date);\r\n\r\n-- Create partitioned index\r\nCREATE INDEX idx_sales_customer_partitioned\r\nON sales_partitioned(customer_id, sale_date)\r\nON sales_date_scheme(sale_date);\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Columnar Indexes"}),"\n",(0,t.jsx)(e.p,{children:"Columnar indexes store data column-wise rather than row-wise, providing exceptional performance for analytical queries."}),"\n",(0,t.jsx)(e.h4,{children:"SQL Server Columnstore Indexes"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create clustered columnstore index for OLAP workload\r\nCREATE TABLE fact_sales (\r\n    sale_id BIGINT,\r\n    customer_id INT,\r\n    product_id INT,\r\n    sale_date DATE,\r\n    quantity INT,\r\n    unit_price DECIMAL(10,2),\r\n    total_amount DECIMAL(12,2),\r\n    store_id INT,\r\n    region_id INT\r\n);\r\n\r\n-- Clustered columnstore index (entire table stored as columnstore)\r\nCREATE CLUSTERED COLUMNSTORE INDEX cci_fact_sales ON fact_sales;\r\n\r\n-- Non-clustered columnstore index (for mixed workloads)\r\nCREATE TABLE sales_mixed (\r\n    id INT IDENTITY(1,1) PRIMARY KEY,\r\n    customer_id INT,\r\n    product_id INT,\r\n    sale_date DATE,\r\n    amount DECIMAL(10,2),\r\n    created_at DATETIME2 DEFAULT GETDATE()\r\n);\r\n\r\n-- Non-clustered columnstore for analytics\r\nCREATE NONCLUSTERED COLUMNSTORE INDEX ncci_sales_analytics\r\nON sales_mixed(customer_id, product_id, sale_date, amount);\r\n\r\n-- Analytical query performance\r\nSELECT \r\n    YEAR(sale_date) as sale_year,\r\n    region_id,\r\n    SUM(total_amount) as total_sales,\r\n    AVG(total_amount) as avg_sale,\r\n    COUNT(*) as transaction_count\r\nFROM fact_sales\r\nWHERE sale_date >= '2023-01-01'\r\nGROUP BY YEAR(sale_date), region_id\r\nORDER BY total_sales DESC;\n"})}),"\n",(0,t.jsx)(e.h4,{children:"PostgreSQL Columnar Storage (with Citus)"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Using columnar extension for analytics\r\nCREATE EXTENSION columnar;\r\n\r\n-- Create columnar table for analytics\r\nCREATE TABLE analytics_sales (\r\n    customer_id INT,\r\n    product_category VARCHAR(50),\r\n    sale_date DATE,\r\n    amount DECIMAL(10,2),\r\n    quantity INT,\r\n    region VARCHAR(50)\r\n) USING columnar;\r\n\r\n-- Analytical queries perform much better\r\nSELECT \r\n    product_category,\r\n    region,\r\n    DATE_TRUNC('month', sale_date) as month,\r\n    SUM(amount) as total_sales,\r\n    SUM(quantity) as total_quantity\r\nFROM analytics_sales\r\nWHERE sale_date >= '2023-01-01'\r\nGROUP BY product_category, region, DATE_TRUNC('month', sale_date)\r\nORDER BY total_sales DESC;\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Expression-Based and Functional Indexes"}),"\n",(0,t.jsx)(e.p,{children:"Create indexes on computed values and function results for complex query patterns."}),"\n",(0,t.jsx)(e.h4,{children:"PostgreSQL Functional Indexes"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Index on function result\r\nCREATE INDEX idx_users_lower_email ON users(lower(email));\r\nCREATE INDEX idx_products_profit_margin ON products((price - cost) / price * 100);\r\n\r\n-- Index on extracted date parts\r\nCREATE INDEX idx_orders_year_month ON orders(EXTRACT(YEAR FROM order_date), EXTRACT(MONTH FROM order_date));\r\n\r\n-- Complex expression index\r\nCREATE INDEX idx_customer_full_name ON customers((first_name || ' ' || last_name));\r\n\r\n-- JSONB functional indexes\r\nCREATE INDEX idx_user_preferences_theme \r\nON users((preferences->>'theme')) \r\nWHERE preferences->>'theme' IS NOT NULL;\r\n\r\n-- Trigram indexes for fuzzy text search\r\nCREATE EXTENSION pg_trgm;\r\nCREATE INDEX idx_products_name_trgm ON products USING gin(name gin_trgm_ops);\r\n\r\n-- Usage examples\r\nSELECT * FROM users WHERE lower(email) = 'john@example.com';\r\nSELECT * FROM products WHERE (price - cost) / price * 100 > 50;\r\nSELECT * FROM products WHERE name % 'wireless headphne';  -- Fuzzy match\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Oracle Function-Based Indexes"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Function-based indexes\r\nCREATE INDEX idx_employees_upper_last_name ON employees(UPPER(last_name));\r\nCREATE INDEX idx_orders_year ON orders(EXTRACT(YEAR FROM order_date));\r\n\r\n-- Case-insensitive searches\r\nCREATE INDEX idx_products_case_insensitive ON products(UPPER(product_name));\r\n\r\n-- Complex calculations\r\nCREATE INDEX idx_inventory_turnover ON inventory((units_sold / average_inventory) * 365);\r\n\r\n-- Virtual columns with indexes (Oracle 11g+)\r\nALTER TABLE products ADD (profit_margin GENERATED ALWAYS AS ((price - cost) / price * 100));\r\nCREATE INDEX idx_products_profit_margin ON products(profit_margin);\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Specialized Index Types"}),"\n",(0,t.jsx)(e.h4,{children:"Graph Database Indexing (Neo4j)"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cypher",children:"// Create node indexes\r\nCREATE INDEX customer_email_idx FOR (c:Customer) ON (c.email);\r\nCREATE INDEX product_sku_idx FOR (p:Product) ON (p.sku);\r\nCREATE INDEX order_date_idx FOR (o:Order) ON (o.date);\r\n\r\n// Composite indexes\r\nCREATE INDEX customer_region_status FOR (c:Customer) ON (c.region, c.status);\r\n\r\n// Full-text indexes\r\nCREATE FULLTEXT INDEX product_search FOR (p:Product) ON EACH [p.name, p.description];\r\n\r\n// Range indexes for relationships\r\nCREATE RANGE INDEX purchase_amount FOR ()-[r:PURCHASED]-() ON (r.amount);\r\n\r\n// Query using indexes\r\nMATCH (c:Customer {email: 'john@example.com'})-[r:PURCHASED]->(p:Product)\r\nWHERE r.amount > 100\r\nRETURN c.name, p.name, r.amount;\r\n\r\n// Full-text search\r\nCALL db.index.fulltext.queryNodes('product_search', 'wireless bluetooth') \r\nYIELD node, score\r\nRETURN node.name, node.description, score;\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Time-Series Database Indexing (InfluxDB)"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- InfluxDB automatically creates indexes on tags\r\n-- Tags are indexed, fields are not\r\n\r\n-- Example schema design\r\n-- Measurement: cpu_usage\r\n-- Tags: host, region, environment (automatically indexed)\r\n-- Fields: usage_percent, load_average (not indexed)\r\n\r\n-- Query using tag indexes (fast)\r\nSELECT mean(usage_percent) \r\nFROM cpu_usage \r\nWHERE host = 'server-01' \r\n  AND region = 'us-west' \r\n  AND time >= now() - 1h \r\nGROUP BY time(5m);\r\n\r\n-- Query on field (slower, requires scan)\r\nSELECT * FROM cpu_usage WHERE usage_percent > 90;\r\n\r\n-- Best practices for time-series indexing:\r\n-- 1. Use tags for dimensions you filter/group by\r\n-- 2. Keep tag cardinality reasonable (< 1M unique combinations)\r\n-- 3. Use fields for measured values\r\n-- 4. Design retention policies for old data\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Vector Indexes for AI/ML Workloads"}),"\n",(0,t.jsx)(e.h4,{children:"PostgreSQL with pgvector"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Install pgvector extension\r\nCREATE EXTENSION vector;\r\n\r\n-- Create table with vector column\r\nCREATE TABLE document_embeddings (\r\n    id BIGSERIAL PRIMARY KEY,\r\n    document_id INT,\r\n    title TEXT,\r\n    content_vector vector(384),  -- 384-dimensional embeddings\r\n    created_at TIMESTAMP DEFAULT NOW()\r\n);\r\n\r\n-- Create HNSW index for fast similarity search\r\nCREATE INDEX idx_embeddings_hnsw \r\nON document_embeddings \r\nUSING hnsw (content_vector vector_cosine_ops);\r\n\r\n-- Alternative: IVFFlat index\r\nCREATE INDEX idx_embeddings_ivf \r\nON document_embeddings \r\nUSING ivfflat (content_vector vector_cosine_ops) \r\nWITH (lists = 100);\r\n\r\n-- Similarity search queries\r\nSELECT \r\n    document_id,\r\n    title,\r\n    content_vector <=> '[0.1, 0.2, 0.3, ...]'::vector AS distance\r\nFROM document_embeddings\r\nORDER BY content_vector <=> '[0.1, 0.2, 0.3, ...]'::vector\r\nLIMIT 10;\r\n\r\n-- K-nearest neighbors with filters\r\nSELECT \r\n    document_id,\r\n    title,\r\n    content_vector <-> '[0.1, 0.2, 0.3, ...]'::vector AS distance\r\nFROM document_embeddings\r\nWHERE created_at >= '2024-01-01'\r\nORDER BY content_vector <-> '[0.1, 0.2, 0.3, ...]'::vector\r\nLIMIT 5;\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Elasticsearch Vector Search"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-json",children:'// Create index mapping with dense vector field\r\nPUT /documents\r\n{\r\n  "mappings": {\r\n    "properties": {\r\n      "title": { "type": "text" },\r\n      "content": { "type": "text" },\r\n      "embedding": {\r\n        "type": "dense_vector",\r\n        "dims": 384,\r\n        "index": true,\r\n        "similarity": "cosine"\r\n      },\r\n      "created_at": { "type": "date" }\r\n    }\r\n  }\r\n}\r\n\r\n// Index document with vector\r\nPOST /documents/_doc/1\r\n{\r\n  "title": "Machine Learning Basics",\r\n  "content": "Introduction to machine learning concepts...",\r\n  "embedding": [0.1, 0.2, 0.3, ...],\r\n  "created_at": "2024-01-15"\r\n}\r\n\r\n// Vector similarity search\r\nGET /documents/_search\r\n{\r\n  "knn": {\r\n    "field": "embedding",\r\n    "query_vector": [0.1, 0.2, 0.3, ...],\r\n    "k": 10,\r\n    "num_candidates": 100\r\n  },\r\n  "filter": {\r\n    "range": {\r\n      "created_at": {\r\n        "gte": "2024-01-01"\r\n      }\r\n    }\r\n  }\r\n}\n'})}),"\n",(0,t.jsx)(e.h2,{children:"Big Data Indexing Strategies"}),"\n",(0,t.jsx)(e.h3,{children:"Apache Spark with Delta Lake"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-scala",children:'// Create Delta table with optimized layout\r\nimport io.delta.tables._\r\n\r\n// Create partitioned Delta table\r\nspark.sql("""\r\nCREATE TABLE sales_delta (\r\n    customer_id LONG,\r\n    product_id LONG,\r\n    sale_date DATE,\r\n    amount DECIMAL(10,2),\r\n    region STRING\r\n) \r\nUSING DELTA\r\nPARTITIONED BY (region, date_format(sale_date, \'yyyy-MM\'))\r\n""")\r\n\r\n// Z-ORDER optimization for multi-dimensional clustering\r\nspark.sql("OPTIMIZE sales_delta ZORDER BY (customer_id, product_id)")\r\n\r\n// Data skipping with statistics\r\nspark.sql("ANALYZE TABLE sales_delta COMPUTE STATISTICS FOR ALL COLUMNS")\r\n\r\n// Bloom filters for high-cardinality columns\r\nspark.sql("""\r\nALTER TABLE sales_delta \r\nSET TBLPROPERTIES (\r\n    \'delta.bloomFilter.customer_id\' = \'true\',\r\n    \'delta.bloomFilter.product_id\' = \'true\'\r\n)\r\n""")\n'})}),"\n",(0,t.jsx)(e.h3,{children:"Apache Iceberg Indexing"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create Iceberg table with hidden partitioning\r\nCREATE TABLE sales_iceberg (\r\n    customer_id BIGINT,\r\n    product_id BIGINT,\r\n    sale_date DATE,\r\n    amount DECIMAL(10,2),\r\n    region STRING\r\n) \r\nUSING iceberg\r\nPARTITIONED BY (bucket(16, customer_id), days(sale_date));\r\n\r\n-- Iceberg automatically maintains partition statistics\r\n-- Query planning uses these statistics for pruning\r\n\r\n-- Sort order for better clustering\r\nALTER TABLE sales_iceberg WRITE ORDERED BY (customer_id, sale_date);\n"})}),"\n",(0,t.jsx)(e.h3,{children:"ClickHouse Specialized Indexes"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Primary key acts as sparse index\r\nCREATE TABLE events (\r\n    user_id UInt64,\r\n    event_time DateTime,\r\n    event_type String,\r\n    page_url String,\r\n    session_id String\r\n) \r\nENGINE = MergeTree()\r\nORDER BY (user_id, event_time);\r\n\r\n-- Skip indexes for non-primary key columns\r\nALTER TABLE events ADD INDEX idx_event_type event_type TYPE set(100) GRANULARITY 4;\r\nALTER TABLE events ADD INDEX idx_page_url page_url TYPE bloom_filter(0.01) GRANULARITY 1;\r\n\r\n-- Projection for pre-aggregated data\r\nALTER TABLE events ADD PROJECTION daily_stats (\r\n    SELECT \r\n        user_id,\r\n        toDate(event_time) as date,\r\n        event_type,\r\n        count()\r\n    GROUP BY user_id, date, event_type\r\n);\r\n\r\n-- Materialize the projection\r\nALTER TABLE events MATERIALIZE PROJECTION daily_stats;\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Index Design for Specific Workloads"}),"\n",(0,t.jsx)(e.h3,{children:"OLTP (Online Transaction Processing) Optimization"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Optimize for frequent point lookups and small range scans\r\n-- High concurrency, low latency requirements\r\n\r\n-- Order processing system indexes\r\nCREATE TABLE orders_oltp (\r\n    order_id BIGINT PRIMARY KEY,\r\n    customer_id INT,\r\n    order_date TIMESTAMP,\r\n    status VARCHAR(20),\r\n    total_amount DECIMAL(10,2),\r\n    shipping_address_id INT,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\r\n);\r\n\r\n-- OLTP-optimized indexes\r\nCREATE INDEX idx_orders_customer_status ON orders_oltp(customer_id, status);  -- Customer order lookup\r\nCREATE INDEX idx_orders_date_status ON orders_oltp(order_date, status);       -- Date range queries\r\nCREATE INDEX idx_orders_status_updated ON orders_oltp(status, updated_at);    -- Status monitoring\r\nCREATE UNIQUE INDEX idx_orders_customer_date ON orders_oltp(customer_id, order_date, order_id);  -- Avoid duplicates\r\n\r\n-- Covering index for order summary\r\nCREATE INDEX idx_orders_customer_covering ON orders_oltp(customer_id) \r\nINCLUDE (order_date, status, total_amount);  -- SQL Server syntax\n"})}),"\n",(0,t.jsx)(e.h3,{children:"OLAP (Online Analytical Processing) Optimization"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Optimize for complex aggregations and analytical queries\r\n-- Lower concurrency, higher latency acceptable\r\n\r\n-- Sales analytics table\r\nCREATE TABLE sales_olap (\r\n    sale_id BIGINT,\r\n    customer_id INT,\r\n    product_id INT,\r\n    category_id INT,\r\n    sale_date DATE,\r\n    quantity INT,\r\n    unit_price DECIMAL(10,2),\r\n    total_amount DECIMAL(12,2),\r\n    store_id INT,\r\n    region_id INT,\r\n    salesperson_id INT\r\n);\r\n\r\n-- OLAP-optimized indexes (wider, covering more columns)\r\nCREATE INDEX idx_sales_time_hierarchy ON sales_olap(sale_date, category_id, region_id, store_id);\r\nCREATE INDEX idx_sales_product_analysis ON sales_olap(product_id, category_id, sale_date);\r\nCREATE INDEX idx_sales_customer_behavior ON sales_olap(customer_id, sale_date, product_id);\r\n\r\n-- Columnstore index for analytics (SQL Server)\r\nCREATE NONCLUSTERED COLUMNSTORE INDEX ncci_sales_analytics \r\nON sales_olap(sale_date, customer_id, product_id, category_id, quantity, total_amount, region_id);\r\n\r\n-- Aggregate tables with appropriate indexes\r\nCREATE TABLE sales_daily_summary (\r\n    sale_date DATE,\r\n    category_id INT,\r\n    region_id INT,\r\n    total_sales DECIMAL(15,2),\r\n    total_quantity INT,\r\n    transaction_count INT,\r\n    PRIMARY KEY (sale_date, category_id, region_id)\r\n);\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Hybrid Workload (HTAP) Optimization"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Balance between OLTP and OLAP requirements\r\n-- Use read replicas or specialized engines\r\n\r\n-- Main OLTP table with minimal indexes\r\nCREATE TABLE transactions_htap (\r\n    transaction_id BIGINT PRIMARY KEY,\r\n    account_id INT,\r\n    transaction_date TIMESTAMP,\r\n    amount DECIMAL(12,2),\r\n    transaction_type VARCHAR(20),\r\n    description TEXT,\r\n    status VARCHAR(20),\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\r\n);\r\n\r\n-- OLTP indexes (lean and focused)\r\nCREATE INDEX idx_transactions_account_date ON transactions_htap(account_id, transaction_date);\r\nCREATE INDEX idx_transactions_status ON transactions_htap(status) WHERE status != 'completed';\r\n\r\n-- Analytical read replica with additional indexes\r\n-- (This could be a separate analytical database)\r\nCREATE INDEX idx_transactions_analytics_time ON transactions_htap(transaction_date, transaction_type, amount);\r\nCREATE INDEX idx_transactions_analytics_type ON transactions_htap(transaction_type, transaction_date, account_id);\r\n\r\n-- Use database-specific features for HTAP\r\n-- SQL Server: In-Memory OLTP with columnstore\r\n-- MySQL: HeatWave analytics engine\r\n-- PostgreSQL: Parallel query execution\r\n-- Oracle: In-Memory column store\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Performance Optimization Patterns"}),"\n",(0,t.jsx)(e.h3,{children:"Index Design Principles for Scale"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Minimize Index Count"}),": Each index has maintenance overhead"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Maximize Index Utilization"}),": Design for multiple query patterns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Consider Data Distribution"}),": Account for skewed data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Plan for Growth"}),": Design for future data volumes"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Advanced Optimization Techniques"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Filtered indexes for skewed data\r\nCREATE INDEX idx_orders_recent ON orders(customer_id, order_date) \r\nWHERE order_date >= '2024-01-01';\r\n\r\n-- Partial unique indexes\r\nCREATE UNIQUE INDEX idx_users_active_email ON users(email) \r\nWHERE status = 'active';\r\n\r\n-- Conditional indexes for sparse data\r\nCREATE INDEX idx_products_discount ON products(discount_percentage) \r\nWHERE discount_percentage > 0;\r\n\r\n-- Descending indexes for recent-first queries\r\nCREATE INDEX idx_logs_timestamp_desc ON application_logs(timestamp DESC);\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"In Part 7, we'll explore client-side optimization strategies including connection pooling, query caching, application-level indexing, and CDN optimization techniques to complement database indexing strategies."})]})}function o(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}={...(0,s.a)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(i,{...n})}):i(n)}},8296:function(n,e,r){r.r(e),r.d(e,{default:function(){return o},frontmatter:function(){return a}});var t=r(7437),s=r(4229);let a=void 0;function i(n){let e={code:"code",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",...(0,s.a)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{children:"Client-Side Database Optimization Strategies"}),"\n",(0,t.jsx)(e.p,{children:"While database indexes optimize server-side performance, client-side optimizations are equally crucial for overall application performance. This comprehensive guide covers connection management, caching strategies, query optimization, and application-level techniques."}),"\n",(0,t.jsx)(e.h3,{children:"Connection Pooling and Management"}),"\n",(0,t.jsx)(e.h4,{children:"Connection Pool Configuration"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Node.js with PostgreSQL (pg-pool)\r\nconst { Pool } = require('pg');\r\n\r\nconst pool = new Pool({\r\n    user: 'username',\r\n    host: 'localhost',\r\n    database: 'myapp',\r\n    password: 'password',\r\n    port: 5432,\r\n    \r\n    // Connection pool settings\r\n    min: 5,                    // Minimum connections\r\n    max: 20,                   // Maximum connections\r\n    idleTimeoutMillis: 30000,  // Close idle connections after 30s\r\n    connectionTimeoutMillis: 2000,  // Timeout when getting connection\r\n    \r\n    // Performance optimizations\r\n    keepAlive: true,\r\n    keepAliveInitialDelayMillis: 0,\r\n    \r\n    // Query timeout\r\n    query_timeout: 10000,      // 10 second query timeout\r\n    statement_timeout: 10000   // 10 second statement timeout\r\n});\r\n\r\n// Connection with retry logic\r\nasync function getConnectionWithRetry(maxRetries = 3) {\r\n    for (let i = 0; i < maxRetries; i++) {\r\n        try {\r\n            return await pool.connect();\r\n        } catch (error) {\r\n            if (i === maxRetries - 1) throw error;\r\n            await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));\r\n        }\r\n    }\r\n}\r\n\r\n// Graceful shutdown\r\nprocess.on('SIGINT', async () => {\r\n    console.log('Closing connection pool...');\r\n    await pool.end();\r\n    process.exit(0);\r\n});\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Python with SQLAlchemy connection pooling\r\nfrom sqlalchemy import create_engine, pool\r\nfrom sqlalchemy.pool import QueuePool\r\nimport logging\r\n\r\n# Configure connection pool\r\nengine = create_engine(\r\n    'postgresql://user:password@localhost/myapp',\r\n    \r\n    # Pool configuration\r\n    poolclass=QueuePool,\r\n    pool_size=10,              # Number of connections to maintain\r\n    max_overflow=20,           # Additional connections when pool is full\r\n    pool_recycle=3600,         # Recycle connections after 1 hour\r\n    pool_pre_ping=True,        # Validate connections before use\r\n    \r\n    # Connection timeout\r\n    connect_args={\r\n        'connect_timeout': 10,\r\n        'application_name': 'myapp'\r\n    },\r\n    \r\n    # Logging\r\n    echo=False  # Set to True for query logging\r\n)\r\n\r\n# Connection context manager\r\nfrom contextlib import contextmanager\r\n\r\n@contextmanager\r\ndef get_db_connection():\r\n    connection = engine.connect()\r\n    try:\r\n        yield connection\r\n    except Exception as e:\r\n        connection.rollback()\r\n        raise\r\n    finally:\r\n        connection.close()\r\n\r\n# Usage\r\ndef get_user_orders(user_id):\r\n    with get_db_connection() as conn:\r\n        result = conn.execute(\r\n            \"SELECT * FROM orders WHERE customer_id = %s\",\r\n            (user_id,)\r\n        )\r\n        return result.fetchall()\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Connection Pool Monitoring"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Node.js connection pool monitoring\r\nfunction monitorConnectionPool(pool) {\r\n    setInterval(() => {\r\n        console.log('Pool Stats:', {\r\n            totalCount: pool.totalCount,\r\n            idleCount: pool.idleCount,\r\n            waitingCount: pool.waitingCount\r\n        });\r\n        \r\n        // Alert if pool is under pressure\r\n        if (pool.waitingCount > 0) {\r\n            console.warn('Connection pool under pressure!');\r\n        }\r\n        \r\n        // Alert if too many idle connections\r\n        if (pool.idleCount > pool.options.max * 0.8) {\r\n            console.warn('Too many idle connections');\r\n        }\r\n    }, 30000); // Check every 30 seconds\r\n}\r\n\r\nmonitorConnectionPool(pool);\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Query Result Caching"}),"\n",(0,t.jsx)(e.h4,{children:"Redis-Based Query Caching"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Node.js with Redis caching\r\nconst redis = require('redis');\r\nconst client = redis.createClient({\r\n    host: 'localhost',\r\n    port: 6379,\r\n    retry_strategy: (options) => {\r\n        if (options.error && options.error.code === 'ECONNREFUSED') {\r\n            return new Error('Redis server connection refused');\r\n        }\r\n        if (options.total_retry_time > 1000 * 60 * 60) {\r\n            return new Error('Retry time exhausted');\r\n        }\r\n        return Math.min(options.attempt * 100, 3000);\r\n    }\r\n});\r\n\r\nclass QueryCache {\r\n    constructor(redisClient, defaultTTL = 300) {\r\n        this.redis = redisClient;\r\n        this.defaultTTL = defaultTTL;\r\n    }\r\n    \r\n    // Generate cache key from query and parameters\r\n    generateCacheKey(query, params = []) {\r\n        const crypto = require('crypto');\r\n        const keyData = query + JSON.stringify(params);\r\n        return `query_cache:${crypto.createHash('md5').update(keyData).digest('hex')}`;\r\n    }\r\n    \r\n    // Get cached result\r\n    async getCachedResult(query, params = []) {\r\n        const cacheKey = this.generateCacheKey(query, params);\r\n        try {\r\n            const cached = await this.redis.get(cacheKey);\r\n            return cached ? JSON.parse(cached) : null;\r\n        } catch (error) {\r\n            console.error('Cache retrieval error:', error);\r\n            return null;\r\n        }\r\n    }\r\n    \r\n    // Cache query result\r\n    async setCachedResult(query, params = [], result, ttl = null) {\r\n        const cacheKey = this.generateCacheKey(query, params);\r\n        const expiration = ttl || this.defaultTTL;\r\n        \r\n        try {\r\n            await this.redis.setex(cacheKey, expiration, JSON.stringify(result));\r\n        } catch (error) {\r\n            console.error('Cache storage error:', error);\r\n        }\r\n    }\r\n    \r\n    // Invalidate cache by pattern\r\n    async invalidatePattern(pattern) {\r\n        try {\r\n            const keys = await this.redis.keys(`query_cache:${pattern}`);\r\n            if (keys.length > 0) {\r\n                await this.redis.del(keys);\r\n            }\r\n        } catch (error) {\r\n            console.error('Cache invalidation error:', error);\r\n        }\r\n    }\r\n}\r\n\r\n// Usage example\r\nconst queryCache = new QueryCache(client);\r\n\r\nasync function getUserOrders(userId) {\r\n    const query = \"SELECT * FROM orders WHERE customer_id = $1 ORDER BY order_date DESC\";\r\n    const params = [userId];\r\n    \r\n    // Try cache first\r\n    let result = await queryCache.getCachedResult(query, params);\r\n    \r\n    if (!result) {\r\n        // Cache miss - execute query\r\n        const dbResult = await pool.query(query, params);\r\n        result = dbResult.rows;\r\n        \r\n        // Cache for 5 minutes\r\n        await queryCache.setCachedResult(query, params, result, 300);\r\n    }\r\n    \r\n    return result;\r\n}\r\n\r\n// Cache invalidation on data changes\r\nasync function createOrder(orderData) {\r\n    const result = await pool.query(\r\n        \"INSERT INTO orders (...) VALUES (...) RETURNING *\",\r\n        orderData\r\n    );\r\n    \r\n    // Invalidate related caches\r\n    await queryCache.invalidatePattern(`*customer_id*${orderData.customer_id}*`);\r\n    \r\n    return result.rows[0];\r\n}\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Application-Level Caching"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Python with in-memory caching using functools.lru_cache\r\nfrom functools import lru_cache, wraps\r\nimport time\r\nimport hashlib\r\nimport json\r\n\r\nclass TTLCache:\r\n    def __init__(self, maxsize=128, ttl=300):\r\n        self.cache = {}\r\n        self.timestamps = {}\r\n        self.maxsize = maxsize\r\n        self.ttl = ttl\r\n    \r\n    def get(self, key):\r\n        if key in self.cache:\r\n            if time.time() - self.timestamps[key] < self.ttl:\r\n                return self.cache[key]\r\n            else:\r\n                # Expired\r\n                del self.cache[key]\r\n                del self.timestamps[key]\r\n        return None\r\n    \r\n    def set(self, key, value):\r\n        # Implement LRU eviction if needed\r\n        if len(self.cache) >= self.maxsize:\r\n            oldest_key = min(self.timestamps.keys(), key=self.timestamps.get)\r\n            del self.cache[oldest_key]\r\n            del self.timestamps[oldest_key]\r\n        \r\n        self.cache[key] = value\r\n        self.timestamps[key] = time.time()\r\n\r\n# Cache decorator\r\ndef cached_query(ttl=300, maxsize=128):\r\n    cache = TTLCache(maxsize, ttl)\r\n    \r\n    def decorator(func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            # Create cache key from function args\r\n            key_data = json.dumps((args, sorted(kwargs.items())), default=str)\r\n            cache_key = hashlib.md5(key_data.encode()).hexdigest()\r\n            \r\n            # Try cache first\r\n            result = cache.get(cache_key)\r\n            if result is not None:\r\n                return result\r\n            \r\n            # Cache miss - execute function\r\n            result = func(*args, **kwargs)\r\n            cache.set(cache_key, result)\r\n            \r\n            return result\r\n        return wrapper\r\n    return decorator\r\n\r\n# Usage\r\n@cached_query(ttl=600, maxsize=100)  # Cache for 10 minutes\r\ndef get_product_details(product_id):\r\n    with get_db_connection() as conn:\r\n        result = conn.execute(\r\n            "SELECT * FROM products WHERE id = %s",\r\n            (product_id,)\r\n        )\r\n        return result.fetchone()\r\n\r\n@cached_query(ttl=300)  # Cache for 5 minutes\r\ndef get_category_products(category_id, limit=10):\r\n    with get_db_connection() as conn:\r\n        result = conn.execute(\r\n            """\r\n            SELECT p.*, c.name as category_name \r\n            FROM products p \r\n            JOIN categories c ON p.category_id = c.id \r\n            WHERE p.category_id = %s \r\n            ORDER BY p.created_at DESC \r\n            LIMIT %s\r\n            """,\r\n            (category_id, limit)\r\n        )\r\n        return result.fetchall()\n'})}),"\n",(0,t.jsx)(e.h3,{children:"Lazy Loading and Pagination"}),"\n",(0,t.jsx)(e.h4,{children:"Cursor-Based Pagination"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Efficient cursor-based pagination\r\nclass CursorPaginator {\r\n    constructor(pool) {\r\n        this.pool = pool;\r\n    }\r\n    \r\n    async getPage(table, orderBy, limit = 20, cursor = null, filters = {}) {\r\n        let query = `SELECT * FROM ${table}`;\r\n        let params = [];\r\n        let paramIndex = 1;\r\n        \r\n        // Add filters\r\n        const filterClauses = [];\r\n        for (const [column, value] of Object.entries(filters)) {\r\n            filterClauses.push(`${column} = $${paramIndex++}`);\r\n            params.push(value);\r\n        }\r\n        \r\n        // Add cursor condition\r\n        if (cursor) {\r\n            filterClauses.push(`${orderBy} > $${paramIndex++}`);\r\n            params.push(cursor);\r\n        }\r\n        \r\n        if (filterClauses.length > 0) {\r\n            query += ` WHERE ${filterClauses.join(' AND ')}`;\r\n        }\r\n        \r\n        query += ` ORDER BY ${orderBy} LIMIT $${paramIndex}`;\r\n        params.push(limit + 1); // Fetch one extra to determine if there's a next page\r\n        \r\n        const result = await this.pool.query(query, params);\r\n        const hasNextPage = result.rows.length > limit;\r\n        const items = hasNextPage ? result.rows.slice(0, -1) : result.rows;\r\n        \r\n        return {\r\n            items,\r\n            hasNextPage,\r\n            nextCursor: hasNextPage ? items[items.length - 1][orderBy] : null\r\n        };\r\n    }\r\n}\r\n\r\n// Usage\r\nconst paginator = new CursorPaginator(pool);\r\n\r\nasync function getUserOrdersPage(userId, cursor = null) {\r\n    return await paginator.getPage(\r\n        'orders',\r\n        'created_at',\r\n        20,\r\n        cursor,\r\n        { customer_id: userId }\r\n    );\r\n}\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Lazy Loading with Batch Fetching"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// DataLoader for batching and caching\r\nconst DataLoader = require('dataloader');\r\n\r\n// Batch function to load multiple users at once\r\nasync function batchLoadUsers(userIds) {\r\n    const query = 'SELECT * FROM users WHERE id = ANY($1)';\r\n    const result = await pool.query(query, [userIds]);\r\n    \r\n    // Return results in the same order as input\r\n    const userMap = new Map(result.rows.map(user => [user.id, user]));\r\n    return userIds.map(id => userMap.get(id) || null);\r\n}\r\n\r\n// Create DataLoader instance\r\nconst userLoader = new DataLoader(batchLoadUsers, {\r\n    cache: true,\r\n    maxBatchSize: 100,\r\n    batchScheduleFn: callback => setTimeout(callback, 10) // Batch within 10ms\r\n});\r\n\r\n// Batch function for user orders\r\nasync function batchLoadUserOrders(userIds) {\r\n    const query = `\r\n        SELECT customer_id, json_agg(\r\n            json_build_object(\r\n                'id', id,\r\n                'order_date', order_date,\r\n                'total', total_amount\r\n            ) ORDER BY order_date DESC\r\n        ) as orders\r\n        FROM orders \r\n        WHERE customer_id = ANY($1) \r\n        GROUP BY customer_id\r\n    `;\r\n    \r\n    const result = await pool.query(query, [userIds]);\r\n    const orderMap = new Map(result.rows.map(row => [row.customer_id, row.orders]));\r\n    \r\n    return userIds.map(id => orderMap.get(id) || []);\r\n}\r\n\r\nconst userOrdersLoader = new DataLoader(batchLoadUserOrders);\r\n\r\n// Usage in resolvers or route handlers\r\nasync function handleUserDetails(req, res) {\r\n    const userId = req.params.userId;\r\n    \r\n    // These will be batched if called within the same event loop tick\r\n    const user = await userLoader.load(userId);\r\n    const orders = await userOrdersLoader.load(userId);\r\n    \r\n    res.json({ user, orders });\r\n}\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Query Optimization Techniques"}),"\n",(0,t.jsx)(e.h4,{children:"Prepared Statements"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Node.js prepared statements\r\nclass PreparedStatements {\r\n    constructor(pool) {\r\n        this.pool = pool;\r\n        this.statements = new Map();\r\n    }\r\n    \r\n    async prepare(name, query) {\r\n        if (!this.statements.has(name)) {\r\n            const client = await this.pool.connect();\r\n            try {\r\n                await client.query(`PREPARE ${name} AS ${query}`);\r\n                this.statements.set(name, query);\r\n            } finally {\r\n                client.release();\r\n            }\r\n        }\r\n    }\r\n    \r\n    async execute(name, params = []) {\r\n        const client = await this.pool.connect();\r\n        try {\r\n            return await client.query(`EXECUTE ${name}(${params.map((_, i) => `$${i + 1}`).join(',')})`, params);\r\n        } finally {\r\n            client.release();\r\n        }\r\n    }\r\n}\r\n\r\n// Usage\r\nconst preparedStatements = new PreparedStatements(pool);\r\n\r\n// Prepare frequently used queries\r\nawait preparedStatements.prepare('get_user_orders', \r\n    'SELECT * FROM orders WHERE customer_id = $1 ORDER BY order_date DESC'\r\n);\r\n\r\nawait preparedStatements.prepare('get_product_by_sku',\r\n    'SELECT * FROM products WHERE sku = $1'\r\n);\r\n\r\n// Execute prepared statements\r\nconst orders = await preparedStatements.execute('get_user_orders', [userId]);\r\nconst product = await preparedStatements.execute('get_product_by_sku', [sku]);\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Query Builder Optimization"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Knex.js query builder with optimizations\r\nconst knex = require('knex')({\r\n    client: 'postgresql',\r\n    connection: {\r\n        host: 'localhost',\r\n        user: 'username',\r\n        password: 'password',\r\n        database: 'myapp'\r\n    },\r\n    pool: {\r\n        min: 2,\r\n        max: 10\r\n    },\r\n    // Enable query debugging\r\n    debug: process.env.NODE_ENV === 'development'\r\n});\r\n\r\nclass OrderService {\r\n    // Optimized query with selective fields\r\n    async getUserOrders(userId, options = {}) {\r\n        const {\r\n            limit = 20,\r\n            offset = 0,\r\n            status = null,\r\n            startDate = null,\r\n            endDate = null,\r\n            includeItems = false\r\n        } = options;\r\n        \r\n        let query = knex('orders')\r\n            .select([\r\n                'orders.id',\r\n                'orders.order_date',\r\n                'orders.status',\r\n                'orders.total_amount'\r\n            ])\r\n            .where('orders.customer_id', userId)\r\n            .orderBy('orders.order_date', 'desc')\r\n            .limit(limit)\r\n            .offset(offset);\r\n        \r\n        // Add optional filters\r\n        if (status) {\r\n            query = query.where('orders.status', status);\r\n        }\r\n        \r\n        if (startDate) {\r\n            query = query.where('orders.order_date', '>=', startDate);\r\n        }\r\n        \r\n        if (endDate) {\r\n            query = query.where('orders.order_date', '<=', endDate);\r\n        }\r\n        \r\n        // Conditional joins\r\n        if (includeItems) {\r\n            query = query\r\n                .select([\r\n                    'orders.*',\r\n                    knex.raw(`\r\n                        json_agg(\r\n                            json_build_object(\r\n                                'product_id', oi.product_id,\r\n                                'quantity', oi.quantity,\r\n                                'price', oi.unit_price\r\n                            )\r\n                        ) as items\r\n                    `)\r\n                ])\r\n                .leftJoin('order_items as oi', 'orders.id', 'oi.order_id')\r\n                .groupBy('orders.id');\r\n        }\r\n        \r\n        return await query;\r\n    }\r\n    \r\n    // Bulk operations\r\n    async createMultipleOrders(orderData) {\r\n        return await knex.transaction(async (trx) => {\r\n            const orders = await trx('orders')\r\n                .insert(orderData)\r\n                .returning('*');\r\n            \r\n            // Batch insert order items if provided\r\n            const orderItems = [];\r\n            orders.forEach((order, index) => {\r\n                if (orderData[index].items) {\r\n                    orderData[index].items.forEach(item => {\r\n                        orderItems.push({\r\n                            order_id: order.id,\r\n                            ...item\r\n                        });\r\n                    });\r\n                }\r\n            });\r\n            \r\n            if (orderItems.length > 0) {\r\n                await trx('order_items').insert(orderItems);\r\n            }\r\n            \r\n            return orders;\r\n        });\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(e.h3,{children:"CDN and Static Asset Optimization"}),"\n",(0,t.jsx)(e.h4,{children:"Database-Driven CDN Invalidation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// CDN cache invalidation service\r\nclass CDNService {\r\n    constructor(cdnProvider, cacheService) {\r\n        this.cdn = cdnProvider;\r\n        this.cache = cacheService;\r\n    }\r\n    \r\n    // Generate cache tags for database entities\r\n    generateCacheTags(entityType, entityId, additionalTags = []) {\r\n        return [\r\n            `${entityType}:${entityId}`,\r\n            entityType,\r\n            ...additionalTags\r\n        ];\r\n    }\r\n    \r\n    // Invalidate CDN cache when data changes\r\n    async invalidateOnDataChange(entityType, entityId, affectedPaths = []) {\r\n        const tags = this.generateCacheTags(entityType, entityId);\r\n        \r\n        try {\r\n            // Purge CDN cache by tags\r\n            await this.cdn.purgeByTags(tags);\r\n            \r\n            // Purge specific paths if provided\r\n            if (affectedPaths.length > 0) {\r\n                await this.cdn.purgeByPaths(affectedPaths);\r\n            }\r\n            \r\n            // Clear application cache\r\n            await this.cache.invalidatePattern(`*${entityType}*${entityId}*`);\r\n            \r\n        } catch (error) {\r\n            console.error('CDN invalidation failed:', error);\r\n        }\r\n    }\r\n    \r\n    // Smart cache headers based on data freshness\r\n    getCacheHeaders(entityType, lastModified) {\r\n        const now = Date.now();\r\n        const age = now - new Date(lastModified).getTime();\r\n        \r\n        // Shorter cache for recently modified data\r\n        let maxAge = 3600; // 1 hour default\r\n        \r\n        if (age < 300000) { // Modified in last 5 minutes\r\n            maxAge = 60;\r\n        } else if (age < 3600000) { // Modified in last hour\r\n            maxAge = 300;\r\n        } else if (age > 86400000) { // Modified more than 1 day ago\r\n            maxAge = 86400; // Cache for 24 hours\r\n        }\r\n        \r\n        return {\r\n            'Cache-Control': `public, max-age=${maxAge}, s-maxage=${maxAge * 2}`,\r\n            'ETag': `\"${entityType}-${lastModified}\"`,\r\n            'Last-Modified': new Date(lastModified).toUTCString()\r\n        };\r\n    }\r\n}\r\n\r\n// Usage in API endpoints\r\napp.get('/api/products/:id', async (req, res) => {\r\n    const productId = req.params.id;\r\n    \r\n    try {\r\n        // Get product with cache tags\r\n        const product = await productService.getById(productId);\r\n        \r\n        if (!product) {\r\n            return res.status(404).json({ error: 'Product not found' });\r\n        }\r\n        \r\n        // Set cache headers\r\n        const cacheHeaders = cdnService.getCacheHeaders('product', product.updated_at);\r\n        res.set(cacheHeaders);\r\n        \r\n        // Add cache tags for CDN\r\n        res.set('Cache-Tag', cdnService.generateCacheTags('product', productId).join(','));\r\n        \r\n        res.json(product);\r\n        \r\n    } catch (error) {\r\n        res.status(500).json({ error: 'Internal server error' });\r\n    }\r\n});\r\n\r\n// Invalidate cache on product updates\r\napp.put('/api/products/:id', async (req, res) => {\r\n    const productId = req.params.id;\r\n    \r\n    try {\r\n        const updatedProduct = await productService.update(productId, req.body);\r\n        \r\n        // Invalidate related caches\r\n        await cdnService.invalidateOnDataChange('product', productId, [\r\n            `/api/products/${productId}`,\r\n            `/products/${productId}`,\r\n            '/api/products' // Product list might be affected\r\n        ]);\r\n        \r\n        res.json(updatedProduct);\r\n        \r\n    } catch (error) {\r\n        res.status(500).json({ error: 'Update failed' });\r\n    }\r\n});\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Read Replicas and Load Balancing"}),"\n",(0,t.jsx)(e.h4,{children:"Database Read/Write Splitting"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Database connection manager with read/write splitting\r\nclass DatabaseManager {\r\n    constructor(config) {\r\n        // Primary database for writes\r\n        this.writePool = new Pool({\r\n            ...config.primary,\r\n            max: config.primary.maxConnections || 10\r\n        });\r\n        \r\n        // Read replicas for reads\r\n        this.readPools = config.replicas.map(replicaConfig => \r\n            new Pool({\r\n                ...replicaConfig,\r\n                max: replicaConfig.maxConnections || 15\r\n            })\r\n        );\r\n        \r\n        this.readPoolIndex = 0;\r\n    }\r\n    \r\n    // Get connection for write operations\r\n    async getWriteConnection() {\r\n        return await this.writePool.connect();\r\n    }\r\n    \r\n    // Get connection for read operations (round-robin)\r\n    async getReadConnection() {\r\n        const pool = this.readPools[this.readPoolIndex];\r\n        this.readPoolIndex = (this.readPoolIndex + 1) % this.readPools.length;\r\n        return await pool.connect();\r\n    }\r\n    \r\n    // Execute read query\r\n    async queryRead(text, params) {\r\n        const client = await this.getReadConnection();\r\n        try {\r\n            return await client.query(text, params);\r\n        } finally {\r\n            client.release();\r\n        }\r\n    }\r\n    \r\n    // Execute write query\r\n    async queryWrite(text, params) {\r\n        const client = await this.getWriteConnection();\r\n        try {\r\n            return await client.query(text, params);\r\n        } finally {\r\n            client.release();\r\n        }\r\n    }\r\n    \r\n    // Transaction support (always uses primary)\r\n    async transaction(callback) {\r\n        const client = await this.getWriteConnection();\r\n        \r\n        try {\r\n            await client.query('BEGIN');\r\n            const result = await callback(client);\r\n            await client.query('COMMIT');\r\n            return result;\r\n        } catch (error) {\r\n            await client.query('ROLLBACK');\r\n            throw error;\r\n        } finally {\r\n            client.release();\r\n        }\r\n    }\r\n}\r\n\r\n// Configuration\r\nconst dbManager = new DatabaseManager({\r\n    primary: {\r\n        host: 'primary-db.example.com',\r\n        user: 'app_user',\r\n        password: 'password',\r\n        database: 'myapp',\r\n        maxConnections: 10\r\n    },\r\n    replicas: [\r\n        {\r\n            host: 'replica1-db.example.com',\r\n            user: 'app_user',\r\n            password: 'password',\r\n            database: 'myapp',\r\n            maxConnections: 15\r\n        },\r\n        {\r\n            host: 'replica2-db.example.com',\r\n            user: 'app_user',\r\n            password: 'password',\r\n            database: 'myapp',\r\n            maxConnections: 15\r\n        }\r\n    ]\r\n});\r\n\r\n// Service layer using read/write splitting\r\nclass UserService {\r\n    // Read operations use replicas\r\n    async getUser(userId) {\r\n        const result = await dbManager.queryRead(\r\n            'SELECT * FROM users WHERE id = $1',\r\n            [userId]\r\n        );\r\n        return result.rows[0];\r\n    }\r\n    \r\n    async searchUsers(criteria) {\r\n        const result = await dbManager.queryRead(\r\n            'SELECT * FROM users WHERE name ILIKE $1 LIMIT 50',\r\n            [`%${criteria}%`]\r\n        );\r\n        return result.rows;\r\n    }\r\n    \r\n    // Write operations use primary\r\n    async createUser(userData) {\r\n        const result = await dbManager.queryWrite(\r\n            'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING *',\r\n            [userData.name, userData.email]\r\n        );\r\n        return result.rows[0];\r\n    }\r\n    \r\n    async updateUser(userId, updates) {\r\n        return await dbManager.transaction(async (client) => {\r\n            // All operations in transaction use primary\r\n            const result = await client.query(\r\n                'UPDATE users SET name = $1, email = $2, updated_at = NOW() WHERE id = $3 RETURNING *',\r\n                [updates.name, updates.email, userId]\r\n            );\r\n            \r\n            // Log the update\r\n            await client.query(\r\n                'INSERT INTO user_audit (user_id, action, changed_at) VALUES ($1, $2, NOW())',\r\n                [userId, 'update']\r\n            );\r\n            \r\n            return result.rows[0];\r\n        });\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Performance Monitoring and Optimization"}),"\n",(0,t.jsx)(e.h3,{children:"Client-Side Performance Metrics"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"// Performance monitoring middleware\r\nclass PerformanceMonitor {\r\n    constructor() {\r\n        this.metrics = new Map();\r\n    }\r\n    \r\n    // Middleware to track query performance\r\n    trackQuery(queryName) {\r\n        return (req, res, next) => {\r\n            const startTime = process.hrtime.bigint();\r\n            \r\n            // Override res.json to capture response time\r\n            const originalJson = res.json;\r\n            res.json = function(data) {\r\n                const endTime = process.hrtime.bigint();\r\n                const duration = Number(endTime - startTime) / 1000000; // Convert to milliseconds\r\n                \r\n                // Store metrics\r\n                monitor.recordMetric(queryName, duration, req, res);\r\n                \r\n                return originalJson.call(this, data);\r\n            };\r\n            \r\n            next();\r\n        };\r\n    }\r\n    \r\n    recordMetric(queryName, duration, req, res) {\r\n        const metric = {\r\n            queryName,\r\n            duration,\r\n            timestamp: Date.now(),\r\n            statusCode: res.statusCode,\r\n            userAgent: req.get('User-Agent'),\r\n            ip: req.ip\r\n        };\r\n        \r\n        // Store in time-series format\r\n        if (!this.metrics.has(queryName)) {\r\n            this.metrics.set(queryName, []);\r\n        }\r\n        \r\n        this.metrics.get(queryName).push(metric);\r\n        \r\n        // Keep only last 1000 measurements per query\r\n        const measurements = this.metrics.get(queryName);\r\n        if (measurements.length > 1000) {\r\n            measurements.shift();\r\n        }\r\n        \r\n        // Alert on slow queries\r\n        if (duration > 1000) {\r\n            console.warn(`Slow query detected: ${queryName} took ${duration}ms`);\r\n        }\r\n    }\r\n    \r\n    getStats(queryName) {\r\n        const measurements = this.metrics.get(queryName) || [];\r\n        if (measurements.length === 0) return null;\r\n        \r\n        const durations = measurements.map(m => m.duration);\r\n        const sorted = durations.sort((a, b) => a - b);\r\n        \r\n        return {\r\n            count: measurements.length,\r\n            avg: durations.reduce((sum, d) => sum + d, 0) / durations.length,\r\n            min: sorted[0],\r\n            max: sorted[sorted.length - 1],\r\n            p50: sorted[Math.floor(sorted.length * 0.5)],\r\n            p95: sorted[Math.floor(sorted.length * 0.95)],\r\n            p99: sorted[Math.floor(sorted.length * 0.99)]\r\n        };\r\n    }\r\n}\r\n\r\nconst monitor = new PerformanceMonitor();\r\n\r\n// Usage\r\napp.get('/api/users/:id', \r\n    monitor.trackQuery('get_user'),\r\n    async (req, res) => {\r\n        const user = await userService.getUser(req.params.id);\r\n        res.json(user);\r\n    }\r\n);\r\n\r\n// Metrics endpoint\r\napp.get('/metrics', (req, res) => {\r\n    const stats = {};\r\n    for (const [queryName] of monitor.metrics) {\r\n        stats[queryName] = monitor.getStats(queryName);\r\n    }\r\n    res.json(stats);\r\n});\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"In Part 8 (final part), we'll explore real-world case studies and best practices, including production optimization examples, migration strategies, troubleshooting guides, and comprehensive checklists for database index optimization across different industries and use cases."})]})}function o(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}={...(0,s.a)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(i,{...n})}):i(n)}},8342:function(n,e,r){r.r(e),r.d(e,{default:function(){return o},frontmatter:function(){return a}});var t=r(7437),s=r(4229);let a=void 0;function i(n){let e={code:"code",h2:"h2",h3:"h3",h4:"h4",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{children:"Real-World Case Studies"}),"\n",(0,t.jsx)(e.h3,{children:"Case Study 1: E-commerce Platform Optimization"}),"\n",(0,t.jsx)(e.h4,{children:"The Challenge"}),"\n",(0,t.jsx)(e.p,{children:"An e-commerce platform with 10 million products and 1 million daily active users was experiencing:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Product search queries taking 3-5 seconds"}),"\n",(0,t.jsx)(e.li,{children:"Checkout process timeouts during peak hours"}),"\n",(0,t.jsx)(e.li,{children:"Admin dashboard reports timing out"}),"\n",(0,t.jsx)(e.li,{children:"Database CPU at 90% during traffic spikes"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{children:"The Solution"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 1: Critical Query Optimization"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Original slow product search query\r\nSELECT p.*, c.name as category_name, AVG(r.rating) as avg_rating\r\nFROM products p\r\nLEFT JOIN categories c ON p.category_id = c.id\r\nLEFT JOIN reviews r ON p.id = r.product_id\r\nWHERE p.name ILIKE '%wireless%'\r\n   OR p.description ILIKE '%wireless%'\r\nGROUP BY p.id, c.name\r\nORDER BY avg_rating DESC, p.created_at DESC\r\nLIMIT 20;\r\n\r\n-- Problem: Full table scans, expensive ILIKE operations, complex aggregations\r\n-- Execution time: 4.2 seconds\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Optimized Approach:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Step 1: Create full-text search index\r\nCREATE INDEX idx_products_search \r\nON products \r\nUSING gin(to_tsvector('english', name || ' ' || description));\r\n\r\n-- Step 2: Denormalize ratings for faster access\r\nCREATE TABLE product_ratings_cache (\r\n    product_id INT PRIMARY KEY,\r\n    avg_rating DECIMAL(3,2),\r\n    review_count INT,\r\n    last_updated TIMESTAMP DEFAULT NOW()\r\n);\r\n\r\n-- Trigger to maintain ratings cache\r\nCREATE OR REPLACE FUNCTION update_product_rating_cache()\r\nRETURNS TRIGGER AS $$\r\nBEGIN\r\n    INSERT INTO product_ratings_cache (product_id, avg_rating, review_count)\r\n    SELECT \r\n        NEW.product_id,\r\n        AVG(rating),\r\n        COUNT(*)\r\n    FROM reviews \r\n    WHERE product_id = NEW.product_id\r\n    GROUP BY product_id\r\n    ON CONFLICT (product_id) \r\n    DO UPDATE SET \r\n        avg_rating = EXCLUDED.avg_rating,\r\n        review_count = EXCLUDED.review_count,\r\n        last_updated = NOW();\r\n    \r\n    RETURN NEW;\r\nEND;\r\n$$ LANGUAGE plpgsql;\r\n\r\n-- Step 3: Optimized search query\r\nSELECT \r\n    p.id,\r\n    p.name,\r\n    p.price,\r\n    p.image_url,\r\n    c.name as category_name,\r\n    prc.avg_rating,\r\n    prc.review_count\r\nFROM products p\r\nJOIN categories c ON p.category_id = c.id\r\nLEFT JOIN product_ratings_cache prc ON p.id = prc.product_id\r\nWHERE to_tsvector('english', p.name || ' ' || p.description) @@ to_tsquery('english', 'wireless')\r\nORDER BY \r\n    CASE WHEN prc.avg_rating IS NOT NULL THEN prc.avg_rating ELSE 0 END DESC,\r\n    p.created_at DESC\r\nLIMIT 20;\r\n\r\n-- Result: Query time reduced from 4.2s to 0.08s (98% improvement)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 2: Checkout Optimization"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Original checkout process issues:\r\n-- 1. Inventory checks were slow\r\n-- 2. Multiple round trips to database\r\n-- 3. Lock contention during updates\r\n\r\n-- Solution: Batch operations with proper indexing\r\nCREATE INDEX idx_inventory_product_location ON inventory(product_id, warehouse_location);\r\nCREATE INDEX idx_orders_processing ON orders(status, created_at) WHERE status = 'processing';\r\n\r\n-- Optimized checkout procedure\r\nCREATE OR REPLACE FUNCTION process_checkout(\r\n    p_customer_id INT,\r\n    p_items JSONB,\r\n    p_shipping_address JSONB\r\n) RETURNS JSON AS $$\r\nDECLARE\r\n    v_order_id INT;\r\n    v_item JSONB;\r\n    v_total DECIMAL(10,2) := 0;\r\n    v_insufficient_stock TEXT[];\r\nBEGIN\r\n    -- Step 1: Validate inventory in batch\r\n    SELECT array_agg(\r\n        CASE \r\n            WHEN i.available_quantity < (item->>'quantity')::INT \r\n            THEN item->>'product_id'\r\n        END\r\n    ) INTO v_insufficient_stock\r\n    FROM jsonb_array_elements(p_items) AS item\r\n    JOIN inventory i ON i.product_id = (item->>'product_id')::INT\r\n    WHERE i.available_quantity < (item->>'quantity')::INT;\r\n    \r\n    IF array_length(v_insufficient_stock, 1) > 0 THEN\r\n        RETURN json_build_object(\r\n            'success', false,\r\n            'error', 'insufficient_stock',\r\n            'products', v_insufficient_stock\r\n        );\r\n    END IF;\r\n    \r\n    -- Step 2: Create order and reserve inventory atomically\r\n    INSERT INTO orders (customer_id, status, shipping_address, created_at)\r\n    VALUES (p_customer_id, 'confirmed', p_shipping_address, NOW())\r\n    RETURNING id INTO v_order_id;\r\n    \r\n    -- Step 3: Batch insert order items and update inventory\r\n    INSERT INTO order_items (order_id, product_id, quantity, unit_price)\r\n    SELECT \r\n        v_order_id,\r\n        (item->>'product_id')::INT,\r\n        (item->>'quantity')::INT,\r\n        p.price\r\n    FROM jsonb_array_elements(p_items) AS item\r\n    JOIN products p ON p.id = (item->>'product_id')::INT;\r\n    \r\n    -- Step 4: Update inventory in batch\r\n    UPDATE inventory \r\n    SET available_quantity = available_quantity - subquery.quantity\r\n    FROM (\r\n        SELECT \r\n            (item->>'product_id')::INT as product_id,\r\n            (item->>'quantity')::INT as quantity\r\n        FROM jsonb_array_elements(p_items) AS item\r\n    ) AS subquery\r\n    WHERE inventory.product_id = subquery.product_id;\r\n    \r\n    -- Step 5: Calculate total\r\n    SELECT SUM(oi.quantity * oi.unit_price) INTO v_total\r\n    FROM order_items oi\r\n    WHERE oi.order_id = v_order_id;\r\n    \r\n    UPDATE orders SET total_amount = v_total WHERE id = v_order_id;\r\n    \r\n    RETURN json_build_object(\r\n        'success', true,\r\n        'order_id', v_order_id,\r\n        'total', v_total\r\n    );\r\nEND;\r\n$$ LANGUAGE plpgsql;\r\n\r\n-- Result: Checkout time reduced from 2.3s to 0.3s (87% improvement)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Results:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Search performance: 98% improvement (4.2s → 0.08s)"}),"\n",(0,t.jsx)(e.li,{children:"Checkout performance: 87% improvement (2.3s → 0.3s)"}),"\n",(0,t.jsx)(e.li,{children:"Database CPU utilization: 90% → 45%"}),"\n",(0,t.jsx)(e.li,{children:"Peak hour success rate: 85% → 99.5%"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Case Study 2: Social Media Analytics Platform"}),"\n",(0,t.jsx)(e.h4,{children:"The Challenge"}),"\n",(0,t.jsx)(e.p,{children:"A social media analytics platform processing 100M events/day faced:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Real-time dashboard queries taking 15+ seconds"}),"\n",(0,t.jsx)(e.li,{children:"ETL processes blocking user queries"}),"\n",(0,t.jsx)(e.li,{children:"Reporting queries causing memory issues"}),"\n",(0,t.jsx)(e.li,{children:"Unable to scale beyond current load"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{children:"The Solution"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 1: Time-Series Data Optimization"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Original events table (100M+ rows)\r\nCREATE TABLE events (\r\n    id BIGSERIAL PRIMARY KEY,\r\n    user_id BIGINT,\r\n    event_type VARCHAR(50),\r\n    platform VARCHAR(20),\r\n    timestamp TIMESTAMP,\r\n    metadata JSONB,\r\n    processed_at TIMESTAMP DEFAULT NOW()\r\n);\r\n\r\n-- Problem: Single massive table, no partitioning, slow aggregations\r\n\r\n-- Solution: Partitioned time-series design\r\nCREATE TABLE events_partitioned (\r\n    id BIGINT,\r\n    user_id BIGINT,\r\n    event_type VARCHAR(50),\r\n    platform VARCHAR(20),\r\n    timestamp TIMESTAMP,\r\n    metadata JSONB,\r\n    processed_at TIMESTAMP DEFAULT NOW()\r\n) PARTITION BY RANGE (timestamp);\r\n\r\n-- Create monthly partitions\r\nCREATE TABLE events_2024_01 PARTITION OF events_partitioned\r\n    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\r\nCREATE TABLE events_2024_02 PARTITION OF events_partitioned\r\n    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\r\n-- ... continue for each month\r\n\r\n-- Indexes per partition\r\nCREATE INDEX idx_events_2024_01_user_type ON events_2024_01(user_id, event_type, timestamp);\r\nCREATE INDEX idx_events_2024_01_platform_time ON events_2024_01(platform, timestamp);\r\n\r\n-- Automated partition management\r\nCREATE OR REPLACE FUNCTION create_monthly_partition(target_date DATE)\r\nRETURNS VOID AS $$\r\nDECLARE\r\n    partition_name TEXT;\r\n    start_date DATE;\r\n    end_date DATE;\r\nBEGIN\r\n    start_date := date_trunc('month', target_date);\r\n    end_date := start_date + INTERVAL '1 month';\r\n    partition_name := 'events_' || to_char(start_date, 'YYYY_MM');\r\n    \r\n    EXECUTE format('\r\n        CREATE TABLE %I PARTITION OF events_partitioned\r\n        FOR VALUES FROM (%L) TO (%L)',\r\n        partition_name, start_date, end_date\r\n    );\r\n    \r\n    -- Create indexes\r\n    EXECUTE format('\r\n        CREATE INDEX %I ON %I(user_id, event_type, timestamp)',\r\n        'idx_' || partition_name || '_user_type', partition_name\r\n    );\r\n    \r\n    EXECUTE format('\r\n        CREATE INDEX %I ON %I(platform, timestamp)',\r\n        'idx_' || partition_name || '_platform_time', partition_name\r\n    );\r\nEND;\r\n$$ LANGUAGE plpgsql;\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 2: Materialized Views for Analytics"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create materialized views for common aggregations\r\nCREATE MATERIALIZED VIEW hourly_event_stats AS\r\nSELECT \r\n    date_trunc('hour', timestamp) as hour,\r\n    platform,\r\n    event_type,\r\n    COUNT(*) as event_count,\r\n    COUNT(DISTINCT user_id) as unique_users\r\nFROM events_partitioned\r\nWHERE timestamp >= CURRENT_DATE - INTERVAL '30 days'\r\nGROUP BY date_trunc('hour', timestamp), platform, event_type;\r\n\r\nCREATE INDEX idx_hourly_stats_time_platform ON hourly_event_stats(hour, platform);\r\n\r\n-- Automated refresh\r\nCREATE OR REPLACE FUNCTION refresh_hourly_stats()\r\nRETURNS VOID AS $$\r\nBEGIN\r\n    REFRESH MATERIALIZED VIEW CONCURRENTLY hourly_event_stats;\r\nEND;\r\n$$ LANGUAGE plpgsql;\r\n\r\n-- Schedule refresh every hour\r\nSELECT cron.schedule('refresh-hourly-stats', '0 * * * *', 'SELECT refresh_hourly_stats();');\r\n\r\n-- Daily aggregations\r\nCREATE MATERIALIZED VIEW daily_platform_stats AS\r\nSELECT \r\n    date_trunc('day', timestamp) as day,\r\n    platform,\r\n    COUNT(*) as total_events,\r\n    COUNT(DISTINCT user_id) as daily_active_users,\r\n    COUNT(DISTINCT user_id) FILTER (WHERE event_type = 'login') as login_users\r\nFROM events_partitioned\r\nWHERE timestamp >= CURRENT_DATE - INTERVAL '365 days'\r\nGROUP BY date_trunc('day', timestamp), platform;\r\n\r\n-- Fast dashboard queries\r\nSELECT \r\n    platform,\r\n    SUM(total_events) as events_last_7_days,\r\n    AVG(daily_active_users) as avg_daily_users\r\nFROM daily_platform_stats\r\nWHERE day >= CURRENT_DATE - INTERVAL '7 days'\r\nGROUP BY platform;\r\n\r\n-- Result: Dashboard query time 15s → 0.2s (99% improvement)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 3: Columnar Storage for Analytics"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Create columnar table for heavy analytics\r\n-- (Using Citus columnar extension)\r\nCREATE TABLE events_analytics (\r\n    user_id BIGINT,\r\n    event_type VARCHAR(50),\r\n    platform VARCHAR(20),\r\n    event_date DATE,\r\n    event_hour INT,\r\n    metadata_category VARCHAR(100),\r\n    session_duration INT\r\n) USING columnar;\r\n\r\n-- ETL process to populate columnar table\r\nINSERT INTO events_analytics\r\nSELECT \r\n    user_id,\r\n    event_type,\r\n    platform,\r\n    DATE(timestamp) as event_date,\r\n    EXTRACT(HOUR FROM timestamp) as event_hour,\r\n    metadata->>'category' as metadata_category,\r\n    CASE \r\n        WHEN event_type = 'session_end' \r\n        THEN (metadata->>'duration')::INT \r\n        ELSE NULL \r\n    END as session_duration\r\nFROM events_partitioned\r\nWHERE DATE(timestamp) = CURRENT_DATE - INTERVAL '1 day';\r\n\r\n-- Complex analytics queries now run much faster\r\nSELECT \r\n    platform,\r\n    event_date,\r\n    COUNT(*) as events,\r\n    COUNT(DISTINCT user_id) as unique_users,\r\n    AVG(session_duration) FILTER (WHERE session_duration IS NOT NULL) as avg_session\r\nFROM events_analytics\r\nWHERE event_date >= CURRENT_DATE - INTERVAL '30 days'\r\nGROUP BY platform, event_date\r\nORDER BY platform, event_date;\r\n\r\n-- Result: Complex analytics queries 45s → 3s (93% improvement)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Results:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Dashboard performance: 99% improvement (15s → 0.2s)"}),"\n",(0,t.jsx)(e.li,{children:"Complex analytics: 93% improvement (45s → 3s)"}),"\n",(0,t.jsx)(e.li,{children:"ETL impact on user queries: Eliminated"}),"\n",(0,t.jsx)(e.li,{children:"System scalability: 3x increase in throughput"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Case Study 3: Financial Services Transaction Processing"}),"\n",(0,t.jsx)(e.h4,{children:"The Challenge"}),"\n",(0,t.jsx)(e.p,{children:"A fintech company processing 50M transactions/day experienced:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Fraud detection queries timing out"}),"\n",(0,t.jsx)(e.li,{children:"Account balance calculations taking minutes"}),"\n",(0,t.jsx)(e.li,{children:"Compliance reports causing system outages"}),"\n",(0,t.jsx)(e.li,{children:"Unable to provide real-time balance updates"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{children:"The Solution"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 1: Transaction Processing Optimization"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Original design issues:\r\n-- 1. All transactions in single table\r\n-- 2. Balance calculated by summing all transactions\r\n-- 3. No proper indexing for fraud detection patterns\r\n\r\n-- Solution: Event sourcing with balance snapshots\r\nCREATE TABLE transactions (\r\n    id BIGSERIAL PRIMARY KEY,\r\n    account_id BIGINT,\r\n    transaction_type VARCHAR(20),\r\n    amount DECIMAL(15,2),\r\n    currency VARCHAR(3),\r\n    timestamp TIMESTAMP DEFAULT NOW(),\r\n    reference_id VARCHAR(100),\r\n    merchant_id BIGINT,\r\n    category VARCHAR(50),\r\n    metadata JSONB\r\n);\r\n\r\n-- Partition by timestamp for efficient querying\r\nCREATE TABLE transactions_partitioned (\r\n    LIKE transactions INCLUDING ALL\r\n) PARTITION BY RANGE (timestamp);\r\n\r\n-- Create account balance cache\r\nCREATE TABLE account_balances (\r\n    account_id BIGINT PRIMARY KEY,\r\n    current_balance DECIMAL(15,2),\r\n    available_balance DECIMAL(15,2),\r\n    last_transaction_id BIGINT,\r\n    last_updated TIMESTAMP DEFAULT NOW()\r\n);\r\n\r\n-- Indexes for fraud detection\r\nCREATE INDEX idx_transactions_account_time ON transactions_partitioned(account_id, timestamp);\r\nCREATE INDEX idx_transactions_merchant_amount ON transactions_partitioned(merchant_id, amount, timestamp);\r\nCREATE INDEX idx_transactions_amount_time ON transactions_partitioned(amount, timestamp) WHERE amount > 1000;\r\nCREATE INDEX idx_transactions_velocity ON transactions_partitioned(account_id, timestamp, amount);\r\n\r\n-- Real-time balance update trigger\r\nCREATE OR REPLACE FUNCTION update_account_balance()\r\nRETURNS TRIGGER AS $$\r\nBEGIN\r\n    INSERT INTO account_balances (account_id, current_balance, available_balance, last_transaction_id)\r\n    VALUES (\r\n        NEW.account_id,\r\n        NEW.amount,\r\n        NEW.amount,\r\n        NEW.id\r\n    )\r\n    ON CONFLICT (account_id)\r\n    DO UPDATE SET\r\n        current_balance = account_balances.current_balance + NEW.amount,\r\n        available_balance = account_balances.available_balance + NEW.amount,\r\n        last_transaction_id = NEW.id,\r\n        last_updated = NOW();\r\n    \r\n    RETURN NEW;\r\nEND;\r\n$$ LANGUAGE plpgsql;\r\n\r\nCREATE TRIGGER tr_update_balance\r\n    AFTER INSERT ON transactions_partitioned\r\n    FOR EACH ROW\r\n    EXECUTE FUNCTION update_account_balance();\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 2: Fraud Detection Optimization"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Fraud detection patterns\r\nCREATE MATERIALIZED VIEW fraud_detection_patterns AS\r\nSELECT \r\n    account_id,\r\n    date_trunc('hour', timestamp) as hour,\r\n    COUNT(*) as transaction_count,\r\n    SUM(amount) as total_amount,\r\n    COUNT(DISTINCT merchant_id) as unique_merchants,\r\n    MAX(amount) as max_transaction,\r\n    stddev(amount) as amount_stddev\r\nFROM transactions_partitioned\r\nWHERE timestamp >= NOW() - INTERVAL '24 hours'\r\nGROUP BY account_id, date_trunc('hour', timestamp);\r\n\r\n-- Real-time fraud scoring function\r\nCREATE OR REPLACE FUNCTION calculate_fraud_score(\r\n    p_account_id BIGINT,\r\n    p_amount DECIMAL,\r\n    p_merchant_id BIGINT\r\n) RETURNS DECIMAL AS $$\r\nDECLARE\r\n    v_score DECIMAL := 0;\r\n    v_hourly_count INT;\r\n    v_hourly_amount DECIMAL;\r\n    v_avg_transaction DECIMAL;\r\n    v_merchant_history INT;\r\nBEGIN\r\n    -- Check transaction velocity\r\n    SELECT COUNT(*), COALESCE(SUM(amount), 0)\r\n    INTO v_hourly_count, v_hourly_amount\r\n    FROM transactions_partitioned\r\n    WHERE account_id = p_account_id\r\n      AND timestamp >= NOW() - INTERVAL '1 hour';\r\n    \r\n    -- Score based on velocity\r\n    IF v_hourly_count > 10 THEN v_score := v_score + 20; END IF;\r\n    IF v_hourly_amount > 10000 THEN v_score := v_score + 30; END IF;\r\n    \r\n    -- Check merchant history\r\n    SELECT COUNT(*)\r\n    INTO v_merchant_history\r\n    FROM transactions_partitioned\r\n    WHERE account_id = p_account_id\r\n      AND merchant_id = p_merchant_id\r\n      AND timestamp >= NOW() - INTERVAL '30 days';\r\n    \r\n    -- New merchant penalty\r\n    IF v_merchant_history = 0 AND p_amount > 500 THEN\r\n        v_score := v_score + 25;\r\n    END IF;\r\n    \r\n    -- Amount pattern analysis\r\n    SELECT AVG(amount)\r\n    INTO v_avg_transaction\r\n    FROM transactions_partitioned\r\n    WHERE account_id = p_account_id\r\n      AND timestamp >= NOW() - INTERVAL '30 days';\r\n    \r\n    -- Unusual amount penalty\r\n    IF p_amount > v_avg_transaction * 5 THEN\r\n        v_score := v_score + 40;\r\n    END IF;\r\n    \r\n    RETURN v_score;\r\nEND;\r\n$$ LANGUAGE plpgsql;\r\n\r\n-- Fast fraud check during transaction processing\r\nSELECT \r\n    *,\r\n    calculate_fraud_score(account_id, amount, merchant_id) as fraud_score\r\nFROM transactions_partitioned\r\nWHERE id = NEW.id;\r\n\r\n-- Result: Fraud detection time 30s → 0.1s (99.7% improvement)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Phase 3: Compliance Reporting Optimization"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Pre-aggregated compliance data\r\nCREATE TABLE daily_transaction_summary (\r\n    account_id BIGINT,\r\n    transaction_date DATE,\r\n    transaction_count INT,\r\n    total_inflow DECIMAL(15,2),\r\n    total_outflow DECIMAL(15,2),\r\n    max_single_transaction DECIMAL(15,2),\r\n    suspicious_activity_count INT,\r\n    PRIMARY KEY (account_id, transaction_date)\r\n);\r\n\r\n-- Automated daily aggregation\r\nCREATE OR REPLACE FUNCTION generate_daily_summary(target_date DATE)\r\nRETURNS VOID AS $$\r\nBEGIN\r\n    INSERT INTO daily_transaction_summary\r\n    SELECT \r\n        account_id,\r\n        DATE(timestamp) as transaction_date,\r\n        COUNT(*) as transaction_count,\r\n        SUM(CASE WHEN amount > 0 THEN amount ELSE 0 END) as total_inflow,\r\n        SUM(CASE WHEN amount < 0 THEN ABS(amount) ELSE 0 END) as total_outflow,\r\n        MAX(ABS(amount)) as max_single_transaction,\r\n        COUNT(*) FILTER (WHERE ABS(amount) > 10000) as suspicious_activity_count\r\n    FROM transactions_partitioned\r\n    WHERE DATE(timestamp) = target_date\r\n    GROUP BY account_id, DATE(timestamp)\r\n    ON CONFLICT (account_id, transaction_date)\r\n    DO UPDATE SET\r\n        transaction_count = EXCLUDED.transaction_count,\r\n        total_inflow = EXCLUDED.total_inflow,\r\n        total_outflow = EXCLUDED.total_outflow,\r\n        max_single_transaction = EXCLUDED.max_single_transaction,\r\n        suspicious_activity_count = EXCLUDED.suspicious_activity_count;\r\nEND;\r\n$$ LANGUAGE plpgsql;\r\n\r\n-- Fast compliance reporting\r\nSELECT \r\n    account_id,\r\n    SUM(total_inflow) as monthly_inflow,\r\n    SUM(total_outflow) as monthly_outflow,\r\n    MAX(max_single_transaction) as largest_transaction,\r\n    SUM(suspicious_activity_count) as total_suspicious\r\nFROM daily_transaction_summary\r\nWHERE transaction_date >= date_trunc('month', CURRENT_DATE)\r\n  AND transaction_date < date_trunc('month', CURRENT_DATE) + INTERVAL '1 month'\r\nGROUP BY account_id\r\nHAVING SUM(total_inflow) > 100000  -- Accounts with high activity\r\nORDER BY monthly_inflow DESC;\r\n\r\n-- Result: Compliance report generation 2 hours → 5 minutes (96% improvement)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Results:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Balance calculation: 2 minutes → 0.01s (99.99% improvement)"}),"\n",(0,t.jsx)(e.li,{children:"Fraud detection: 30s → 0.1s (99.7% improvement)"}),"\n",(0,t.jsx)(e.li,{children:"Compliance reports: 2 hours → 5 minutes (96% improvement)"}),"\n",(0,t.jsx)(e.li,{children:"Real-time balance updates: Achieved"}),"\n",(0,t.jsx)(e.li,{children:"System availability: 99.9% → 99.99%"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{children:"Migration Strategies"}),"\n",(0,t.jsx)(e.h3,{children:"Zero-Downtime Index Creation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- PostgreSQL: Concurrent index creation\r\nCREATE INDEX CONCURRENTLY idx_users_email_new ON users(email);\r\n\r\n-- Rename old index and activate new one\r\nBEGIN;\r\nALTER INDEX idx_users_email RENAME TO idx_users_email_old;\r\nALTER INDEX idx_users_email_new RENAME TO idx_users_email;\r\nCOMMIT;\r\n\r\n-- Drop old index\r\nDROP INDEX idx_users_email_old;\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- SQL Server: Online index operations\r\nCREATE INDEX idx_users_email_new ON users(email)\r\nWITH (ONLINE = ON, SORT_IN_TEMPDB = ON);\r\n\r\n-- Switch indexes atomically\r\nBEGIN TRANSACTION;\r\nEXEC sp_rename 'users.idx_users_email', 'idx_users_email_old', 'INDEX';\r\nEXEC sp_rename 'users.idx_users_email_new', 'idx_users_email', 'INDEX';\r\nCOMMIT;\r\n\r\nDROP INDEX idx_users_email_old ON users;\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Schema Migration Best Practices"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Database migration script with rollback\r\nclass DatabaseMigration:\r\n    def __init__(self, connection):\r\n        self.conn = connection\r\n        \r\n    def migrate_with_rollback(self):\r\n        savepoint_name = f"migration_{int(time.time())}"\r\n        \r\n        try:\r\n            # Create savepoint\r\n            self.conn.execute(f"SAVEPOINT {savepoint_name}")\r\n            \r\n            # Step 1: Create new indexes\r\n            self.create_new_indexes()\r\n            \r\n            # Step 2: Verify performance\r\n            if not self.verify_performance():\r\n                raise Exception("Performance verification failed")\r\n            \r\n            # Step 3: Drop old indexes\r\n            self.drop_old_indexes()\r\n            \r\n            # Step 4: Update statistics\r\n            self.update_statistics()\r\n            \r\n            print("Migration completed successfully")\r\n            \r\n        except Exception as e:\r\n            print(f"Migration failed: {e}")\r\n            self.conn.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name}")\r\n            print("Migration rolled back")\r\n            raise\r\n    \r\n    def create_new_indexes(self):\r\n        indexes = [\r\n            "CREATE INDEX CONCURRENTLY idx_orders_customer_date_new ON orders(customer_id, order_date)",\r\n            "CREATE INDEX CONCURRENTLY idx_products_category_price_new ON products(category_id, price)",\r\n        ]\r\n        \r\n        for index_sql in indexes:\r\n            print(f"Creating index: {index_sql}")\r\n            self.conn.execute(index_sql)\r\n    \r\n    def verify_performance(self):\r\n        # Run test queries and verify performance\r\n        test_queries = [\r\n            ("SELECT * FROM orders WHERE customer_id = 1000 ORDER BY order_date", 0.1),\r\n            ("SELECT * FROM products WHERE category_id = 5 AND price > 100", 0.05),\r\n        ]\r\n        \r\n        for query, max_time in test_queries:\r\n            start_time = time.time()\r\n            self.conn.execute(query)\r\n            execution_time = time.time() - start_time\r\n            \r\n            if execution_time > max_time:\r\n                print(f"Query too slow: {execution_time}s > {max_time}s")\r\n                return False\r\n        \r\n        return True\n'})}),"\n",(0,t.jsx)(e.h2,{children:"Troubleshooting Guide"}),"\n",(0,t.jsx)(e.h3,{children:"Common Performance Issues"}),"\n",(0,t.jsx)(e.h4,{children:"Issue 1: Query Suddenly Became Slow"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Diagnostic steps:\r\n\r\n-- 1. Check for missing statistics\r\nSELECT \r\n    schemaname,\r\n    tablename,\r\n    last_analyze,\r\n    n_tup_ins + n_tup_upd + n_tup_del as total_changes\r\nFROM pg_stat_user_tables\r\nWHERE last_analyze < NOW() - INTERVAL '1 week'\r\nORDER BY total_changes DESC;\r\n\r\n-- 2. Check for index bloat\r\nSELECT \r\n    schemaname,\r\n    tablename,\r\n    indexname,\r\n    pg_size_pretty(pg_relation_size(indexrelid)) as size,\r\n    idx_scan,\r\n    idx_tup_read\r\nFROM pg_stat_user_indexes\r\nWHERE idx_scan = 0 \r\n  AND pg_relation_size(indexrelid) > 1000000  -- 1MB+\r\nORDER BY pg_relation_size(indexrelid) DESC;\r\n\r\n-- 3. Check for lock contention\r\nSELECT \r\n    mode,\r\n    locktype,\r\n    database,\r\n    relation,\r\n    page,\r\n    tuple,\r\n    classid,\r\n    granted,\r\n    pid\r\nFROM pg_locks\r\nWHERE NOT granted;\r\n\r\n-- Solutions:\r\n-- 1. Update statistics: ANALYZE table_name;\r\n-- 2. Rebuild bloated indexes: REINDEX INDEX index_name;\r\n-- 3. Identify blocking queries and optimize them\n"})}),"\n",(0,t.jsx)(e.h4,{children:"Issue 2: High CPU Usage"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-sql",children:"-- Find expensive queries\r\nSELECT \r\n    query,\r\n    calls,\r\n    total_time / calls as avg_time,\r\n    rows / calls as avg_rows,\r\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\r\nFROM pg_stat_statements \r\nORDER BY total_time DESC\r\nLIMIT 10;\r\n\r\n-- Check for sequential scans on large tables\r\nSELECT \r\n    schemaname,\r\n    tablename,\r\n    seq_scan,\r\n    seq_tup_read,\r\n    seq_tup_read / GREATEST(seq_scan, 1) as avg_seq_read,\r\n    n_tup_ins + n_tup_upd + n_tup_del as total_writes\r\nFROM pg_stat_user_tables\r\nWHERE seq_scan > 100\r\n  AND seq_tup_read / GREATEST(seq_scan, 1) > 10000\r\nORDER BY seq_tup_read DESC;\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Index Optimization Checklist"}),"\n",(0,t.jsx)(e.h4,{children:"Pre-Implementation Checklist"}),"\n",(0,t.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Analyze current query patterns using query logs"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Identify slow queries with EXPLAIN ANALYZE"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Check existing index usage statistics"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Estimate index size and maintenance overhead"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Plan for index creation during low-traffic periods"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Prepare rollback procedures"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{children:"Post-Implementation Checklist"}),"\n",(0,t.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Monitor query performance improvements"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Check index usage statistics"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Verify no regression in write performance"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Monitor disk space usage"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Update documentation"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Schedule regular index maintenance"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Production Deployment Guidelines"}),"\n",(0,t.jsx)(e.h4,{children:"Deployment Strategy"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Test Environment"}),": Replicate production data volume and query patterns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Staging Deployment"}),": Deploy to staging with production-like traffic"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Canary Deployment"}),": Deploy to subset of production servers"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Full Deployment"}),": Roll out to all production servers"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Monitor and Optimize"}),": Continuous monitoring and adjustment"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{children:"Monitoring Checklist"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:'#!/bin/bash\r\n# Production index monitoring script\r\n\r\nDB_NAME="production_db"\r\nALERT_EMAIL="ops-team@company.com"\r\nLOG_FILE="/var/log/db-index-monitor.log"\r\n\r\n# Check for slow queries\r\nSLOW_QUERIES=$(psql -d $DB_NAME -t -c "\r\nSELECT COUNT(*) \r\nFROM pg_stat_statements \r\nWHERE mean_time > 1000  -- Queries taking more than 1 second\r\n")\r\n\r\nif [ "$SLOW_QUERIES" -gt 5 ]; then\r\n    echo "$(date): WARNING: $SLOW_QUERIES slow queries detected" >> $LOG_FILE\r\n    # Send alert email\r\nfi\r\n\r\n# Check for unused indexes\r\nUNUSED_INDEXES=$(psql -d $DB_NAME -t -c "\r\nSELECT COUNT(*) \r\nFROM pg_stat_user_indexes \r\nWHERE idx_scan = 0 \r\n  AND pg_relation_size(indexrelid) > 100000000  -- 100MB+\r\n")\r\n\r\nif [ "$UNUSED_INDEXES" -gt 0 ]; then\r\n    echo "$(date): INFO: $UNUSED_INDEXES large unused indexes found" >> $LOG_FILE\r\nfi\r\n\r\n# Check index fragmentation (example for SQL Server)\r\n# Adapt for your database system\r\n\r\necho "$(date): Index monitoring completed" >> $LOG_FILE\n'})}),"\n",(0,t.jsx)(e.h2,{children:"Best Practices Summary"}),"\n",(0,t.jsx)(e.h3,{children:"Design Principles"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Understand Your Workload"}),": OLTP vs OLAP vs Mixed workloads require different strategies"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Start Simple"}),": Begin with basic indexes, optimize based on actual usage patterns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Measure Everything"}),": Use query analysis tools and performance monitoring"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Test Thoroughly"}),": Always test index changes in production-like environments"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Implementation Guidelines"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Index Selectivity"}),": Create indexes on high-selectivity columns first"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Composite Index Order"}),": Follow the ESR rule (Equality, Sort, Range)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Covering Indexes"}),": Include frequently accessed columns to avoid table lookups"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Maintenance Windows"}),": Plan index operations during low-traffic periods"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Monitoring and Maintenance"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Regular Health Checks"}),": Monitor index usage, fragmentation, and performance"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Automated Maintenance"}),": Set up automated statistics updates and index rebuilding"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Capacity Planning"}),": Monitor index growth and plan for storage requirements"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Documentation"}),": Keep detailed records of index changes and their impact"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Performance Optimization"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Query Optimization"}),": Optimize queries to make effective use of indexes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Connection Management"}),": Use connection pooling and proper timeout settings"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Caching Strategies"}),": Implement appropriate caching at multiple levels"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Read Replicas"}),": Use read replicas to distribute read workload"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{children:"Conclusion"}),"\n",(0,t.jsx)(e.p,{children:"Database indexing is a critical skill for building high-performance applications. This comprehensive series has covered:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fundamentals"}),": Index types, structures, and core concepts"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"SQL Databases"}),": Advanced indexing across MySQL, PostgreSQL, SQL Server, and Oracle"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"NoSQL Systems"}),": Indexing strategies for MongoDB, Cassandra, Redis, and others"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Advanced Techniques"}),": Composite indexes, partitioning, and specialized index types"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Monitoring"}),": Performance tracking, automated maintenance, and health monitoring"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Advanced Features"}),": Columnar storage, vector indexes, and big data strategies"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Client Optimization"}),": Connection pooling, caching, and application-level optimization"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-World Cases"}),": Production examples with measurable performance improvements"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"The key to success is understanding your specific workload, measuring performance systematically, and iterating based on real-world results. Index optimization is an ongoing process that requires continuous monitoring and adjustment as your application grows and evolves."}),"\n",(0,t.jsx)(e.p,{children:"Remember: the best index strategy is one that's tailored to your specific use case, properly tested, and continuously monitored for effectiveness."})]})}function o(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}={...(0,s.a)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(i,{...n})}):i(n)}},2036:function(n,e,r){r.r(e),r.d(e,{default:function(){return o},frontmatter:function(){return a}});var t=r(7437),s=r(4229);let a=void 0;function i(n){let e={blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{children:"Why Use Hash Tables?"}),"\n",(0,t.jsx)(e.p,{children:"Hash tables are ideal for scenarios where you need to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Quickly look up values by a unique key (e.g., username \xe2†’ user profile)"}),"\n",(0,t.jsx)(e.li,{children:"Count occurrences of items (e.g., word frequency in a document)"}),"\n",(0,t.jsx)(e.li,{children:"Implement sets, caches, or associative arrays"}),"\n",(0,t.jsx)(e.li,{children:"Index data for fast retrieval (e.g., database indexes, symbol tables in compilers)"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Example Applications:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Caching web pages or database queries"}),"\n",(0,t.jsxs)(e.li,{children:["Implementing sets/maps in programming languages (e.g., Python's ",(0,t.jsx)(e.code,{children:"dict"}),", JavaScript's ",(0,t.jsx)(e.code,{children:"Object"}),"/",(0,t.jsx)(e.code,{children:"Map"}),")"]}),"\n",(0,t.jsx)(e.li,{children:"Counting unique visitors or items"}),"\n",(0,t.jsx)(e.li,{children:"Storing configuration or environment variables"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{children:"Anatomy of a Hash Table"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hash Function"}),": Transforms keys into array indices. A robust function minimizes collisions and distributes keys uniformly."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Buckets / Slots"}),": Underlying array where values reside."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Collision Resolution"}),": Techniques like chaining or open addressing to handle index conflicts."]}),"\n"]}),"\n",(0,t.jsx)("img",{src:"./assets/anatomy.png",alt:"Hash table components",className:"w-full my-6 rounded"}),"\n",(0,t.jsx)(e.h2,{children:"How Hash Functions Work"}),"\n",(0,t.jsx)(e.p,{children:"A hash function takes an input (the key) and returns an integer (the hash code), which is then mapped to an index in the underlying array. Good hash functions:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Are deterministic (same input always gives same output)"}),"\n",(0,t.jsx)(e.li,{children:"Distribute keys uniformly to minimize clustering"}),"\n",(0,t.jsx)(e.li,{children:"Are fast to compute"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Example: Simple Modulo Hash"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"function simpleHash(key, tableSize) {\n  let hash = 0;\n  for (let char of key) {\n    hash = (hash * 31 + char.charCodeAt(0)) % tableSize;\n  }\n  return hash;\n}\n"})}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:"The choice of multiplier (e.g., 31) affects distribution; primes often yield better spreads."}),"\n"]}),"\n",(0,t.jsx)(e.h3,{children:"Real-World Hash Functions"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"MurmurHash, CityHash, FNV-1a"}),": Used in production systems for better distribution and speed."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cryptographic hashes (SHA-256, MD5)"}),": Used for security, not for hash tables (too slow)."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{children:"Handling Collisions"}),"\n",(0,t.jsx)(e.p,{children:"When two keys hash to the same index, a collision occurs. There are two main strategies:"}),"\n",(0,t.jsx)(e.h3,{children:"Chaining"}),"\n",(0,t.jsx)(e.p,{children:"Each bucket holds a list of entries. Collisions are handled by appending to the list."}),"\n",(0,t.jsx)("img",{src:"./assets/chaining.png",alt:"Chaining illustration",className:"w-full my-6 rounded"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"class HashTableChain {\n  constructor(size = 42) {\n    this.buckets = Array.from({ length: size }, () => []);\n  }\n\n  insert(key, value) {\n    const index = simpleHash(key, this.buckets.length);\n    this.buckets[index].push([key, value]);\n  }\n\n  // ...existing code...\n}\n"})}),"\n",(0,t.jsx)(e.h3,{children:"Open Addressing (Linear Probing)"}),"\n",(0,t.jsx)(e.p,{children:"All entries are stored in the array itself. On collision, the algorithm searches for the next available slot."}),"\n","\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-javascript",children:"class HashTableProbing {\n  constructor(size = 42) {\n    this.table = new Array(size).fill(null);\n  }\n\n  // ...existing code...\n}\n"})}),"\n",(0,t.jsx)(e.h2,{children:"Example Scenario: Username Lookup"}),"\n",(0,t.jsx)(e.p,{children:"Suppose you want to check if a username is taken:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Hash the username to get an index."}),"\n",(0,t.jsx)(e.li,{children:"Check the bucket (or slot) at that index."}),"\n",(0,t.jsx)(e.li,{children:"If found, the username is taken; otherwise, it's available."}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"This operation is extremely fast, even with thousands or millions of users."}),"\n",(0,t.jsx)(e.h2,{children:"Performance Analysis"}),"\n",(0,t.jsxs)("table",{className:"w-full my-12 border-collapse",children:[(0,t.jsx)("thead",{children:(0,t.jsxs)("tr",{children:[(0,t.jsx)("th",{className:"bg-gray-50 border border-gray-200 px-6 py-4 text-left font-semibold text-gray-900",children:"Operation"}),(0,t.jsx)("th",{className:"bg-gray-50 border border-gray-200 px-6 py-4 text-left font-semibold text-gray-900",children:"Average Case"}),(0,t.jsx)("th",{className:"bg-gray-50 border border-gray-200 px-6 py-4 text-left font-semibold text-gray-900",children:"Worst Case"}),(0,t.jsx)("th",{className:"bg-gray-50 border border-gray-200 px-6 py-4 text-left font-semibold text-gray-900",children:"Explanation"})]})}),(0,t.jsxs)("tbody",{children:[(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"Search"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"O(1)"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"O(n)"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"Constant time if well-distributed; O(n) if all keys collide"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"Insert"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"O(1)"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"O(n)"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"Usually constant, but can degrade with many collisions"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"Delete"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"O(1)"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"O(n)"}),(0,t.jsx)("td",{className:"border border-gray-200 px-6 py-4 text-gray-700",children:"Same as above"})]})]})]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Average Case"}),": With a good hash function and low load factor, operations are nearly instantaneous."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Worst Case"}),": If many keys collide (poor hash function or overloaded table), performance degrades to linear time."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{children:"Conclusion"}),"\n",(0,t.jsx)(e.p,{children:"Well-implemented hash tables power applications that require rapid lookups, from caching layers to in-memory databases. Selecting the right collision strategy and hash function is key to maintaining high performance."})]})}function o(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}={...(0,s.a)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(i,{...n})}):i(n)}}}]);