"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[2721],{163:function(e,n,r){r.r(n),r.d(n,{default:function(){return o},frontmatter:function(){return t}});var i=r(7437),s=r(4229);let t=void 0;function l(e){let n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{children:"Design a Social Media Feed (Twitter)"}),"\n",(0,i.jsx)(n.p,{children:"In this part, we'll design a social media feed system like Twitter. This problem introduces complex challenges around content ranking, timeline generation, viral content handling, and personalized content delivery."}),"\n",(0,i.jsx)(n.h2,{children:"1. Functional Requirements"}),"\n",(0,i.jsx)(n.h3,{children:"Actors"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User"}),": Posts and consumes content"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Content Creator"}),": Influential users with many followers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Content Moderator"}),": Reviews flagged content"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System"}),": Manages recommendations and trending"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Use Cases"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"User"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Post tweets (text, images, videos)"}),"\n",(0,i.jsx)(n.li,{children:"Follow/unfollow other users"}),"\n",(0,i.jsx)(n.li,{children:"View personalized timeline"}),"\n",(0,i.jsx)(n.li,{children:"Like, retweet, and comment on posts"}),"\n",(0,i.jsx)(n.li,{children:"Search for tweets and users"}),"\n",(0,i.jsx)(n.li,{children:"View trending topics"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Content Creator"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Publish content to large audiences"}),"\n",(0,i.jsx)(n.li,{children:"View analytics and engagement metrics"}),"\n",(0,i.jsx)(n.li,{children:"Promote content"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Content Moderator"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Review reported content"}),"\n",(0,i.jsx)(n.li,{children:"Take action on policy violations"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Functional Requirements"}),"\n",(0,i.jsxs)(n.p,{children:["✅ ",(0,i.jsx)(n.strong,{children:"In Scope"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Post tweets (280 characters, media support)"}),"\n",(0,i.jsx)(n.li,{children:"Follow/unfollow users"}),"\n",(0,i.jsx)(n.li,{children:"Home timeline (personalized feed)"}),"\n",(0,i.jsx)(n.li,{children:"User timeline (user's own tweets)"}),"\n",(0,i.jsx)(n.li,{children:"Like, retweet, reply functionality"}),"\n",(0,i.jsx)(n.li,{children:"Trending topics and hashtags"}),"\n",(0,i.jsx)(n.li,{children:"Search functionality"}),"\n",(0,i.jsx)(n.li,{children:"Basic analytics"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["❌ ",(0,i.jsx)(n.strong,{children:"Out of Scope"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Direct messaging (covered in Part 3)"}),"\n",(0,i.jsx)(n.li,{children:"Live streaming"}),"\n",(0,i.jsx)(n.li,{children:"Advanced recommendation algorithms"}),"\n",(0,i.jsx)(n.li,{children:"Advertisement system"}),"\n",(0,i.jsx)(n.li,{children:"Advanced analytics dashboard"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"2. Non-Functional Requirements"}),"\n",(0,i.jsx)(n.h3,{children:"Scalability"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Support 500 million users"}),"\n",(0,i.jsx)(n.li,{children:"Handle 300 million tweets per day"}),"\n",(0,i.jsx)(n.li,{children:"Support 100 million daily active users"}),"\n",(0,i.jsx)(n.li,{children:"Handle traffic spikes during viral events"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Availability"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"99.9% uptime for timeline generation"}),"\n",(0,i.jsx)(n.li,{children:"99.99% uptime for tweet reading"}),"\n",(0,i.jsx)(n.li,{children:"Graceful degradation during peak traffic"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Performance"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Timeline generation: <200ms"}),"\n",(0,i.jsx)(n.li,{children:"Tweet posting: <100ms"}),"\n",(0,i.jsx)(n.li,{children:"Search results: <300ms"}),"\n",(0,i.jsx)(n.li,{children:"Handle 300K tweets/second during peak"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Data Consistency"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Eventual consistency for timeline updates"}),"\n",(0,i.jsx)(n.li,{children:"Strong consistency for user actions (follow/unfollow)"}),"\n",(0,i.jsx)(n.li,{children:"Tweet immutability after posting"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"3. Estimations"}),"\n",(0,i.jsx)(n.h3,{children:"User Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total Users"}),": 500 million"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Daily Active Users"}),": 100 million"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Average tweets per user per day"}),": 3"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Average follows per user"}),": 200"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Heavy users (celebrities)"}),": 1% with 1M+ followers"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Tweet Volume"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tweets per day"}),": 300 million"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tweets per second"}),": 3,472 average"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Peak TPS"}),": 17,360 (5x average)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tweet fanout ratio"}),": 1:200 (average followers)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Storage Estimations"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Per Tweet Storage"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Tweet ID: 8 bytes"}),"\n",(0,i.jsx)(n.li,{children:"User ID: 8 bytes"}),"\n",(0,i.jsx)(n.li,{children:"Content: 300 bytes (average with metadata)"}),"\n",(0,i.jsx)(n.li,{children:"Media URLs: 100 bytes"}),"\n",(0,i.jsx)(n.li,{children:"Timestamps: 16 bytes"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total per tweet"}),": ~450 bytes"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Storage Growth"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Per Day"}),": 300M \xd7 450 bytes = 135 GB/day"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Per Year"}),": 135 GB \xd7 365 = 49 TB/year"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Per 5 Years"}),": 245 TB"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Timeline Cache Storage"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cache top 1000 tweets per user"}),"\n",(0,i.jsx)(n.li,{children:"100M users \xd7 1000 tweets \xd7 450 bytes = 45 TB cache"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"4. Design Goals"}),"\n",(0,i.jsx)(n.h3,{children:"Performance Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Timeline Generation"}),": <200ms for cached timelines"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tweet Publishing"}),": <100ms response time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Search"}),": <300ms for result delivery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Viral Content"}),": Handle 100K retweets/minute"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Architecture Patterns"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Event-Driven"}),": Tweet fanout and timeline updates"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CQRS"}),": Separate read and write models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache-Heavy"}),": Aggressive caching for read performance"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Usage Patterns"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Read Heavy"}),": 300:1 read to write ratio"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time"}),": Immediate timeline updates"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Spike Traffic"}),": Viral content creates traffic spikes"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"5. High-Level Design"}),"\n",(0,i.jsx)(n.h3,{children:"Building Blocks"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[Client] → [Load Balancer] → [API Gateway] → [Tweet Service]\r\n                                    ↓           ↓\r\n                            [Timeline Service] [User Service]\r\n                                    ↓           ↓\r\n                            [Fanout Service] → [Cache Layer]\r\n                                    ↓           ↓\r\n                            [Message Queue] → [Database Cluster]\r\n                                    ↓           ↓\r\n                            [Search Service] [Media Service]\n"})}),"\n",(0,i.jsx)(n.h3,{children:"Core Components"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"API Gateway"}),": Routes requests and handles authentication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tweet Service"}),": Handles tweet creation and retrieval"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Timeline Service"}),": Generates and serves user timelines"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fanout Service"}),": Distributes tweets to followers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User Service"}),": Manages user profiles and relationships"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Search Service"}),": Provides tweet and user search"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache Layer"}),": Multi-tier caching for performance"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"API Design"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Post Tweet"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-http",children:'POST /api/v1/tweets\r\nAuthorization: Bearer {token}\r\n{\r\n  "content": "Hello world! #myFirstTweet",\r\n  "media_urls": ["https://cdn.example.com/image1.jpg"],\r\n  "reply_to": null\r\n}\r\n\r\nResponse:\r\n{\r\n  "tweet_id": "1234567890",\r\n  "user_id": "user_123",\r\n  "content": "Hello world! #myFirstTweet",\r\n  "created_at": "2024-06-17T10:00:00Z",\r\n  "engagement": {\r\n    "likes": 0,\r\n    "retweets": 0,\r\n    "replies": 0\r\n  }\r\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Get Timeline"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-http",children:'GET /api/v1/timeline?type=home&limit=20&cursor=tweet_123\r\n\r\nResponse:\r\n{\r\n  "tweets": [\r\n    {\r\n      "tweet_id": "1234567890",\r\n      "user": {\r\n        "user_id": "user_456",\r\n        "username": "@johndoe",\r\n        "display_name": "John Doe",\r\n        "avatar_url": "https://cdn.example.com/avatar.jpg"\r\n      },\r\n      "content": "Great weather today!",\r\n      "created_at": "2024-06-17T10:00:00Z",\r\n      "engagement": {\r\n        "likes": 42,\r\n        "retweets": 15,\r\n        "replies": 8\r\n      },\r\n      "media": []\r\n    }\r\n  ],\r\n  "next_cursor": "tweet_456",\r\n  "has_more": true\r\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Follow User"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-http",children:'POST /api/v1/users/{user_id}/follow\r\n\r\nResponse:\r\n{\r\n  "following": true,\r\n  "follower_count": 1543,\r\n  "following_count": 287\r\n}\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Database Schema"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Users Table"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE users (\r\n    user_id BIGINT PRIMARY KEY,\r\n    username VARCHAR(50) UNIQUE NOT NULL,\r\n    display_name VARCHAR(100),\r\n    bio TEXT,\r\n    avatar_url VARCHAR(500),\r\n    verified BOOLEAN DEFAULT FALSE,\r\n    follower_count INT DEFAULT 0,\r\n    following_count INT DEFAULT 0,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\r\n);\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tweets Table"})," (Partitioned by created_at):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE tweets (\r\n    tweet_id BIGINT PRIMARY KEY,\r\n    user_id BIGINT NOT NULL,\r\n    content TEXT NOT NULL,\r\n    reply_to BIGINT,\r\n    retweet_of BIGINT,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    like_count INT DEFAULT 0,\r\n    retweet_count INT DEFAULT 0,\r\n    reply_count INT DEFAULT 0,\r\n    \r\n    INDEX idx_user_time (user_id, created_at),\r\n    INDEX idx_reply_to (reply_to),\r\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\r\n) PARTITION BY RANGE (UNIX_TIMESTAMP(created_at));\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Follows Table"})," (Sharded by follower_id):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE follows (\r\n    follower_id BIGINT,\r\n    following_id BIGINT,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    \r\n    PRIMARY KEY (follower_id, following_id),\r\n    INDEX idx_following (following_id, follower_id)\r\n);\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Timeline Cache Table"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE user_timelines (\r\n    user_id BIGINT,\r\n    tweet_id BIGINT,\r\n    score DECIMAL(10,2), -- for ranking\r\n    created_at TIMESTAMP,\r\n    \r\n    PRIMARY KEY (user_id, score, tweet_id),\r\n    INDEX idx_user_time (user_id, created_at)\r\n);\n"})}),"\n",(0,i.jsx)(n.h2,{children:"Timeline Generation Strategies"}),"\n",(0,i.jsx)(n.h3,{children:"Push Model (Write-Heavy)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tweet Fanout on Write"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class PushTimelineService:\r\n    def fanout_tweet(self, tweet, user_id):\r\n        # Get all followers\r\n        followers = self.user_service.get_followers(user_id)\r\n        \r\n        # Add tweet to each follower's timeline\r\n        for follower_id in followers:\r\n            self.timeline_cache.add_to_timeline(follower_id, tweet)\r\n            \r\n            # Limit timeline size (keep only latest 1000 tweets)\r\n            self.timeline_cache.trim_timeline(follower_id, max_size=1000)\r\n    \r\n    def get_timeline(self, user_id, limit=20):\r\n        # Timeline is pre-computed, just read from cache\r\n        return self.timeline_cache.get_timeline(user_id, limit)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Fast timeline reads (pre-computed)"}),"\n",(0,i.jsx)(n.li,{children:"Real-time timeline updates"}),"\n",(0,i.jsx)(n.li,{children:"Simple implementation"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Disadvantages"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Expensive for users with many followers"}),"\n",(0,i.jsx)(n.li,{children:"Storage overhead (duplicate tweets)"}),"\n",(0,i.jsx)(n.li,{children:"Celebrity problem (1M+ followers)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Pull Model (Read-Heavy)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Timeline Generation on Read"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class PullTimelineService:\r\n    def get_timeline(self, user_id, limit=20):\r\n        # Get users that this user follows\r\n        following = self.user_service.get_following(user_id)\r\n        \r\n        # Get recent tweets from each followed user\r\n        all_tweets = []\r\n        for followed_user_id in following:\r\n            tweets = self.tweet_service.get_user_tweets(\r\n                followed_user_id, \r\n                limit=100\r\n            )\r\n            all_tweets.extend(tweets)\r\n        \r\n        # Sort by timestamp and return top tweets\r\n        sorted_tweets = sorted(all_tweets, key=lambda x: x.created_at, reverse=True)\r\n        return sorted_tweets[:limit]\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No fanout cost for popular users"}),"\n",(0,i.jsx)(n.li,{children:"No storage duplication"}),"\n",(0,i.jsx)(n.li,{children:"Consistent view of latest data"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Disadvantages"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Slow timeline generation"}),"\n",(0,i.jsx)(n.li,{children:"Database load on read"}),"\n",(0,i.jsx)(n.li,{children:"Difficult to rank by engagement"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Hybrid Model (Recommended)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Smart Fanout Strategy"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class HybridTimelineService:\r\n    def __init__(self):\r\n        self.celebrity_threshold = 1000000  # 1M followers\r\n        \r\n    def fanout_tweet(self, tweet, user_id):\r\n        follower_count = self.user_service.get_follower_count(user_id)\r\n        \r\n        if follower_count > self.celebrity_threshold:\r\n            # Celebrity: don't fanout, use pull on read\r\n            self.celebrity_tweets_cache.add(user_id, tweet)\r\n        else:\r\n            # Regular user: fanout to all followers\r\n            followers = self.user_service.get_followers(user_id)\r\n            for follower_id in followers:\r\n                self.timeline_cache.add_to_timeline(follower_id, tweet)\r\n    \r\n    def get_timeline(self, user_id, limit=20):\r\n        # Get pre-computed timeline\r\n        timeline_tweets = self.timeline_cache.get_timeline(user_id, limit)\r\n        \r\n        # Get tweets from celebrities this user follows\r\n        celebrity_following = self.user_service.get_celebrity_following(user_id)\r\n        celebrity_tweets = []\r\n        \r\n        for celebrity_id in celebrity_following:\r\n            tweets = self.celebrity_tweets_cache.get_recent_tweets(celebrity_id, 10)\r\n            celebrity_tweets.extend(tweets)\r\n        \r\n        # Merge and sort all tweets\r\n        all_tweets = timeline_tweets + celebrity_tweets\r\n        sorted_tweets = sorted(all_tweets, key=lambda x: x.created_at, reverse=True)\r\n        \r\n        return sorted_tweets[:limit]\n"})}),"\n",(0,i.jsx)(n.h2,{children:"Detailed Design Deep Dive"}),"\n",(0,i.jsx)(n.h3,{children:"Fanout Service Architecture"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Async Fanout Processing"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class FanoutService:\r\n    def __init__(self):\r\n        self.message_queue = MessageQueue()\r\n        self.batch_size = 1000\r\n        \r\n    def queue_fanout(self, tweet):\r\n        # Queue fanout job for async processing\r\n        fanout_job = {\r\n            "tweet_id": tweet.id,\r\n            "user_id": tweet.user_id,\r\n            "timestamp": tweet.created_at\r\n        }\r\n        self.message_queue.publish("fanout_queue", fanout_job)\r\n    \r\n    def process_fanout_batch(self, jobs):\r\n        # Process multiple fanout jobs in batch\r\n        for job in jobs:\r\n            followers = self.get_followers_batch(job.user_id)\r\n            \r\n            # Batch insert into timeline cache\r\n            timeline_entries = []\r\n            for follower_id in followers:\r\n                timeline_entries.append({\r\n                    "user_id": follower_id,\r\n                    "tweet_id": job.tweet_id,\r\n                    "score": self.calculate_score(job),\r\n                    "created_at": job.timestamp\r\n                })\r\n            \r\n            self.timeline_cache.batch_insert(timeline_entries)\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Caching Strategy"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multi-Layer Cache Architecture"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"L1 Cache"}),": Application-level cache (Recent timelines)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"L2 Cache"}),": Redis cluster (User timelines, tweet data)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"L3 Cache"}),": CDN (Media files, static content)"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CacheManager:\r\n    def __init__(self):\r\n        self.l1_cache = LRUCache(max_size=10000)  # In-memory\r\n        self.l2_cache = RedisCluster()\r\n        self.l3_cache = CDN()\r\n    \r\n    def get_timeline(self, user_id, limit=20):\r\n        cache_key = f"timeline:{user_id}:{limit}"\r\n        \r\n        # Try L1 cache first\r\n        timeline = self.l1_cache.get(cache_key)\r\n        if timeline:\r\n            return timeline\r\n            \r\n        # Try L2 cache (Redis)\r\n        timeline = self.l2_cache.get(cache_key)\r\n        if timeline:\r\n            self.l1_cache.set(cache_key, timeline, ttl=60)\r\n            return timeline\r\n            \r\n        # Generate timeline and cache\r\n        timeline = self.timeline_service.generate_timeline(user_id, limit)\r\n        \r\n        self.l2_cache.set(cache_key, timeline, ttl=300)\r\n        self.l1_cache.set(cache_key, timeline, ttl=60)\r\n        \r\n        return timeline\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Search Service"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Elasticsearch Integration"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SearchService:\r\n    def __init__(self):\r\n        self.elasticsearch = Elasticsearch()\r\n        \r\n    def index_tweet(self, tweet):\r\n        doc = {\r\n            "tweet_id": tweet.id,\r\n            "user_id": tweet.user_id,\r\n            "username": tweet.user.username,\r\n            "content": tweet.content,\r\n            "hashtags": self.extract_hashtags(tweet.content),\r\n            "mentions": self.extract_mentions(tweet.content),\r\n            "created_at": tweet.created_at,\r\n            "engagement_score": self.calculate_engagement_score(tweet)\r\n        }\r\n        \r\n        self.elasticsearch.index(\r\n            index="tweets",\r\n            id=tweet.id,\r\n            body=doc\r\n        )\r\n    \r\n    def search_tweets(self, query, limit=20, offset=0):\r\n        search_body = {\r\n            "query": {\r\n                "bool": {\r\n                    "should": [\r\n                        {"match": {"content": {"query": query, "boost": 2}}},\r\n                        {"match": {"hashtags": {"query": query, "boost": 3}}},\r\n                        {"match": {"username": {"query": query, "boost": 1.5}}}\r\n                    ]\r\n                }\r\n            },\r\n            "sort": [\r\n                {"engagement_score": {"order": "desc"}},\r\n                {"created_at": {"order": "desc"}}\r\n            ],\r\n            "size": limit,\r\n            "from": offset\r\n        }\r\n        \r\n        return self.elasticsearch.search(index="tweets", body=search_body)\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Trending Topics"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-time Trend Detection"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class TrendingService:\r\n    def __init__(self):\r\n        self.redis = Redis()\r\n        self.trend_window = 3600  # 1 hour window\r\n        \r\n    def update_hashtag_count(self, hashtag):\r\n        current_hour = int(time.time() // self.trend_window)\r\n        key = f"hashtag_count:{current_hour}:{hashtag}"\r\n        \r\n        # Increment count for current hour\r\n        self.redis.incr(key)\r\n        self.redis.expire(key, self.trend_window * 2)  # Keep 2 hours\r\n        \r\n        # Update global trending scores\r\n        self.update_trending_score(hashtag)\r\n    \r\n    def get_trending_topics(self, limit=10):\r\n        # Get top hashtags by score\r\n        return self.redis.zrevrange("trending_hashtags", 0, limit-1, withscores=True)\r\n    \r\n    def calculate_trend_score(self, hashtag, current_count, historical_avg):\r\n        # Simple trending algorithm\r\n        if historical_avg == 0:\r\n            return current_count\r\n        \r\n        trend_ratio = current_count / historical_avg\r\n        velocity_score = trend_ratio * math.log(current_count + 1)\r\n        \r\n        return velocity_score\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Scaling Considerations"}),"\n",(0,i.jsx)(n.h3,{children:"Database Sharding"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tweets Sharding Strategy"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def get_tweet_shard(tweet_id):\r\n    # Shard by tweet_id for even distribution\r\n    return tweet_id % NUM_TWEET_SHARDS\r\n\r\ndef get_user_shard(user_id):\r\n    # Shard by user_id for user-related data\r\n    return user_id % NUM_USER_SHARDS\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Timeline Sharding"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def get_timeline_shard(user_id):\r\n    # Shard user timelines by user_id\r\n    return user_id % NUM_TIMELINE_SHARDS\n"})}),"\n",(0,i.jsx)(n.h3,{children:"Handling Viral Content"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Circuit Breaker for Fanout"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ViralContentHandler:\r\n    def __init__(self):\r\n        self.fanout_threshold = 100000  # 100K followers\r\n        self.circuit_breaker = CircuitBreaker()\r\n        \r\n    def handle_viral_tweet(self, tweet, user_id):\r\n        follower_count = self.user_service.get_follower_count(user_id)\r\n        \r\n        if follower_count > self.fanout_threshold:\r\n            # Skip immediate fanout for viral content\r\n            self.queue_delayed_fanout(tweet, delay=60)  # 1 minute delay\r\n            \r\n            # Use pull model for immediate reads\r\n            self.celebrity_cache.add_hot_tweet(user_id, tweet)\r\n        else:\r\n            # Normal fanout\r\n            self.fanout_service.fanout_tweet(tweet, user_id)\n"})}),"\n",(0,i.jsx)(n.h3,{children:"Media Handling"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"CDN Strategy for Media"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MediaService:\r\n    def __init__(self):\r\n        self.cdn = CloudFrontCDN()\r\n        self.storage = S3Storage()\r\n        \r\n    def upload_media(self, media_file, user_id):\r\n        # Generate unique filename\r\n        filename = f"{user_id}/{uuid.uuid4()}.{media_file.extension}"\r\n        \r\n        # Upload to S3\r\n        s3_url = self.storage.upload(filename, media_file)\r\n        \r\n        # Generate CDN URL\r\n        cdn_url = self.cdn.get_url(filename)\r\n        \r\n        return {\r\n            "media_id": str(uuid.uuid4()),\r\n            "original_url": s3_url,\r\n            "cdn_url": cdn_url,\r\n            "thumbnail_url": self.generate_thumbnail(cdn_url)\r\n        }\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Social Media Feed Design Quiz"}),"\n",(0,i.jsx)(n.p,{children:"Test your understanding of social media feed system design with the interactive quiz that appears after each part of this series."}),"\n",(0,i.jsx)(n.h2,{children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hybrid Approach"}),": Combine push and pull models based on user characteristics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Aggressive Caching"}),": Multi-layer caching is essential for read performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Async Processing"}),": Use message queues for fanout and background processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Viral Content"}),": Design circuit breakers and fallback mechanisms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Search Integration"}),": Elasticsearch enables fast, relevant search results"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"What's Next?"}),"\n",(0,i.jsx)(n.p,{children:"In Part 5, we'll design a video streaming service like YouTube, which introduces challenges around large file storage, content delivery networks, and video processing pipelines."})]})}function o(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},5794:function(e,n,r){r.r(n),r.d(n,{default:function(){return o},frontmatter:function(){return t}});var i=r(7437),s=r(4229);let t=void 0;function l(e){let n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{children:"Design a Video Streaming Service (YouTube)"}),"\n",(0,i.jsx)(n.p,{children:"In this part, we'll design a video streaming service like YouTube or Netflix. This introduces unique challenges around large file storage, content delivery networks, video processing, and global content distribution."}),"\n",(0,i.jsx)(n.h2,{children:"1. Functional Requirements"}),"\n",(0,i.jsx)(n.h3,{children:"Actors"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Content Creator"}),": Uploads and manages videos"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Viewer"}),": Watches and interacts with videos"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Content Moderator"}),": Reviews flagged content"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System"}),": Handles video processing and recommendations"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Use Cases"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Content Creator"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Upload videos (various formats, up to 4K resolution)"}),"\n",(0,i.jsx)(n.li,{children:"Add metadata (title, description, thumbnails, tags)"}),"\n",(0,i.jsx)(n.li,{children:"View analytics (views, engagement, revenue)"}),"\n",(0,i.jsx)(n.li,{children:"Manage video settings (privacy, monetization)"}),"\n",(0,i.jsx)(n.li,{children:"Live streaming capability"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Viewer"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Search and browse videos"}),"\n",(0,i.jsx)(n.li,{children:"Watch videos with adaptive quality"}),"\n",(0,i.jsx)(n.li,{children:"Like, comment, share videos"}),"\n",(0,i.jsx)(n.li,{children:"Subscribe to channels"}),"\n",(0,i.jsx)(n.li,{children:"Create and manage playlists"}),"\n",(0,i.jsx)(n.li,{children:"View personalized recommendations"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Content Moderator"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Review flagged content"}),"\n",(0,i.jsx)(n.li,{children:"Apply community guidelines"}),"\n",(0,i.jsx)(n.li,{children:"Manage copyright claims"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Functional Requirements"}),"\n",(0,i.jsxs)(n.p,{children:["✅ ",(0,i.jsx)(n.strong,{children:"In Scope"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Video upload and storage"}),"\n",(0,i.jsx)(n.li,{children:"Video transcoding (multiple resolutions)"}),"\n",(0,i.jsx)(n.li,{children:"Video playback with adaptive streaming"}),"\n",(0,i.jsx)(n.li,{children:"Search and discovery"}),"\n",(0,i.jsx)(n.li,{children:"User engagement (likes, comments, subscriptions)"}),"\n",(0,i.jsx)(n.li,{children:"Basic recommendation system"}),"\n",(0,i.jsx)(n.li,{children:"Analytics and metrics"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["❌ ",(0,i.jsx)(n.strong,{children:"Out of Scope"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Advanced recommendation algorithms (ML-based)"}),"\n",(0,i.jsx)(n.li,{children:"Monetization and ad serving"}),"\n",(0,i.jsx)(n.li,{children:"Live streaming infrastructure"}),"\n",(0,i.jsx)(n.li,{children:"Advanced content moderation"}),"\n",(0,i.jsx)(n.li,{children:"Content creator studio tools"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"2. Non-Functional Requirements"}),"\n",(0,i.jsx)(n.h3,{children:"Scalability"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Support 2 billion users globally"}),"\n",(0,i.jsx)(n.li,{children:"Handle 500 hours of video uploaded per minute"}),"\n",(0,i.jsx)(n.li,{children:"Support 1 billion hours watched per day"}),"\n",(0,i.jsx)(n.li,{children:"Handle traffic spikes during viral events"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Availability"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"99.9% uptime for video playback"}),"\n",(0,i.jsx)(n.li,{children:"99.5% uptime for video uploads"}),"\n",(0,i.jsx)(n.li,{children:"Global content distribution"}),"\n",(0,i.jsx)(n.li,{children:"Graceful degradation during failures"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Performance"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Video start time: <2 seconds globally"}),"\n",(0,i.jsx)(n.li,{children:"Upload processing: <30 minutes for 1-hour video"}),"\n",(0,i.jsx)(n.li,{children:"Search results: <300ms"}),"\n",(0,i.jsx)(n.li,{children:"Support 4K streaming with <1% rebuffering"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Storage & Bandwidth"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Petabyte-scale storage requirements"}),"\n",(0,i.jsx)(n.li,{children:"Multi-region content replication"}),"\n",(0,i.jsx)(n.li,{children:"Intelligent content placement"}),"\n",(0,i.jsx)(n.li,{children:"Bandwidth optimization"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"3. Estimations"}),"\n",(0,i.jsx)(n.h3,{children:"User Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total Users"}),": 2 billion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Daily Active Users"}),": 500 million"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Average watch time per user"}),": 40 minutes/day"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Concurrent viewers"}),": 50 million peak"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Video Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Videos uploaded per day"}),": 720,000 (500 hours/min \xd7 60 min/hour \xd7 24 hours)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Video views per day"}),": 5 billion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Average video length"}),": 10 minutes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Video upload formats"}),": 90% mobile (1080p), 10% professional (4K)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Storage Estimations"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Per Video Storage"})," (Multiple Resolutions):"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Original file: 1 GB (10 min @ 4K)"}),"\n",(0,i.jsx)(n.li,{children:"1080p: 400 MB"}),"\n",(0,i.jsx)(n.li,{children:"720p: 200 MB"}),"\n",(0,i.jsx)(n.li,{children:"480p: 100 MB"}),"\n",(0,i.jsx)(n.li,{children:"360p: 50 MB"}),"\n",(0,i.jsx)(n.li,{children:"Thumbnails: 1 MB"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total per video"}),": ~1.75 GB"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Storage Growth"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Per Day"}),": 720K videos \xd7 1.75 GB = 1.26 PB/day"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Per Year"}),": 1.26 PB \xd7 365 = 460 PB/year"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"With Replication (3x)"}),": 1.38 EB/year"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Bandwidth Estimations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Average bitrate"}),": 2 Mbps (adaptive streaming)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Concurrent viewers"}),": 50M \xd7 2 Mbps = 100 Tbps"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Daily bandwidth"}),": 50M \xd7 2 Mbps \xd7 40 min = 400 TB/day"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"4. Design Goals"}),"\n",(0,i.jsx)(n.h3,{children:"Performance Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Video Start Time"}),": <2 seconds globally"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Buffering"}),": <1% rebuffering ratio"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Upload Speed"}),": Support simultaneous uploads"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Search Latency"}),": <300ms"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Architecture Patterns"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Microservices"}),": Decomposed by functionality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Event-Driven"}),": Video processing workflows"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CQRS"}),": Separate read/write models for metadata"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Usage Patterns"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Read Heavy"}),": 100:1 read to write ratio"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Large Files"}),": Multi-GB video files"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global Distribution"}),": Viewers worldwide"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Batch Processing"}),": Video encoding workflows"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"5. High-Level Design"}),"\n",(0,i.jsx)(n.h3,{children:"Building Blocks"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[Client] → [CDN] → [Load Balancer] → [API Gateway]\r\n                           ↓              ↓\r\n                   [Video Service] → [Upload Service]\r\n                           ↓              ↓\r\n                   [Metadata DB] ← [Video Processing]\r\n                           ↓              ↓\r\n                   [Search Service] → [Blob Storage]\r\n                           ↓              ↓\r\n                   [Analytics] ← [Recommendation Service]\n"})}),"\n",(0,i.jsx)(n.h3,{children:"Core Components"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CDN"}),": Global content delivery network"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Upload Service"}),": Handles video file uploads"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Video Processing"}),": Transcoding and optimization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Metadata Service"}),": Video information and user data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Search Service"}),": Video discovery and search"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Streaming Service"}),": Adaptive video delivery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Analytics Service"}),": View tracking and metrics"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"API Design"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Upload Video"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-http",children:'POST /api/v1/videos/upload\r\nAuthorization: Bearer {token}\r\nContent-Type: multipart/form-data\r\n\r\n{\r\n  "title": "Amazing Travel Video",\r\n  "description": "My trip to Iceland",\r\n  "tags": ["travel", "iceland", "nature"],\r\n  "category": "travel",\r\n  "privacy": "public",\r\n  "thumbnail": {file},\r\n  "video_file": {file}\r\n}\r\n\r\nResponse:\r\n{\r\n  "video_id": "abc123def456",\r\n  "upload_url": "https://upload.example.com/abc123def456",\r\n  "status": "processing",\r\n  "estimated_completion": "2024-06-17T10:30:00Z"\r\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Get Video"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-http",children:'GET /api/v1/videos/{video_id}\r\n\r\nResponse:\r\n{\r\n  "video_id": "abc123def456",\r\n  "title": "Amazing Travel Video",\r\n  "description": "My trip to Iceland",\r\n  "channel": {\r\n    "channel_id": "channel_789",\r\n    "name": "Travel Enthusiast",\r\n    "subscriber_count": 15420\r\n  },\r\n  "duration": 600,\r\n  "views": 12543,\r\n  "likes": 892,\r\n  "upload_date": "2024-06-17T10:00:00Z",\r\n  "streaming_urls": {\r\n    "4k": "https://cdn.example.com/abc123def456/4k.m3u8",\r\n    "1080p": "https://cdn.example.com/abc123def456/1080p.m3u8",\r\n    "720p": "https://cdn.example.com/abc123def456/720p.m3u8",\r\n    "480p": "https://cdn.example.com/abc123def456/480p.m3u8"\r\n  },\r\n  "thumbnails": {\r\n    "default": "https://cdn.example.com/abc123def456/thumb.jpg",\r\n    "medium": "https://cdn.example.com/abc123def456/thumb_medium.jpg"\r\n  }\r\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Search Videos"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-http",children:'GET /api/v1/search?q=travel iceland&limit=20&offset=0&sort=relevance\r\n\r\nResponse:\r\n{\r\n  "results": [\r\n    {\r\n      "video_id": "abc123def456",\r\n      "title": "Amazing Travel Video",\r\n      "thumbnail": "https://cdn.example.com/abc123def456/thumb.jpg",\r\n      "duration": 600,\r\n      "views": 12543,\r\n      "channel_name": "Travel Enthusiast",\r\n      "upload_date": "2024-06-17T10:00:00Z"\r\n    }\r\n  ],\r\n  "total_results": 15420,\r\n  "next_page_token": "eyJvZmZzZXQiOjIwfQ=="\r\n}\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Database Schema"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Videos Table"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE videos (\r\n    video_id VARCHAR(20) PRIMARY KEY,\r\n    channel_id BIGINT NOT NULL,\r\n    title VARCHAR(255) NOT NULL,\r\n    description TEXT,\r\n    duration INT NOT NULL,\r\n    category VARCHAR(50),\r\n    privacy ENUM('public', 'unlisted', 'private') DEFAULT 'public',\r\n    status ENUM('processing', 'ready', 'failed') DEFAULT 'processing',\r\n    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    view_count BIGINT DEFAULT 0,\r\n    like_count INT DEFAULT 0,\r\n    dislike_count INT DEFAULT 0,\r\n    comment_count INT DEFAULT 0,\r\n    \r\n    INDEX idx_channel_date (channel_id, upload_date),\r\n    INDEX idx_category_views (category, view_count),\r\n    FULLTEXT idx_search (title, description)\r\n);\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Video Files Table"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE video_files (\r\n    video_id VARCHAR(20),\r\n    resolution ENUM('4k', '1080p', '720p', '480p', '360p'),\r\n    file_url VARCHAR(500) NOT NULL,\r\n    file_size BIGINT NOT NULL,\r\n    bitrate INT NOT NULL,\r\n    codec VARCHAR(20) NOT NULL,\r\n    \r\n    PRIMARY KEY (video_id, resolution),\r\n    FOREIGN KEY (video_id) REFERENCES videos(video_id)\r\n);\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Channels Table"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE channels (\r\n    channel_id BIGINT PRIMARY KEY,\r\n    user_id BIGINT NOT NULL,\r\n    name VARCHAR(100) NOT NULL,\r\n    description TEXT,\r\n    subscriber_count BIGINT DEFAULT 0,\r\n    video_count INT DEFAULT 0,\r\n    total_views BIGINT DEFAULT 0,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    \r\n    INDEX idx_subscribers (subscriber_count),\r\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\r\n);\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"View Events Table"})," (Time-series data):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE view_events (\r\n    event_id BIGINT AUTO_INCREMENT PRIMARY KEY,\r\n    video_id VARCHAR(20) NOT NULL,\r\n    user_id BIGINT,\r\n    session_id VARCHAR(50),\r\n    watched_duration INT NOT NULL,\r\n    total_duration INT NOT NULL,\r\n    quality VARCHAR(10),\r\n    device_type VARCHAR(20),\r\n    geo_location VARCHAR(10),\r\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    \r\n    INDEX idx_video_time (video_id, timestamp),\r\n    INDEX idx_user_time (user_id, timestamp)\r\n) PARTITION BY RANGE (UNIX_TIMESTAMP(timestamp));\n"})}),"\n",(0,i.jsx)(n.h2,{children:"Video Processing Pipeline"}),"\n",(0,i.jsx)(n.h3,{children:"Upload and Processing Workflow"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VideoProcessingPipeline:\r\n    def __init__(self):\r\n        self.upload_service = UploadService()\r\n        self.transcoding_service = TranscodingService()\r\n        self.storage_service = StorageService()\r\n        self.metadata_service = MetadataService()\r\n        \r\n    def process_upload(self, video_file, metadata):\r\n        # 1. Generate unique video ID\r\n        video_id = self.generate_video_id()\r\n        \r\n        # 2. Upload original file to staging storage\r\n        staging_url = self.upload_service.upload_to_staging(video_file, video_id)\r\n        \r\n        # 3. Extract video metadata\r\n        video_info = self.extract_video_metadata(staging_url)\r\n        \r\n        # 4. Create database record\r\n        self.metadata_service.create_video_record(video_id, metadata, video_info)\r\n        \r\n        # 5. Queue transcoding jobs\r\n        self.queue_transcoding_jobs(video_id, staging_url, video_info)\r\n        \r\n        return {"video_id": video_id, "status": "processing"}\r\n    \r\n    def queue_transcoding_jobs(self, video_id, source_url, video_info):\r\n        resolutions = self.determine_target_resolutions(video_info)\r\n        \r\n        for resolution in resolutions:\r\n            job = {\r\n                "video_id": video_id,\r\n                "source_url": source_url,\r\n                "target_resolution": resolution,\r\n                "output_format": "mp4",\r\n                "codec": "h264"\r\n            }\r\n            \r\n            self.transcoding_queue.publish(job)\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Transcoding Service"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class TranscodingService:\r\n    def __init__(self):\r\n        self.ffmpeg = FFMpegWrapper()\r\n        self.storage = StorageService()\r\n        \r\n    def transcode_video(self, job):\r\n        video_id = job[\'video_id\']\r\n        source_url = job[\'source_url\']\r\n        resolution = job[\'target_resolution\']\r\n        \r\n        try:\r\n            # 1. Download source file\r\n            local_source = self.download_source_file(source_url)\r\n            \r\n            # 2. Transcode to target resolution\r\n            output_file = self.ffmpeg.transcode(\r\n                input_file=local_source,\r\n                resolution=resolution,\r\n                codec=job[\'codec\'],\r\n                bitrate=self.get_target_bitrate(resolution)\r\n            )\r\n            \r\n            # 3. Upload transcoded file to CDN\r\n            cdn_url = self.storage.upload_to_cdn(output_file, video_id, resolution)\r\n            \r\n            # 4. Generate thumbnails\r\n            thumbnails = self.generate_thumbnails(local_source, video_id)\r\n            \r\n            # 5. Update metadata with file URLs\r\n            self.metadata_service.update_video_files(video_id, {\r\n                "resolution": resolution,\r\n                "file_url": cdn_url,\r\n                "file_size": os.path.getsize(output_file),\r\n                "bitrate": self.get_target_bitrate(resolution)\r\n            })\r\n            \r\n            # 6. Cleanup temporary files\r\n            self.cleanup_temp_files([local_source, output_file])\r\n            \r\n        except Exception as e:\r\n            self.handle_transcoding_error(video_id, resolution, str(e))\r\n    \r\n    def get_target_bitrate(self, resolution):\r\n        bitrates = {\r\n            "4k": 20000,      # 20 Mbps\r\n            "1080p": 8000,    # 8 Mbps\r\n            "720p": 4000,     # 4 Mbps\r\n            "480p": 2000,     # 2 Mbps\r\n            "360p": 1000      # 1 Mbps\r\n        }\r\n        return bitrates.get(resolution, 2000)\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Content Delivery and Streaming"}),"\n",(0,i.jsx)(n.h3,{children:"CDN Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CDNManager:\r\n    def __init__(self):\r\n        self.primary_regions = ["us-east", "us-west", "eu-west", "asia-pacific"]\r\n        self.edge_locations = self.load_edge_locations()\r\n        \r\n    def upload_to_cdn(self, video_file, video_id, resolution):\r\n        # 1. Upload to primary storage\r\n        primary_url = self.upload_to_primary_storage(video_file, video_id, resolution)\r\n        \r\n        # 2. Replicate to major regions\r\n        replication_jobs = []\r\n        for region in self.primary_regions:\r\n            job = {\r\n                "source_url": primary_url,\r\n                "target_region": region,\r\n                "video_id": video_id,\r\n                "resolution": resolution\r\n            }\r\n            replication_jobs.append(job)\r\n        \r\n        self.queue_replication_jobs(replication_jobs)\r\n        \r\n        return primary_url\r\n    \r\n    def get_optimal_cdn_url(self, video_id, resolution, user_location):\r\n        # Find nearest CDN edge location\r\n        nearest_edge = self.find_nearest_edge(user_location)\r\n        \r\n        # Check if content is available at edge\r\n        edge_url = f"https://{nearest_edge}/videos/{video_id}/{resolution}.mp4"\r\n        \r\n        if self.check_content_availability(edge_url):\r\n            return edge_url\r\n        else:\r\n            # Fallback to regional CDN\r\n            regional_url = self.get_regional_url(video_id, resolution, user_location)\r\n            \r\n            # Trigger cache warming for future requests\r\n            self.trigger_cache_warming(edge_url, regional_url)\r\n            \r\n            return regional_url\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Adaptive Streaming"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class AdaptiveStreamingService:\r\n    def generate_hls_manifest(self, video_id, available_resolutions):\r\n        """Generate HLS master playlist for adaptive streaming"""\r\n        \r\n        manifest = "#EXTM3U\\n#EXT-X-VERSION:3\\n\\n"\r\n        \r\n        for resolution in available_resolutions:\r\n            bandwidth = self.get_bandwidth_for_resolution(resolution)\r\n            resolution_str = self.get_resolution_string(resolution)\r\n            \r\n            manifest += f"#EXT-X-STREAM-INF:BANDWIDTH={bandwidth},RESOLUTION={resolution_str}\\n"\r\n            manifest += f"{resolution}.m3u8\\n"\r\n        \r\n        return manifest\r\n    \r\n    def generate_resolution_playlist(self, video_id, resolution):\r\n        """Generate playlist for specific resolution"""\r\n        \r\n        # Get video segments for this resolution\r\n        segments = self.get_video_segments(video_id, resolution)\r\n        \r\n        playlist = "#EXTM3U\\n#EXT-X-VERSION:3\\n#EXT-X-TARGETDURATION:10\\n\\n"\r\n        \r\n        for segment in segments:\r\n            playlist += f"#EXTINF:{segment.duration},\\n"\r\n            playlist += f"{segment.url}\\n"\r\n        \r\n        playlist += "#EXT-X-ENDLIST\\n"\r\n        \r\n        return playlist\r\n    \r\n    def get_bandwidth_for_resolution(self, resolution):\r\n        bandwidths = {\r\n            "4k": 20000000,     # 20 Mbps\r\n            "1080p": 8000000,   # 8 Mbps\r\n            "720p": 4000000,    # 4 Mbps\r\n            "480p": 2000000,    # 2 Mbps\r\n            "360p": 1000000     # 1 Mbps\r\n        }\r\n        return bandwidths.get(resolution, 2000000)\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Search and Discovery"}),"\n",(0,i.jsx)(n.h3,{children:"Video Search Service"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VideoSearchService:\r\n    def __init__(self):\r\n        self.elasticsearch = Elasticsearch()\r\n        self.search_analytics = SearchAnalytics()\r\n        \r\n    def index_video(self, video):\r\n        """Index video for search"""\r\n        \r\n        doc = {\r\n            "video_id": video.video_id,\r\n            "title": video.title,\r\n            "description": video.description,\r\n            "tags": video.tags,\r\n            "category": video.category,\r\n            "channel_name": video.channel.name,\r\n            "upload_date": video.upload_date,\r\n            "duration": video.duration,\r\n            "view_count": video.view_count,\r\n            "like_count": video.like_count,\r\n            "engagement_score": self.calculate_engagement_score(video)\r\n        }\r\n        \r\n        self.elasticsearch.index(\r\n            index="videos",\r\n            id=video.video_id,\r\n            body=doc\r\n        )\r\n    \r\n    def search_videos(self, query, filters=None, limit=20, offset=0):\r\n        """Search videos with ranking"""\r\n        \r\n        search_body = {\r\n            "query": {\r\n                "bool": {\r\n                    "must": [\r\n                        {\r\n                            "multi_match": {\r\n                                "query": query,\r\n                                "fields": [\r\n                                    "title^3",\r\n                                    "description^2", \r\n                                    "tags^2",\r\n                                    "channel_name^1.5"\r\n                                ]\r\n                            }\r\n                        }\r\n                    ],\r\n                    "filter": self.build_filters(filters)\r\n                }\r\n            },\r\n            "sort": [\r\n                {"_score": {"order": "desc"}},\r\n                {"engagement_score": {"order": "desc"}},\r\n                {"upload_date": {"order": "desc"}}\r\n            ],\r\n            "size": limit,\r\n            "from": offset\r\n        }\r\n        \r\n        results = self.elasticsearch.search(index="videos", body=search_body)\r\n        \r\n        # Log search for analytics\r\n        self.search_analytics.log_search(query, results[\'hits\'][\'total\'][\'value\'])\r\n        \r\n        return self.format_search_results(results)\r\n    \r\n    def calculate_engagement_score(self, video):\r\n        """Calculate video engagement score for ranking"""\r\n        \r\n        age_in_days = (datetime.now() - video.upload_date).days\r\n        age_factor = 1.0 / (age_in_days + 1)\r\n        \r\n        view_score = math.log(video.view_count + 1)\r\n        like_ratio = video.like_count / max(video.view_count, 1)\r\n        \r\n        engagement_score = (view_score * 0.7 + like_ratio * 100 * 0.3) * age_factor\r\n        \r\n        return engagement_score\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Analytics and Monitoring"}),"\n",(0,i.jsx)(n.h3,{children:"Video Analytics"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VideoAnalyticsService:\r\n    def __init__(self):\r\n        self.time_series_db = InfluxDB()\r\n        self.cache = Redis()\r\n        \r\n    def track_video_view(self, video_id, user_id, watch_data):\r\n        """Track video view event"""\r\n        \r\n        event = {\r\n            "video_id": video_id,\r\n            "user_id": user_id,\r\n            "watched_duration": watch_data[\'watched_duration\'],\r\n            "total_duration": watch_data[\'total_duration\'],\r\n            "completion_rate": watch_data[\'watched_duration\'] / watch_data[\'total_duration\'],\r\n            "quality": watch_data[\'quality\'],\r\n            "device_type": watch_data[\'device_type\'],\r\n            "geo_location": watch_data[\'geo_location\'],\r\n            "timestamp": datetime.utcnow()\r\n        }\r\n        \r\n        # Store in time-series database\r\n        self.time_series_db.write_point("video_views", event)\r\n        \r\n        # Update real-time counters\r\n        self.update_realtime_metrics(video_id, event)\r\n    \r\n    def update_realtime_metrics(self, video_id, event):\r\n        """Update real-time view counters"""\r\n        \r\n        # Increment view count\r\n        self.cache.incr(f"video_views:{video_id}")\r\n        \r\n        # Update hourly view count\r\n        hour_key = f"video_views_hourly:{video_id}:{datetime.utcnow().strftime(\'%Y%m%d%H\')}"\r\n        self.cache.incr(hour_key)\r\n        self.cache.expire(hour_key, 86400)  # 24 hours\r\n        \r\n        # Track completion rate\r\n        if event[\'completion_rate\'] > 0.8:  # 80% completion\r\n            self.cache.incr(f"video_completions:{video_id}")\r\n    \r\n    def get_video_analytics(self, video_id, time_range="24h"):\r\n        """Get analytics for a specific video"""\r\n        \r\n        metrics = {\r\n            "total_views": self.cache.get(f"video_views:{video_id}") or 0,\r\n            "hourly_views": self.get_hourly_views(video_id, time_range),\r\n            "completion_rate": self.calculate_completion_rate(video_id),\r\n            "geographic_distribution": self.get_geographic_stats(video_id, time_range),\r\n            "device_breakdown": self.get_device_stats(video_id, time_range),\r\n            "quality_distribution": self.get_quality_stats(video_id, time_range)\r\n        }\r\n        \r\n        return metrics\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Scaling Considerations"}),"\n",(0,i.jsx)(n.h3,{children:"Storage Optimization"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Intelligent Storage Tiering"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class StorageTierManager:\r\n    def __init__(self):\r\n        self.hot_storage = "SSD_TIER"      # Recent, popular content\r\n        self.warm_storage = "HDD_TIER"     # Older, moderate popularity\r\n        self.cold_storage = "GLACIER_TIER" # Archived content\r\n        \r\n    def determine_storage_tier(self, video_id):\r\n        video_stats = self.get_video_stats(video_id)\r\n        \r\n        # Recent videos (< 30 days) → Hot storage\r\n        if video_stats[\'age_days\'] < 30:\r\n            return self.hot_storage\r\n            \r\n        # Popular videos (> 1000 views/day) → Hot storage\r\n        if video_stats[\'daily_views\'] > 1000:\r\n            return self.hot_storage\r\n            \r\n        # Moderate popularity → Warm storage\r\n        if video_stats[\'daily_views\'] > 10:\r\n            return self.warm_storage\r\n            \r\n        # Low popularity → Cold storage\r\n        return self.cold_storage\r\n    \r\n    def migrate_storage_tier(self, video_id, target_tier):\r\n        current_urls = self.get_video_file_urls(video_id)\r\n        \r\n        for resolution, url in current_urls.items():\r\n            # Copy to new storage tier\r\n            new_url = self.copy_to_tier(url, target_tier)\r\n            \r\n            # Update database with new URL\r\n            self.update_video_file_url(video_id, resolution, new_url)\r\n            \r\n            # Delete from old tier (after verification)\r\n            self.schedule_deletion(url, delay="24h")\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Global Distribution"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Edge Cache Management"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class EdgeCacheManager:\r\n    def __init__(self):\r\n        self.popularity_threshold = 1000  # views per hour\r\n        self.cache_regions = ["NA", "EU", "ASIA", "SA", "AFRICA"]\r\n        \r\n    def should_cache_at_edge(self, video_id):\r\n        recent_views = self.get_recent_views(video_id, hours=1)\r\n        return recent_views > self.popularity_threshold\r\n    \r\n    def predict_viral_content(self, video_id):\r\n        """Predict if content will go viral based on early metrics"""\r\n        \r\n        # Get first hour metrics\r\n        first_hour_views = self.get_views_in_timeframe(video_id, "1h")\r\n        first_hour_engagement = self.get_engagement_rate(video_id, "1h")\r\n        \r\n        # Simple viral prediction\r\n        viral_score = first_hour_views * first_hour_engagement\r\n        \r\n        if viral_score > 10000:  # Threshold for viral prediction\r\n            # Pre-cache in all regions\r\n            self.precache_in_all_regions(video_id)\r\n            return True\r\n            \r\n        return False\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Video Streaming Design Quiz"}),"\n",(0,i.jsx)(n.p,{children:"Test your understanding of video streaming system design with the interactive quiz that appears after each part of this series."}),"\n",(0,i.jsx)(n.h2,{children:"Security and Content Protection"}),"\n",(0,i.jsx)(n.h3,{children:"Copyright Protection"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Content ID system for automatic detection"}),"\n",(0,i.jsx)(n.li,{children:"DMCA takedown process automation"}),"\n",(0,i.jsx)(n.li,{children:"Watermarking and fingerprinting"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Access Control"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"JWT-based authentication"}),"\n",(0,i.jsx)(n.li,{children:"CDN token authentication"}),"\n",(0,i.jsx)(n.li,{children:"Geographic content restrictions"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"DRM Implementation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Encrypted video streams"}),"\n",(0,i.jsx)(n.li,{children:"License server integration"}),"\n",(0,i.jsx)(n.li,{children:"Device-specific decryption keys"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Resolution Strategy"}),": Store videos in multiple qualities for adaptive streaming"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global CDN"}),": Essential for low latency worldwide video delivery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intelligent Caching"}),": Predict and pre-cache viral content"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage Tiering"}),": Optimize costs with hot/warm/cold storage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Async Processing"}),": Use message queues for video transcoding workflows"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"What's Next?"}),"\n",(0,i.jsx)(n.p,{children:"In Part 6, we'll design a distributed cache system like Redis, which covers fundamental concepts of caching, data consistency, and distributed system coordination."})]})}function o(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},9230:function(e,n,r){r.r(n),r.d(n,{default:function(){return o},frontmatter:function(){return t}});var i=r(7437),s=r(4229);let t=void 0;function l(e){let n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{children:"Design a Distributed Cache (Redis)"}),"\n",(0,i.jsx)(n.p,{children:"In this final part, we'll design a distributed cache system like Redis or Memcached. This introduces fundamental concepts of distributed systems including data consistency, partitioning, replication, and coordination."}),"\n",(0,i.jsx)(n.h2,{children:"1. Functional Requirements"}),"\n",(0,i.jsx)(n.h3,{children:"Actors"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Application Client"}),": Reads and writes cache data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache Administrator"}),": Monitors and manages cache cluster"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System"}),": Handles replication and failover"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Use Cases"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Application Client"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Store key-value pairs with TTL"}),"\n",(0,i.jsx)(n.li,{children:"Retrieve values by key"}),"\n",(0,i.jsx)(n.li,{children:"Delete specific keys"}),"\n",(0,i.jsx)(n.li,{children:"Perform atomic operations (increment, append)"}),"\n",(0,i.jsx)(n.li,{children:"Execute batch operations"}),"\n",(0,i.jsx)(n.li,{children:"Subscribe to key events"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cache Administrator"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Monitor cluster health and performance"}),"\n",(0,i.jsx)(n.li,{children:"Add/remove nodes from cluster"}),"\n",(0,i.jsx)(n.li,{children:"Configure replication settings"}),"\n",(0,i.jsx)(n.li,{children:"Manage memory usage and eviction policies"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"System Functions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Automatic failover and recovery"}),"\n",(0,i.jsx)(n.li,{children:"Data replication across nodes"}),"\n",(0,i.jsx)(n.li,{children:"Load balancing and sharding"}),"\n",(0,i.jsx)(n.li,{children:"Memory management and eviction"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Functional Requirements"}),"\n",(0,i.jsxs)(n.p,{children:["✅ ",(0,i.jsx)(n.strong,{children:"In Scope"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Basic operations (GET, SET, DELETE)"}),"\n",(0,i.jsx)(n.li,{children:"TTL (Time To Live) support"}),"\n",(0,i.jsx)(n.li,{children:"Data partitioning across nodes"}),"\n",(0,i.jsx)(n.li,{children:"Replication for high availability"}),"\n",(0,i.jsx)(n.li,{children:"Atomic operations and transactions"}),"\n",(0,i.jsx)(n.li,{children:"Pub/Sub messaging"}),"\n",(0,i.jsx)(n.li,{children:"Memory management and eviction"}),"\n",(0,i.jsx)(n.li,{children:"Cluster management"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["❌ ",(0,i.jsx)(n.strong,{children:"Out of Scope"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Complex data structures (sorted sets, streams)"}),"\n",(0,i.jsx)(n.li,{children:"Persistence to disk"}),"\n",(0,i.jsx)(n.li,{children:"Advanced scripting (Lua scripts)"}),"\n",(0,i.jsx)(n.li,{children:"Advanced security features"}),"\n",(0,i.jsx)(n.li,{children:"Cross-datacenter replication"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"2. Non-Functional Requirements"}),"\n",(0,i.jsx)(n.h3,{children:"Scalability"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Support thousands of nodes in a cluster"}),"\n",(0,i.jsx)(n.li,{children:"Handle millions of operations per second"}),"\n",(0,i.jsx)(n.li,{children:"Linear scaling with node addition"}),"\n",(0,i.jsx)(n.li,{children:"Support for multiple data centers"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Availability"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"99.99% uptime"}),"\n",(0,i.jsx)(n.li,{children:"Automatic failover <30 seconds"}),"\n",(0,i.jsx)(n.li,{children:"No single point of failure"}),"\n",(0,i.jsx)(n.li,{children:"Graceful degradation during failures"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Performance"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Sub-millisecond latency for cache hits"}),"\n",(0,i.jsx)(n.li,{children:"Support 100K+ ops/sec per node"}),"\n",(0,i.jsx)(n.li,{children:"Efficient memory utilization (>90%)"}),"\n",(0,i.jsx)(n.li,{children:"Minimal network overhead"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Consistency"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Strong consistency within partition"}),"\n",(0,i.jsx)(n.li,{children:"Eventual consistency across replicas"}),"\n",(0,i.jsx)(n.li,{children:"Configurable consistency levels"}),"\n",(0,i.jsx)(n.li,{children:"Conflict resolution mechanisms"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"3. Estimations"}),"\n",(0,i.jsx)(n.h3,{children:"Usage Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache Cluster Size"}),": 100 nodes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory per Node"}),": 64 GB"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total Cache Capacity"}),": 6.4 TB"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Operations per Second"}),": 10 million"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Performance Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Average Key Size"}),": 100 bytes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Average Value Size"}),": 1 KB"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache Hit Ratio"}),": 95%"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Network Bandwidth"}),": 10 Gbps per node"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Memory Estimations"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Per Node Storage"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Available Memory: 64 GB"}),"\n",(0,i.jsx)(n.li,{children:"OS and Overhead: 4 GB"}),"\n",(0,i.jsx)(n.li,{children:"Cache Data: 60 GB"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Effective Storage"}),": ~50 million key-value pairs per node"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cluster Totals"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total Effective Storage"}),": 5 billion key-value pairs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Efficiency"}),": 90% (accounting for fragmentation)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Replication Factor"}),": 3x for high availability"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"4. Design Goals"}),"\n",(0,i.jsx)(n.h3,{children:"Performance Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency"}),": <1ms for local operations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Throughput"}),": 100K ops/sec per node"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Efficiency"}),": >90% utilization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Network Efficiency"}),": Minimal cross-node communication"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Architecture Patterns"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consistent Hashing"}),": For data partitioning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Master-Slave Replication"}),": For data consistency"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gossip Protocol"}),": For cluster coordination"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Usage Patterns"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Read Heavy"}),": 80% reads, 20% writes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hot Keys"}),": Power-law distribution of key access"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TTL Patterns"}),": Mix of short and long-lived data"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"5. High-Level Design"}),"\n",(0,i.jsx)(n.h3,{children:"Building Blocks"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[Client] → [Smart Client/Proxy] → [Cache Node 1] ← [Replica 1A]\r\n                    ↓                     ↓              ↓\r\n                [Cache Node 2] ← [Replica 2A] ← [Coordinator]\r\n                    ↓                     ↓              ↓\r\n                [Cache Node 3] ← [Replica 3A] ← [Gossip Network]\n"})}),"\n",(0,i.jsx)(n.h3,{children:"Core Components"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache Nodes"}),": Store actual key-value data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cluster Coordinator"}),": Manages cluster membership"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smart Client"}),": Routes requests to correct nodes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Replication Manager"}),": Handles data replication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gossip Protocol"}),": Disseminates cluster state"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Manager"}),": Handles eviction and garbage collection"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Data Distribution Strategy"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Consistent Hashing"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ConsistentHashing:\r\n    def __init__(self, nodes, virtual_nodes=150):\r\n        self.virtual_nodes = virtual_nodes\r\n        self.ring = {}\r\n        self.sorted_keys = []\r\n        \r\n        for node in nodes:\r\n            self.add_node(node)\r\n    \r\n    def hash(self, key):\r\n        return hashlib.md5(key.encode()).hexdigest()\r\n    \r\n    def add_node(self, node):\r\n        for i in range(self.virtual_nodes):\r\n            virtual_key = self.hash(f"{node}:{i}")\r\n            self.ring[virtual_key] = node\r\n        \r\n        self.sorted_keys = sorted(self.ring.keys())\r\n    \r\n    def get_node(self, key):\r\n        if not self.ring:\r\n            return None\r\n            \r\n        hash_key = self.hash(key)\r\n        \r\n        # Find the first node clockwise\r\n        for ring_key in self.sorted_keys:\r\n            if hash_key <= ring_key:\r\n                return self.ring[ring_key]\r\n        \r\n        # Wrap around to the first node\r\n        return self.ring[self.sorted_keys[0]]\r\n    \r\n    def remove_node(self, node):\r\n        for i in range(self.virtual_nodes):\r\n            virtual_key = self.hash(f"{node}:{i}")\r\n            if virtual_key in self.ring:\r\n                del self.ring[virtual_key]\r\n        \r\n        self.sorted_keys = sorted(self.ring.keys())\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Cache Node Implementation"}),"\n",(0,i.jsx)(n.h3,{children:"Core Cache Operations"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CacheNode:\r\n    def __init__(self, node_id, max_memory=64*1024*1024*1024):  # 64GB\r\n        self.node_id = node_id\r\n        self.max_memory = max_memory\r\n        self.data = {}\r\n        self.ttl_data = {}\r\n        self.access_times = {}\r\n        self.memory_usage = 0\r\n        self.eviction_policy = LRUEvictionPolicy()\r\n        \r\n    def get(self, key):\r\n        # Check if key exists and not expired\r\n        if key not in self.data:\r\n            return None\r\n            \r\n        if self.is_expired(key):\r\n            self.delete(key)\r\n            return None\r\n        \r\n        # Update access time for LRU\r\n        self.access_times[key] = time.time()\r\n        \r\n        return self.data[key]\r\n    \r\n    def set(self, key, value, ttl=None):\r\n        # Check memory constraints\r\n        value_size = self.calculate_size(value)\r\n        \r\n        if key in self.data:\r\n            # Update existing key\r\n            old_size = self.calculate_size(self.data[key])\r\n            self.memory_usage += (value_size - old_size)\r\n        else:\r\n            # New key\r\n            self.memory_usage += value_size + self.calculate_size(key)\r\n        \r\n        # Evict if necessary\r\n        while self.memory_usage > self.max_memory:\r\n            evicted_key = self.eviction_policy.evict(self.data, self.access_times)\r\n            if evicted_key:\r\n                self.delete(evicted_key)\r\n            else:\r\n                break  # No more keys to evict\r\n        \r\n        # Store the data\r\n        self.data[key] = value\r\n        self.access_times[key] = time.time()\r\n        \r\n        # Set TTL if provided\r\n        if ttl:\r\n            self.ttl_data[key] = time.time() + ttl\r\n    \r\n    def delete(self, key):\r\n        if key in self.data:\r\n            value_size = self.calculate_size(self.data[key])\r\n            key_size = self.calculate_size(key)\r\n            \r\n            del self.data[key]\r\n            del self.access_times[key]\r\n            \r\n            if key in self.ttl_data:\r\n                del self.ttl_data[key]\r\n            \r\n            self.memory_usage -= (value_size + key_size)\r\n            return True\r\n        \r\n        return False\r\n    \r\n    def is_expired(self, key):\r\n        if key not in self.ttl_data:\r\n            return False\r\n        \r\n        return time.time() > self.ttl_data[key]\r\n    \r\n    def cleanup_expired_keys(self):\r\n        """Background task to clean up expired keys"""\r\n        current_time = time.time()\r\n        expired_keys = []\r\n        \r\n        for key, expiry_time in self.ttl_data.items():\r\n            if current_time > expiry_time:\r\n                expired_keys.append(key)\r\n        \r\n        for key in expired_keys:\r\n            self.delete(key)\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Eviction Policies"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class LRUEvictionPolicy:\r\n    def evict(self, data, access_times):\r\n        if not access_times:\r\n            return None\r\n        \r\n        # Find least recently used key\r\n        lru_key = min(access_times.keys(), key=lambda k: access_times[k])\r\n        return lru_key\r\n\r\nclass LFUEvictionPolicy:\r\n    def __init__(self):\r\n        self.access_counts = {}\r\n    \r\n    def evict(self, data, access_times):\r\n        if not self.access_counts:\r\n            return None\r\n        \r\n        # Find least frequently used key\r\n        lfu_key = min(self.access_counts.keys(), key=lambda k: self.access_counts[k])\r\n        return lfu_key\r\n    \r\n    def on_access(self, key):\r\n        self.access_counts[key] = self.access_counts.get(key, 0) + 1\r\n\r\nclass TTLEvictionPolicy:\r\n    def evict(self, data, access_times, ttl_data):\r\n        # Prioritize expired keys\r\n        current_time = time.time()\r\n        \r\n        for key, expiry_time in ttl_data.items():\r\n            if current_time > expiry_time:\r\n                return key\r\n        \r\n        # If no expired keys, fall back to LRU\r\n        return LRUEvictionPolicy().evict(data, access_times)\n"})}),"\n",(0,i.jsx)(n.h2,{children:"Replication and Consistency"}),"\n",(0,i.jsx)(n.h3,{children:"Master-Slave Replication"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ReplicationManager:\r\n    def __init__(self, node_id, replication_factor=3):\r\n        self.node_id = node_id\r\n        self.replication_factor = replication_factor\r\n        self.replicas = set()\r\n        self.masters = set()\r\n        \r\n    def add_replica(self, replica_node):\r\n        self.replicas.add(replica_node)\r\n    \r\n    def replicate_write(self, key, value, ttl=None):\r\n        """Replicate write operation to all replicas"""\r\n        operation = {\r\n            "type": "SET",\r\n            "key": key,\r\n            "value": value,\r\n            "ttl": ttl,\r\n            "timestamp": time.time(),\r\n            "node_id": self.node_id\r\n        }\r\n        \r\n        # Synchronous replication to ensure consistency\r\n        successful_replications = 0\r\n        \r\n        for replica in self.replicas:\r\n            try:\r\n                result = replica.apply_operation(operation)\r\n                if result:\r\n                    successful_replications += 1\r\n            except Exception as e:\r\n                # Log replication failure\r\n                self.log_replication_error(replica, operation, str(e))\r\n        \r\n        # Require majority for success (quorum)\r\n        required_replications = (self.replication_factor + 1) // 2\r\n        \r\n        if successful_replications >= required_replications:\r\n            return True\r\n        else:\r\n            # Rollback operation if quorum not reached\r\n            self.rollback_operation(operation)\r\n            return False\r\n    \r\n    def handle_failover(self, failed_node):\r\n        """Handle node failure and promote replica"""\r\n        if failed_node in self.masters:\r\n            # Promote a replica to master\r\n            replica_to_promote = self.select_replica_for_promotion(failed_node)\r\n            if replica_to_promote:\r\n                self.promote_replica_to_master(replica_to_promote, failed_node)\r\n        \r\n        # Update routing tables\r\n        self.update_cluster_topology(failed_node, "FAILED")\n'})}),"\n",(0,i.jsx)(n.h3,{children:"Conflict Resolution"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ConflictResolver:\r\n    def resolve_write_conflict(self, operations):\r\n        \"\"\"Resolve conflicts using last-write-wins with vector clocks\"\"\"\r\n        \r\n        if len(operations) == 1:\r\n            return operations[0]\r\n        \r\n        # Sort by timestamp (last write wins)\r\n        sorted_ops = sorted(operations, key=lambda op: op['timestamp'])\r\n        latest_operation = sorted_ops[-1]\r\n        \r\n        # For concurrent writes (same timestamp), use node_id as tiebreaker\r\n        concurrent_ops = [op for op in sorted_ops if op['timestamp'] == latest_operation['timestamp']]\r\n        \r\n        if len(concurrent_ops) > 1:\r\n            # Use lexicographic ordering of node_id\r\n            latest_operation = min(concurrent_ops, key=lambda op: op['node_id'])\r\n        \r\n        return latest_operation\r\n    \r\n    def detect_concurrent_writes(self, operation1, operation2):\r\n        \"\"\"Detect if two operations are concurrent using vector clocks\"\"\"\r\n        \r\n        # Simple timestamp-based detection\r\n        time_diff = abs(operation1['timestamp'] - operation2['timestamp'])\r\n        \r\n        # Consider operations concurrent if within 100ms\r\n        return time_diff < 0.1\n"})}),"\n",(0,i.jsx)(n.h2,{children:"Cluster Management"}),"\n",(0,i.jsx)(n.h3,{children:"Gossip Protocol for Cluster Coordination"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class GossipProtocol:\r\n    def __init__(self, node_id, initial_nodes):\r\n        self.node_id = node_id\r\n        self.cluster_state = {}\r\n        self.heartbeat_interval = 1  # 1 second\r\n        self.failure_detection_timeout = 5  # 5 seconds\r\n        \r\n        # Initialize cluster state\r\n        for node in initial_nodes:\r\n            self.cluster_state[node] = {\r\n                "status": "ALIVE",\r\n                "last_seen": time.time(),\r\n                "metadata": {}\r\n            }\r\n    \r\n    def start_gossip(self):\r\n        """Start gossip protocol background tasks"""\r\n        threading.Thread(target=self.gossip_loop, daemon=True).start()\r\n        threading.Thread(target=self.failure_detection_loop, daemon=True).start()\r\n    \r\n    def gossip_loop(self):\r\n        """Periodically gossip cluster state with random nodes"""\r\n        while True:\r\n            try:\r\n                # Select random subset of nodes to gossip with\r\n                alive_nodes = [node for node, state in self.cluster_state.items() \r\n                              if state["status"] == "ALIVE" and node != self.node_id]\r\n                \r\n                if alive_nodes:\r\n                    random_nodes = random.sample(alive_nodes, min(3, len(alive_nodes)))\r\n                    \r\n                    for node in random_nodes:\r\n                        self.send_gossip_message(node)\r\n                \r\n                time.sleep(self.heartbeat_interval)\r\n                \r\n            except Exception as e:\r\n                self.log_error(f"Gossip loop error: {e}")\r\n    \r\n    def send_gossip_message(self, target_node):\r\n        """Send gossip message to target node"""\r\n        message = {\r\n            "type": "GOSSIP",\r\n            "sender": self.node_id,\r\n            "cluster_state": self.cluster_state,\r\n            "timestamp": time.time()\r\n        }\r\n        \r\n        try:\r\n            response = self.send_message(target_node, message)\r\n            if response:\r\n                self.merge_cluster_state(response["cluster_state"])\r\n        except Exception as e:\r\n            # Mark node as potentially failed\r\n            self.mark_node_suspect(target_node)\r\n    \r\n    def merge_cluster_state(self, remote_state):\r\n        """Merge remote cluster state with local state"""\r\n        for node, remote_info in remote_state.items():\r\n            if node not in self.cluster_state:\r\n                # New node discovered\r\n                self.cluster_state[node] = remote_info\r\n            else:\r\n                # Update if remote info is newer\r\n                local_info = self.cluster_state[node]\r\n                if remote_info["last_seen"] > local_info["last_seen"]:\r\n                    self.cluster_state[node] = remote_info\r\n    \r\n    def failure_detection_loop(self):\r\n        """Detect failed nodes based on heartbeat timeouts"""\r\n        while True:\r\n            current_time = time.time()\r\n            \r\n            for node, state in self.cluster_state.items():\r\n                if node == self.node_id:\r\n                    continue\r\n                \r\n                time_since_seen = current_time - state["last_seen"]\r\n                \r\n                if (time_since_seen > self.failure_detection_timeout and \r\n                    state["status"] == "ALIVE"):\r\n                    \r\n                    self.mark_node_failed(node)\r\n            \r\n            time.sleep(self.heartbeat_interval)\r\n    \r\n    def mark_node_failed(self, node):\r\n        """Mark node as failed and trigger failover"""\r\n        self.cluster_state[node]["status"] = "FAILED"\r\n        self.cluster_state[node]["last_seen"] = time.time()\r\n        \r\n        # Notify cluster about node failure\r\n        self.broadcast_node_failure(node)\r\n        \r\n        # Trigger rebalancing if necessary\r\n        self.trigger_rebalancing(node)\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Client Implementation"}),"\n",(0,i.jsx)(n.h3,{children:"Smart Client with Connection Pooling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CacheClient:\r\n    def __init__(self, cluster_nodes, pool_size=10):\r\n        self.cluster_nodes = cluster_nodes\r\n        self.consistent_hash = ConsistentHashing(cluster_nodes)\r\n        self.connection_pools = {}\r\n        \r\n        # Create connection pools for each node\r\n        for node in cluster_nodes:\r\n            self.connection_pools[node] = ConnectionPool(node, pool_size)\r\n    \r\n    def get(self, key):\r\n        """Get value for key with automatic retry and failover"""\r\n        target_node = self.consistent_hash.get_node(key)\r\n        replica_nodes = self.get_replica_nodes(key)\r\n        \r\n        # Try primary node first\r\n        try:\r\n            return self.execute_on_node(target_node, "GET", key)\r\n        except NodeUnavailableException:\r\n            # Try replica nodes\r\n            for replica in replica_nodes:\r\n                try:\r\n                    return self.execute_on_node(replica, "GET", key)\r\n                except NodeUnavailableException:\r\n                    continue\r\n            \r\n            raise CacheUnavailableException(f"All nodes unavailable for key: {key}")\r\n    \r\n    def set(self, key, value, ttl=None):\r\n        """Set key-value with replication"""\r\n        target_node = self.consistent_hash.get_node(key)\r\n        replica_nodes = self.get_replica_nodes(key)\r\n        \r\n        # Write to primary node\r\n        success = self.execute_on_node(target_node, "SET", key, value, ttl)\r\n        \r\n        if success:\r\n            # Asynchronously replicate to replicas\r\n            self.async_replicate(replica_nodes, "SET", key, value, ttl)\r\n        \r\n        return success\r\n    \r\n    def execute_on_node(self, node, operation, *args):\r\n        """Execute operation on specific node"""\r\n        connection = self.connection_pools[node].get_connection()\r\n        \r\n        try:\r\n            if operation == "GET":\r\n                return connection.get(args[0])\r\n            elif operation == "SET":\r\n                return connection.set(args[0], args[1], args[2] if len(args) > 2 else None)\r\n            elif operation == "DELETE":\r\n                return connection.delete(args[0])\r\n        finally:\r\n            self.connection_pools[node].return_connection(connection)\r\n    \r\n    def get_replica_nodes(self, key):\r\n        """Get replica nodes for a given key"""\r\n        primary_node = self.consistent_hash.get_node(key)\r\n        \r\n        # Get next N nodes in the ring as replicas\r\n        replicas = []\r\n        nodes = list(self.cluster_nodes)\r\n        primary_index = nodes.index(primary_node)\r\n        \r\n        for i in range(1, 4):  # 3 replicas\r\n            replica_index = (primary_index + i) % len(nodes)\r\n            replicas.append(nodes[replica_index])\r\n        \r\n        return replicas\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.h3,{children:"Metrics Collection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CacheMetrics:\r\n    def __init__(self):\r\n        self.hit_count = 0\r\n        self.miss_count = 0\r\n        self.operation_latencies = []\r\n        self.memory_usage = 0\r\n        self.eviction_count = 0\r\n        \r\n    def record_hit(self):\r\n        self.hit_count += 1\r\n    \r\n    def record_miss(self):\r\n        self.miss_count += 1\r\n    \r\n    def record_latency(self, operation, latency_ms):\r\n        self.operation_latencies.append({\r\n            "operation": operation,\r\n            "latency": latency_ms,\r\n            "timestamp": time.time()\r\n        })\r\n        \r\n        # Keep only last 1000 measurements\r\n        if len(self.operation_latencies) > 1000:\r\n            self.operation_latencies = self.operation_latencies[-1000:]\r\n    \r\n    def get_hit_ratio(self):\r\n        total_requests = self.hit_count + self.miss_count\r\n        if total_requests == 0:\r\n            return 0\r\n        return self.hit_count / total_requests\r\n    \r\n    def get_average_latency(self, operation=None):\r\n        if operation:\r\n            latencies = [l["latency"] for l in self.operation_latencies if l["operation"] == operation]\r\n        else:\r\n            latencies = [l["latency"] for l in self.operation_latencies]\r\n        \r\n        if not latencies:\r\n            return 0\r\n        \r\n        return sum(latencies) / len(latencies)\r\n    \r\n    def get_p99_latency(self, operation=None):\r\n        if operation:\r\n            latencies = [l["latency"] for l in self.operation_latencies if l["operation"] == operation]\r\n        else:\r\n            latencies = [l["latency"] for l in self.operation_latencies]\r\n        \r\n        if not latencies:\r\n            return 0\r\n        \r\n        sorted_latencies = sorted(latencies)\r\n        p99_index = int(0.99 * len(sorted_latencies))\r\n        return sorted_latencies[p99_index]\n'})}),"\n",(0,i.jsx)(n.h2,{children:"Distributed Cache Design Quiz"}),"\n",(0,i.jsx)(n.p,{children:"Test your understanding of distributed cache system design with the interactive quiz that appears after each part of this series."}),"\n",(0,i.jsx)(n.h2,{children:"Advanced Features"}),"\n",(0,i.jsx)(n.h3,{children:"Pub/Sub Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class PubSubManager:\r\n    def __init__(self):\r\n        self.subscriptions = {}  # channel -> set of subscribers\r\n        self.pattern_subscriptions = {}  # pattern -> set of subscribers\r\n        \r\n    def subscribe(self, client_id, channel):\r\n        if channel not in self.subscriptions:\r\n            self.subscriptions[channel] = set()\r\n        self.subscriptions[channel].add(client_id)\r\n    \r\n    def unsubscribe(self, client_id, channel):\r\n        if channel in self.subscriptions:\r\n            self.subscriptions[channel].discard(client_id)\r\n    \r\n    def publish(self, channel, message):\r\n        # Direct channel subscribers\r\n        if channel in self.subscriptions:\r\n            for subscriber in self.subscriptions[channel]:\r\n                self.send_message_to_client(subscriber, channel, message)\r\n        \r\n        # Pattern subscribers\r\n        for pattern, subscribers in self.pattern_subscriptions.items():\r\n            if self.matches_pattern(channel, pattern):\r\n                for subscriber in subscribers:\r\n                    self.send_message_to_client(subscriber, channel, message)\n"})}),"\n",(0,i.jsx)(n.h3,{children:"Memory Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class MemoryManager:\r\n    def __init__(self, max_memory):\r\n        self.max_memory = max_memory\r\n        self.current_usage = 0\r\n        self.fragmentation_threshold = 0.1\r\n        \r\n    def should_evict(self):\r\n        return self.current_usage > self.max_memory * 0.9\r\n    \r\n    def calculate_fragmentation(self):\r\n        # Simplified fragmentation calculation\r\n        allocated_memory = sum(sys.getsizeof(obj) for obj in self.data.values())\r\n        return 1 - (allocated_memory / self.current_usage)\r\n    \r\n    def defragment_memory(self):\r\n        if self.calculate_fragmentation() > self.fragmentation_threshold:\r\n            # Trigger garbage collection\r\n            gc.collect()\r\n            \r\n            # Reorganize data structure if needed\r\n            self.reorganize_data_structures()\n"})}),"\n",(0,i.jsx)(n.h2,{children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consistent Hashing"}),": Essential for distributed data partitioning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Replication Strategy"}),": Balance consistency, availability, and performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Failure Detection"}),": Use gossip protocols for robust cluster management"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smart Clients"}),": Implement client-side logic for routing and failover"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Management"}),": Efficient eviction policies and memory monitoring"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{children:"Series Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"Congratulations! You've completed the System Design Mastery series. You've learned to design:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"URL Shortener"}),": Read-heavy systems with caching"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chat System"}),": Real-time communication and WebSockets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Social Media Feed"}),": Content ranking and viral traffic handling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Video Streaming"}),": Large file storage and global CDN"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Distributed Cache"}),": Consistency and distributed coordination"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Final Interview Tips"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Practice Regularly"}),": Work through problems weekly"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Think Out Loud"}),": Communicate your reasoning clearly"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Start Simple"}),": Begin with basic design, then add complexity"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consider Trade-offs"}),": Discuss pros and cons of each decision"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learn from Real Systems"}),": Study how companies like Google, Facebook, and Netflix solve similar problems"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{children:"Continue Learning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Study real-world system architectures"}),"\n",(0,i.jsx)(n.li,{children:"Read engineering blogs from top tech companies"}),"\n",(0,i.jsx)(n.li,{children:"Practice with system design interview platforms"}),"\n",(0,i.jsx)(n.li,{children:"Build distributed systems to gain hands-on experience"}),"\n",(0,i.jsx)(n.li,{children:"Stay current with emerging technologies and patterns"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You're now ready to tackle any system design interview with confidence!"})})]})}function o(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);