"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[1709],{7598:function(e,n,r){r.r(n),r.d(n,{default:function(){return c},frontmatter:function(){return a},metadata:function(){return i}});var t=r(57437),s=r(52671);let a=void 0,i={postId:"160ddac5-f16c-4d23-91f9-3c846a186204",title:"Step-by-Step AI Agent Development: From Concept to Production",date:"2025-06-26",excerpt:"Master the complete development lifecycle of AI agents. This comprehensive guide covers everything from initial design and prototyping to testing, deployment, and monitoring in production environments.",author:"Abstract Algorithms",tags:["AI Agent Development","LangChain","Development Process","Agent Framework","Production Deployment"],status:"published",series:{name:"AI Agent Development",order:2,total:5,prev:"/posts/core-components-of-ai-agents-understanding-the-building-blocks",next:"/posts/multi-agent-architectures-orchestrating-intelligent-agent-teams",coverImage:"./assets/series-overview.png",overview:"/posts/ai-agent-development-series/"}};function o(e){let n={blockquote:"blockquote",br:"br",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Part 2 of the AI Agent Development Series"}),(0,t.jsx)(n.br,{}),"\n","Now that you understand the core components of AI agents, let's dive into the practical development process. This guide walks you through building agents from concept to production deployment."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Building production-ready AI agents requires a structured approach that goes far beyond simple LLM integration. This guide walks you through the complete development lifecycle, from initial concept to production deployment, with practical examples and best practices learned from real-world implementations."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83D\uDCCB Development Lifecycle Overview"}),"\n",(0,t.jsx)(n.p,{children:"The AI agent development process consists of seven key phases:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Requirements Analysis & Design"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Environment Setup & Architecture"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Core Agent Implementation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Tool Integration & Testing"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Memory & State Management"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Evaluation & Optimization"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Production Deployment & Monitoring"})}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83C\uDFAF Phase 1: Requirements Analysis & Design"}),"\n",(0,t.jsx)(n.h3,{children:"Define Agent Scope and Capabilities"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Agent Requirements Document Template\r\nagent_requirements = {\r\n    "name": "IncidentHandlingAgent",\r\n    "primary_goal": "Automate incident detection, analysis, and initial response",\r\n    "capabilities": [\r\n        "Monitor alert streams",\r\n        "Analyze log patterns", \r\n        "Create incident tickets",\r\n        "Notify relevant teams",\r\n        "Suggest remediation steps"\r\n    ],\r\n    "constraints": [\r\n        "Cannot execute destructive commands",\r\n        "Must escalate critical incidents to humans",\r\n        "All actions must be logged and auditable"\r\n    ],\r\n    "success_metrics": [\r\n        "Reduce mean time to detection (MTTD)",\r\n        "Improve alert signal-to-noise ratio",\r\n        "Decrease manual intervention for common issues"\r\n    ]\r\n}\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Design Agent Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# High-level architecture design\r\nclass AgentArchitecture:\r\n    def __init__(self):\r\n        self.components = {\r\n            "input_processor": "Handles incoming alerts and requests",\r\n            "reasoning_engine": "LLM-based decision making",\r\n            "memory_system": "Context and experience storage", \r\n            "tool_manager": "External system integration",\r\n            "output_formatter": "Response generation and formatting",\r\n            "monitoring": "Performance and behavior tracking"\r\n        }\r\n        \r\n        self.data_flow = [\r\n            "Input → Processing → Reasoning → Action → Output",\r\n            "Continuous: Memory Updates, Monitoring, Learning"\r\n        ]\r\n        \r\n        self.external_dependencies = [\r\n            "OpenAI API for LLM",\r\n            "Redis for session state",\r\n            "Elasticsearch for log search",\r\n            "Jira API for ticket creation",\r\n            "Slack API for notifications"\r\n        ]\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Create Agent Persona and Behavior Guidelines"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'agent_persona = """\r\nYou are an experienced DevOps engineer with expertise in:\r\n- System monitoring and alerting\r\n- Log analysis and troubleshooting  \r\n- Incident response procedures\r\n- Service dependency mapping\r\n\r\nYour communication style is:\r\n- Clear and concise\r\n- Action-oriented\r\n- Includes confidence levels for recommendations\r\n- Escalates when uncertain\r\n\r\nYour decision-making process:\r\n1. Gather all available context\r\n2. Analyze patterns and correlations\r\n3. Check historical similar incidents\r\n4. Recommend actions with risk assessment\r\n5. Document decisions and reasoning\r\n"""\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83C\uDFD7️ Phase 2: Environment Setup & Architecture"}),"\n",(0,t.jsx)(n.h3,{children:"Project Structure Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Create project structure\r\nmkdir ai-incident-agent\r\ncd ai-incident-agent\r\n\r\n# Create directory structure\r\nmkdir -p {src/{agents,tools,memory,utils},tests,config,docs,scripts}\r\n\r\n# Create core files\r\ntouch {src/__init__.py,src/agents/__init__.py,src/tools/__init__.py}\r\ntouch {requirements.txt,config/settings.py,.env.example}\n"})}),"\n",(0,t.jsx)(n.h3,{children:"Dependency Management"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# requirements.txt\r\nlangchain==0.1.0\r\nlangchain-openai==0.0.5\r\nlangchain-community==0.0.10\r\nredis==4.5.1\r\nelasticsearch==8.11.0\r\npydantic==2.5.0\r\nfastapi==0.104.0\r\nuvicorn==0.24.0\r\npytest==7.4.0\r\npython-dotenv==1.0.0\r\nprometheus-client==0.19.0\r\nstructlog==23.2.0\n"})}),"\n",(0,t.jsx)(n.h3,{children:"Configuration Management"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# config/settings.py\r\nfrom pydantic import BaseSettings\r\nfrom typing import List, Optional\r\n\r\nclass AgentSettings(BaseSettings):\r\n    # LLM Configuration\r\n    openai_api_key: str\r\n    model_name: str = "gpt-4"\r\n    temperature: float = 0.1\r\n    max_tokens: int = 2000\r\n    \r\n    # Memory Configuration\r\n    redis_url: str = "redis://localhost:6379"\r\n    memory_ttl: int = 3600  # 1 hour\r\n    \r\n    # Tool Configuration\r\n    elasticsearch_url: str = "http://localhost:9200"\r\n    jira_url: str\r\n    jira_token: str\r\n    slack_token: str\r\n    \r\n    # Agent Behavior\r\n    max_reasoning_steps: int = 10\r\n    confidence_threshold: float = 0.7\r\n    escalation_timeout: int = 300  # 5 minutes\r\n    \r\n    # Monitoring\r\n    metrics_port: int = 8000\r\n    log_level: str = "INFO"\r\n    \r\n    class Config:\r\n        env_file = ".env"\r\n\r\nsettings = AgentSettings()\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Logging and Monitoring Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# src/utils/logging.py\r\nimport structlog\r\nimport logging\r\nfrom prometheus_client import Counter, Histogram, Gauge\r\n\r\n# Configure structured logging\r\nstructlog.configure(\r\n    processors=[\r\n        structlog.stdlib.filter_by_level,\r\n        structlog.stdlib.add_logger_name,\r\n        structlog.stdlib.add_log_level,\r\n        structlog.stdlib.PositionalArgumentsFormatter(),\r\n        structlog.processors.TimeStamper(fmt=\"iso\"),\r\n        structlog.processors.StackInfoRenderer(),\r\n        structlog.processors.format_exc_info,\r\n        structlog.processors.UnicodeDecoder(),\r\n        structlog.processors.JSONRenderer()\r\n    ],\r\n    context_class=dict,\r\n    logger_factory=structlog.stdlib.LoggerFactory(),\r\n    cache_logger_on_first_use=True,\r\n)\r\n\r\nlogger = structlog.get_logger()\r\n\r\n# Prometheus metrics\r\nAGENT_REQUESTS = Counter('agent_requests_total', 'Total agent requests', ['agent_type', 'status'])\r\nAGENT_RESPONSE_TIME = Histogram('agent_response_seconds', 'Agent response time')\r\nACTIVE_INCIDENTS = Gauge('active_incidents', 'Number of active incidents')\r\nTOOL_USAGE = Counter('tool_usage_total', 'Tool usage count', ['tool_name', 'status'])\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83E\uDD16 Phase 3: Core Agent Implementation"}),"\n",(0,t.jsx)(n.h3,{children:"Base Agent Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/agents/base_agent.py\r\nfrom abc import ABC, abstractmethod\r\nfrom typing import Dict, Any, List, Optional\r\nimport uuid\r\nfrom datetime import datetime\r\n\r\nclass BaseAgent(ABC):\r\n    def __init__(self, name: str, settings: AgentSettings):\r\n        self.id = str(uuid.uuid4())\r\n        self.name = name\r\n        self.settings = settings\r\n        self.created_at = datetime.utcnow()\r\n        self.session_history = []\r\n        \r\n    @abstractmethod\r\n    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\r\n        """Main processing method - must be implemented by subclasses"""\r\n        pass\r\n    \r\n    def log_interaction(self, input_data: Dict[str, Any], output_data: Dict[str, Any]):\r\n        """Log agent interactions for debugging and analysis"""\r\n        interaction = {\r\n            "timestamp": datetime.utcnow().isoformat(),\r\n            "agent_id": self.id,\r\n            "input": input_data,\r\n            "output": output_data\r\n        }\r\n        self.session_history.append(interaction)\r\n        logger.info("Agent interaction logged", **interaction)\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Incident Handling Agent Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/agents/incident_agent.py\r\nfrom langchain.agents import initialize_agent, AgentType\r\nfrom langchain.chat_models import ChatOpenAI\r\nfrom langchain.memory import ConversationBufferMemory\r\nfrom src.tools.log_search import LogSearchTool\r\nfrom src.tools.ticket_creation import TicketTool\r\nfrom src.tools.notification import NotificationTool\r\n\r\nclass IncidentHandlingAgent(BaseAgent):\r\n    def __init__(self, settings: AgentSettings):\r\n        super().__init__("IncidentHandler", settings)\r\n        \r\n        # Initialize LLM\r\n        self.llm = ChatOpenAI(\r\n            openai_api_key=settings.openai_api_key,\r\n            model_name=settings.model_name,\r\n            temperature=settings.temperature\r\n        )\r\n        \r\n        # Initialize tools\r\n        self.tools = [\r\n            LogSearchTool(elasticsearch_url=settings.elasticsearch_url),\r\n            TicketTool(jira_url=settings.jira_url, token=settings.jira_token),\r\n            NotificationTool(slack_token=settings.slack_token)\r\n        ]\r\n        \r\n        # Initialize memory\r\n        self.memory = ConversationBufferMemory(\r\n            memory_key="chat_history",\r\n            return_messages=True\r\n        )\r\n        \r\n        # Initialize agent\r\n        self.agent = initialize_agent(\r\n            tools=self.tools,\r\n            llm=self.llm,\r\n            agent=AgentType.OPENAI_FUNCTIONS,\r\n            memory=self.memory,\r\n            verbose=True,\r\n            max_iterations=settings.max_reasoning_steps\r\n        )\r\n        \r\n    async def process(self, alert_data: Dict[str, Any]) -> Dict[str, Any]:\r\n        """Process incoming alert and determine response"""\r\n        try:\r\n            # Format alert for agent processing\r\n            formatted_input = self.format_alert_input(alert_data)\r\n            \r\n            # Process with reasoning agent\r\n            response = await self.agent.arun(formatted_input)\r\n            \r\n            # Parse and structure response\r\n            structured_response = self.parse_agent_response(response)\r\n            \r\n            # Log interaction\r\n            self.log_interaction(alert_data, structured_response)\r\n            \r\n            return structured_response\r\n            \r\n        except Exception as e:\r\n            logger.error("Agent processing failed", error=str(e), alert_data=alert_data)\r\n            return self.create_error_response(str(e))\r\n    \r\n    def format_alert_input(self, alert_data: Dict[str, Any]) -> str:\r\n        """Format alert data for agent consumption"""\r\n        return f"""\r\n        INCIDENT ALERT:\r\n        \r\n        Severity: {alert_data.get(\'severity\', \'Unknown\')}\r\n        Service: {alert_data.get(\'service\', \'Unknown\')}\r\n        Message: {alert_data.get(\'message\', \'\')}\r\n        Timestamp: {alert_data.get(\'timestamp\', \'\')}\r\n        Metrics: {alert_data.get(\'metrics\', {})}\r\n        \r\n        Please analyze this incident and provide:\r\n        1. Initial assessment and severity confirmation\r\n        2. Recommended investigation steps\r\n        3. Potential root causes to explore\r\n        4. Immediate actions to take\r\n        5. Team to notify and escalation path\r\n        \r\n        If you need additional information, use the available tools to search logs,\r\n        check related systems, or gather more context.\r\n        """\r\n    \r\n    def parse_agent_response(self, response: str) -> Dict[str, Any]:\r\n        """Parse agent response into structured format"""\r\n        return {\r\n            "timestamp": datetime.utcnow().isoformat(),\r\n            "agent_id": self.id,\r\n            "response": response,\r\n            "actions_taken": self.extract_actions_taken(),\r\n            "confidence_score": self.calculate_confidence_score(response),\r\n            "escalation_required": self.requires_escalation(response)\r\n        }\r\n    \r\n    def extract_actions_taken(self) -> List[Dict[str, Any]]:\r\n        """Extract actions taken during processing"""\r\n        actions = []\r\n        for tool_call in self.agent.intermediate_steps:\r\n            actions.append({\r\n                "tool": tool_call[0].tool,\r\n                "input": tool_call[0].tool_input,\r\n                "output": tool_call[1]\r\n            })\r\n        return actions\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83D\uDEE0️ Phase 4: Tool Integration & Testing"}),"\n",(0,t.jsx)(n.h3,{children:"Tool Development Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/tools/base_tool.py\r\nfrom langchain.tools import BaseTool\r\nfrom abc import abstractmethod\r\nfrom typing import Any, Dict\r\nimport asyncio\r\n\r\nclass BaseAgentTool(BaseTool):\r\n    """Base class for all agent tools with common functionality"""\r\n    \r\n    def __init__(self, name: str, description: str):\r\n        super().__init__(name=name, description=description)\r\n        self.usage_count = 0\r\n        self.error_count = 0\r\n    \r\n    def _run(self, *args, **kwargs) -> Any:\r\n        """Synchronous run with error handling and metrics"""\r\n        try:\r\n            self.usage_count += 1\r\n            TOOL_USAGE.labels(tool_name=self.name, status=\'attempted\').inc()\r\n            \r\n            result = self.execute(*args, **kwargs)\r\n            \r\n            TOOL_USAGE.labels(tool_name=self.name, status=\'success\').inc()\r\n            return result\r\n            \r\n        except Exception as e:\r\n            self.error_count += 1\r\n            TOOL_USAGE.labels(tool_name=self.name, status=\'error\').inc()\r\n            logger.error("Tool execution failed", tool=self.name, error=str(e))\r\n            return {"error": str(e), "tool": self.name}\r\n    \r\n    async def _arun(self, *args, **kwargs) -> Any:\r\n        """Asynchronous run"""\r\n        return await asyncio.get_event_loop().run_in_executor(\r\n            None, self._run, *args, **kwargs\r\n        )\r\n    \r\n    @abstractmethod\r\n    def execute(self, *args, **kwargs) -> Any:\r\n        """Tool-specific execution logic"""\r\n        pass\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Log Search Tool Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/tools/log_search.py\r\nfrom elasticsearch import Elasticsearch\r\nfrom typing import Dict, List, Any\r\n\r\nclass LogSearchTool(BaseAgentTool):\r\n    def __init__(self, elasticsearch_url: str):\r\n        super().__init__(\r\n            name="log_search",\r\n            description="Search application and system logs for patterns, errors, and events"\r\n        )\r\n        self.es_client = Elasticsearch([elasticsearch_url])\r\n    \r\n    def execute(self, query: str, time_range: str = "1h", max_results: int = 50) -> Dict[str, Any]:\r\n        """Search logs using Elasticsearch"""\r\n        try:\r\n            search_body = {\r\n                "query": {\r\n                    "bool": {\r\n                        "must": [\r\n                            {"query_string": {"query": query}},\r\n                            {"range": {"@timestamp": {"gte": "now-{time_range}".format(time_range)}}}\r\n                        ]\r\n                    }\r\n                },\r\n                "sort": [{"@timestamp": {"order": "desc"}}],\r\n                "size": max_results\r\n            }\r\n            \r\n            response = self.es_client.search(index="logs-*", body=search_body)\r\n            \r\n            hits = response["hits"]["hits"]\r\n            results = []\r\n            \r\n            for hit in hits:\r\n                source = hit["_source"]\r\n                results.append({\r\n                    "timestamp": source.get("@timestamp"),\r\n                    "level": source.get("level"),\r\n                    "message": source.get("message"),\r\n                    "service": source.get("service"),\r\n                    "host": source.get("host")\r\n                })\r\n            \r\n            return {\r\n                "total_hits": response["hits"]["total"]["value"],\r\n                "results": results,\r\n                "query": query,\r\n                "time_range": time_range\r\n            }\r\n            \r\n        except Exception as e:\r\n            return {"error": "Log search failed: {str(e)}".format(str(e))}\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Tool Testing Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# tests/test_tools.py\r\nimport pytest\r\nimport asyncio\r\nfrom unittest.mock import Mock, patch\r\nfrom src.tools.log_search import LogSearchTool\r\n\r\nclass TestLogSearchTool:\r\n    @pytest.fixture\r\n    def mock_elasticsearch(self):\r\n        with patch(\'src.tools.log_search.Elasticsearch\') as mock_es:\r\n            mock_client = Mock()\r\n            mock_es.return_value = mock_client\r\n            yield mock_client\r\n    \r\n    @pytest.fixture\r\n    def log_search_tool(self, mock_elasticsearch):\r\n        return LogSearchTool("http://localhost:9200")\r\n    \r\n    def test_successful_log_search(self, log_search_tool, mock_elasticsearch):\r\n        # Mock Elasticsearch response\r\n        mock_response = {\r\n            "hits": {\r\n                "total": {"value": 10},\r\n                "hits": [\r\n                    {\r\n                        "_source": {\r\n                            "@timestamp": "2025-06-26T10:00:00Z",\r\n                            "level": "ERROR",\r\n                            "message": "Database connection failed",\r\n                            "service": "api-service",\r\n                            "host": "web-01"\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\n        }\r\n        mock_elasticsearch.search.return_value = mock_response\r\n        \r\n        # Execute tool\r\n        result = log_search_tool.execute("ERROR database", "1h")\r\n        \r\n        # Assertions\r\n        assert result["total_hits"] == 10\r\n        assert len(result["results"]) == 1\r\n        assert result["results"][0]["level"] == "ERROR"\r\n        assert "database" in result["results"][0]["message"].lower()\r\n    \r\n    def test_log_search_error_handling(self, log_search_tool, mock_elasticsearch):\r\n        # Mock Elasticsearch error\r\n        mock_elasticsearch.search.side_effect = Exception("Connection timeout")\r\n        \r\n        # Execute tool\r\n        result = log_search_tool.execute("test query")\r\n        \r\n        # Assertions\r\n        assert "error" in result\r\n        assert "Connection timeout" in result["error"]\r\n    \r\n    @pytest.mark.asyncio\r\n    async def test_async_log_search(self, log_search_tool, mock_elasticsearch):\r\n        # Mock successful response\r\n        mock_response = {"hits": {"total": {"value": 0}, "hits": []}}\r\n        mock_elasticsearch.search.return_value = mock_response\r\n        \r\n        # Execute async tool\r\n        result = await log_search_tool._arun("async test query")\r\n        \r\n        # Assertions\r\n        assert result["total_hits"] == 0\r\n        assert isinstance(result["results"], list)\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Integration Testing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# tests/test_agent_integration.py\r\nimport pytest\r\nfrom unittest.mock import AsyncMock, patch\r\nfrom src.agents.incident_agent import IncidentHandlingAgent\r\nfrom config.settings import AgentSettings\r\n\r\nclass TestAgentIntegration:\r\n    @pytest.fixture\r\n    def mock_settings(self):\r\n        return AgentSettings(\r\n            openai_api_key="test-key",\r\n            redis_url="redis://localhost:6379",\r\n            elasticsearch_url="http://localhost:9200",\r\n            jira_url="https://test.atlassian.net",\r\n            jira_token="test-token",\r\n            slack_token="test-slack-token"\r\n        )\r\n    \r\n    @pytest.fixture\r\n    def agent(self, mock_settings):\r\n        with patch(\'src.agents.incident_agent.ChatOpenAI\'), \\\r\n             patch(\'src.tools.log_search.Elasticsearch\'), \\\r\n             patch(\'src.tools.ticket_creation.JIRA\'), \\\r\n             patch(\'src.tools.notification.WebClient\'):\r\n            return IncidentHandlingAgent(mock_settings)\r\n    \r\n    @pytest.mark.asyncio\r\n    async def test_end_to_end_incident_processing(self, agent):\r\n        # Mock alert data\r\n        alert_data = {\r\n            "severity": "HIGH",\r\n            "service": "payment-api",\r\n            "message": "High error rate detected",\r\n            "timestamp": "2025-06-26T10:00:00Z",\r\n            "metrics": {"error_rate": 0.15, "response_time": 2000}\r\n        }\r\n        \r\n        # Mock agent response\r\n        with patch.object(agent.agent, \'arun\') as mock_run:\r\n            mock_run.return_value = """\r\n            INCIDENT ANALYSIS:\r\n            1. Confirmed HIGH severity incident in payment-api\r\n            2. Error rate spike to 15% indicates service degradation\r\n            3. Response time increase suggests resource contention\r\n            \r\n            ACTIONS TAKEN:\r\n            - Searched logs for error patterns\r\n            - Created incident ticket INC-12345\r\n            - Notified payment team via Slack\r\n            \r\n            RECOMMENDATIONS:\r\n            - Scale up payment-api instances\r\n            - Check database connection pool\r\n            - Monitor for recovery within 15 minutes\r\n            """\r\n            \r\n            # Execute agent\r\n            result = await agent.process(alert_data)\r\n            \r\n            # Assertions\r\n            assert result["agent_id"] == agent.id\r\n            assert "INCIDENT ANALYSIS" in result["response"]\r\n            assert not result["escalation_required"]\r\n            assert result["confidence_score"] > 0.5\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83D\uDCBE Phase 5: Memory & State Management"}),"\n",(0,t.jsx)(n.h3,{children:"Memory System Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/memory/memory_manager.py\r\nimport redis\r\nimport json\r\nfrom typing import Dict, Any, List, Optional\r\nfrom datetime import datetime, timedelta\r\n\r\nclass AgentMemoryManager:\r\n    def __init__(self, redis_url: str, ttl: int = 3600):\r\n        self.redis_client = redis.from_url(redis_url)\r\n        self.ttl = ttl\r\n    \r\n    def store_conversation(self, agent_id: str, conversation_data: Dict[str, Any]):\r\n        """Store conversation history for an agent"""\r\n        key = "conversation:{agent_id}".format(agent_id)\r\n        \r\n        # Get existing conversation or create new\r\n        existing = self.redis_client.get(key)\r\n        if existing:\r\n            conversation = json.loads(existing)\r\n        else:\r\n            conversation = {"agent_id": agent_id, "messages": [], "created_at": datetime.utcnow().isoformat()}\r\n        \r\n        # Add new message\r\n        conversation["messages"].append({\r\n            "timestamp": datetime.utcnow().isoformat(),\r\n            "data": conversation_data\r\n        })\r\n        \r\n        # Store with TTL\r\n        self.redis_client.setex(key, self.ttl, json.dumps(conversation))\r\n    \r\n    def get_conversation_history(self, agent_id: str, limit: int = 10) -> List[Dict[str, Any]]:\r\n        """Retrieve conversation history for an agent"""\r\n        key = "conversation:{agent_id}".format(agent_id)\r\n        data = self.redis_client.get(key)\r\n        \r\n        if not data:\r\n            return []\r\n        \r\n        conversation = json.loads(data)\r\n        messages = conversation.get("messages", [])\r\n        \r\n        # Return most recent messages\r\n        return messages[-limit:] if len(messages) > limit else messages\r\n    \r\n    def store_incident_context(self, incident_id: str, context: Dict[str, Any]):\r\n        """Store incident-specific context and resolution data"""\r\n        key = "incident:{incident_id}".format(incident_id)\r\n        \r\n        context_data = {\r\n            "incident_id": incident_id,\r\n            "created_at": datetime.utcnow().isoformat(),\r\n            "context": context,\r\n            "resolution_status": "in_progress"\r\n        }\r\n        \r\n        # Store incident context with longer TTL (24 hours)\r\n        self.redis_client.setex(key, 86400, json.dumps(context_data))\r\n    \r\n    def search_similar_incidents(self, current_incident: Dict[str, Any]) -> List[Dict[str, Any]]:\r\n        """Find similar past incidents for pattern matching"""\r\n        # Simple implementation - in production, use vector similarity\r\n        all_incidents = []\r\n        \r\n        # Get all incident keys\r\n        incident_keys = self.redis_client.keys("incident:*")\r\n        \r\n        for key in incident_keys:\r\n            data = self.redis_client.get(key)\r\n            if data:\r\n                incident_data = json.loads(data)\r\n                \r\n                # Simple similarity check (service + error type)\r\n                if self.calculate_similarity(current_incident, incident_data["context"]) > 0.7:\r\n                    all_incidents.append(incident_data)\r\n        \r\n        return sorted(all_incidents, key=lambda x: x["created_at"], reverse=True)[:5]\r\n    \r\n    def calculate_similarity(self, incident1: Dict[str, Any], incident2: Dict[str, Any]) -> float:\r\n        """Calculate similarity score between incidents"""\r\n        score = 0.0\r\n        \r\n        # Service match\r\n        if incident1.get("service") == incident2.get("service"):\r\n            score += 0.4\r\n        \r\n        # Severity match\r\n        if incident1.get("severity") == incident2.get("severity"):\r\n            score += 0.2\r\n        \r\n        # Error pattern match (simplified)\r\n        message1 = incident1.get("message", "").lower()\r\n        message2 = incident2.get("message", "").lower()\r\n        \r\n        common_words = set(message1.split()) & set(message2.split())\r\n        if len(common_words) > 2:\r\n            score += 0.4\r\n        \r\n        return score\n'})}),"\n",(0,t.jsx)(n.h3,{children:"State Management for Long-Running Tasks"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/memory/state_manager.py\r\nfrom enum import Enum\r\nfrom typing import Dict, Any, Optional\r\nimport json\r\n\r\nclass TaskState(Enum):\r\n    PENDING = "pending"\r\n    IN_PROGRESS = "in_progress"\r\n    COMPLETED = "completed"\r\n    FAILED = "failed"\r\n    ESCALATED = "escalated"\r\n\r\nclass TaskStateManager:\r\n    def __init__(self, memory_manager: AgentMemoryManager):\r\n        self.memory = memory_manager\r\n    \r\n    def create_task(self, task_id: str, task_data: Dict[str, Any]) -> None:\r\n        """Create a new task with initial state"""\r\n        task = {\r\n            "task_id": task_id,\r\n            "state": TaskState.PENDING.value,\r\n            "created_at": datetime.utcnow().isoformat(),\r\n            "data": task_data,\r\n            "steps": [],\r\n            "progress": 0.0\r\n        }\r\n        \r\n        key = "task:{task_id}".format(task_id)\r\n        self.memory.redis_client.setex(key, 7200, json.dumps(task))  # 2 hour TTL\r\n    \r\n    def update_task_state(self, task_id: str, new_state: TaskState, \r\n                         step_data: Optional[Dict[str, Any]] = None) -> None:\r\n        """Update task state and add step information"""\r\n        key = "task:{task_id}".format(task_id)\r\n        data = self.memory.redis_client.get(key)\r\n        \r\n        if not data:\r\n            raise ValueError("Task {task_id} not found".format(task_id))\r\n        \r\n        task = json.loads(data)\r\n        task["state"] = new_state.value\r\n        task["updated_at"] = datetime.utcnow().isoformat()\r\n        \r\n        if step_data:\r\n            task["steps"].append({\r\n                "timestamp": datetime.utcnow().isoformat(),\r\n                "step_data": step_data\r\n            })\r\n            \r\n            # Update progress based on steps\r\n            if new_state == TaskState.COMPLETED:\r\n                task["progress"] = 1.0\r\n            elif new_state == TaskState.FAILED:\r\n                task["progress"] = task.get("progress", 0.0)  # Keep current progress\r\n            else:\r\n                # Estimate progress based on number of steps\r\n                task["progress"] = min(0.9, len(task["steps"]) * 0.2)\r\n        \r\n        self.memory.redis_client.setex(key, 7200, json.dumps(task))\r\n    \r\n    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:\r\n        """Get current task status and progress"""\r\n        key = "task:{task_id}".format(task_id)\r\n        data = self.memory.redis_client.get(key)\r\n        \r\n        if not data:\r\n            return None\r\n        \r\n        return json.loads(data)\r\n    \r\n    def get_active_tasks(self, agent_id: str) -> List[Dict[str, Any]]:\r\n        """Get all active tasks for an agent"""\r\n        task_keys = self.memory.redis_client.keys(f"task:*")\r\n        active_tasks = []\r\n        \r\n        for key in task_keys:\r\n            data = self.memory.redis_client.get(key)\r\n            if data:\r\n                task = json.loads(data)\r\n                if (task.get("data", {}).get("agent_id") == agent_id and \r\n                    task["state"] in [TaskState.PENDING.value, TaskState.IN_PROGRESS.value]):\r\n                    active_tasks.append(task)\r\n        \r\n        return active_tasks\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83D\uDCCA Phase 6: Evaluation & Optimization"}),"\n",(0,t.jsx)(n.h3,{children:"Performance Metrics Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/evaluation/metrics.py\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Dict, Any\r\nimport numpy as np\r\nfrom datetime import datetime, timedelta\r\n\r\n@dataclass\r\nclass AgentPerformanceMetrics:\r\n    response_time: float\r\n    accuracy_score: float\r\n    tool_usage_efficiency: float\r\n    escalation_rate: float\r\n    user_satisfaction: float\r\n    error_rate: float\r\n\r\nclass AgentEvaluator:\r\n    def __init__(self, memory_manager: AgentMemoryManager):\r\n        self.memory = memory_manager\r\n        self.metrics_history = []\r\n    \r\n    def evaluate_response_quality(self, agent_response: str, expected_actions: List[str]) -> float:\r\n        """Evaluate quality of agent response against expected actions"""\r\n        score = 0.0\r\n        \r\n        # Check if response contains expected action keywords\r\n        response_lower = agent_response.lower()\r\n        for action in expected_actions:\r\n            if action.lower() in response_lower:\r\n                score += 1.0 / len(expected_actions)\r\n        \r\n        return score\r\n    \r\n    def calculate_response_time_metrics(self, agent_id: str, time_window: timedelta) -> Dict[str, float]:\r\n        """Calculate response time statistics"""\r\n        conversations = self.memory.get_conversation_history(agent_id, limit=100)\r\n        \r\n        response_times = []\r\n        cutoff_time = datetime.utcnow() - time_window\r\n        \r\n        for conv in conversations:\r\n            conv_time = datetime.fromisoformat(conv["timestamp"])\r\n            if conv_time > cutoff_time and "response_time" in conv["data"]:\r\n                response_times.append(conv["data"]["response_time"])\r\n        \r\n        if not response_times:\r\n            return {"mean": 0, "median": 0, "p95": 0, "p99": 0}\r\n        \r\n        return {\r\n            "mean": np.mean(response_times),\r\n            "median": np.median(response_times),\r\n            "p95": np.percentile(response_times, 95),\r\n            "p99": np.percentile(response_times, 99)\r\n        }\r\n    \r\n    def calculate_tool_efficiency(self, agent_id: str) -> float:\r\n        """Calculate tool usage efficiency (successful tool calls / total calls)"""\r\n        conversations = self.memory.get_conversation_history(agent_id, limit=50)\r\n        \r\n        total_tool_calls = 0\r\n        successful_calls = 0\r\n        \r\n        for conv in conversations:\r\n            actions = conv["data"].get("actions_taken", [])\r\n            for action in actions:\r\n                total_tool_calls += 1\r\n                if not action.get("output", {}).get("error"):\r\n                    successful_calls += 1\r\n        \r\n        return successful_calls / total_tool_calls if total_tool_calls > 0 else 1.0\r\n    \r\n    def generate_performance_report(self, agent_id: str) -> Dict[str, Any]:\r\n        """Generate comprehensive performance report"""\r\n        time_window = timedelta(hours=24)\r\n        \r\n        # Calculate metrics\r\n        response_time_stats = self.calculate_response_time_metrics(agent_id, time_window)\r\n        tool_efficiency = self.calculate_tool_efficiency(agent_id)\r\n        \r\n        # Get recent conversations for analysis\r\n        recent_conversations = self.memory.get_conversation_history(agent_id, limit=20)\r\n        \r\n        # Calculate escalation rate\r\n        escalations = sum(1 for conv in recent_conversations \r\n                         if conv["data"].get("escalation_required", False))\r\n        escalation_rate = escalations / len(recent_conversations) if recent_conversations else 0\r\n        \r\n        # Calculate error rate\r\n        errors = sum(1 for conv in recent_conversations \r\n                    if "error" in conv["data"].get("response", "").lower())\r\n        error_rate = errors / len(recent_conversations) if recent_conversations else 0\r\n        \r\n        return {\r\n            "agent_id": agent_id,\r\n            "evaluation_timestamp": datetime.utcnow().isoformat(),\r\n            "time_window": str(time_window),\r\n            "response_time": response_time_stats,\r\n            "tool_efficiency": tool_efficiency,\r\n            "escalation_rate": escalation_rate,\r\n            "error_rate": error_rate,\r\n            "total_interactions": len(recent_conversations),\r\n            "recommendations": self.generate_recommendations(\r\n                tool_efficiency, escalation_rate, error_rate\r\n            )\r\n        }\r\n    \r\n    def generate_recommendations(self, tool_efficiency: float, \r\n                               escalation_rate: float, error_rate: float) -> List[str]:\r\n        """Generate optimization recommendations based on metrics"""\r\n        recommendations = []\r\n        \r\n        if tool_efficiency < 0.8:\r\n            recommendations.append("Improve tool error handling and validation")\r\n        \r\n        if escalation_rate > 0.3:\r\n            recommendations.append("Review agent confidence thresholds and decision criteria")\r\n        \r\n        if error_rate > 0.1:\r\n            recommendations.append("Enhance prompt engineering and add more examples")\r\n        \r\n        if not recommendations:\r\n            recommendations.append("Performance is within acceptable ranges")\r\n        \r\n        return recommendations\n'})}),"\n",(0,t.jsx)(n.h3,{children:"A/B Testing Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/evaluation/ab_testing.py\r\nimport random\r\nfrom typing import Dict, Any, Optional\r\nfrom enum import Enum\r\n\r\nclass VariantType(Enum):\r\n    CONTROL = "control"\r\n    TREATMENT = "treatment"\r\n\r\nclass ABTestManager:\r\n    def __init__(self, memory_manager: AgentMemoryManager):\r\n        self.memory = memory_manager\r\n        self.active_tests = {}\r\n    \r\n    def create_test(self, test_id: str, test_config: Dict[str, Any]) -> None:\r\n        """Create a new A/B test configuration"""\r\n        test = {\r\n            "test_id": test_id,\r\n            "config": test_config,\r\n            "created_at": datetime.utcnow().isoformat(),\r\n            "participants": {},\r\n            "results": {"control": [], "treatment": []}\r\n        }\r\n        \r\n        self.active_tests[test_id] = test\r\n        \r\n        # Store in Redis for persistence\r\n        key = "abtest:{test_id}".format(test_id)\r\n        self.memory.redis_client.setex(key, 604800, json.dumps(test))  # 7 days\r\n    \r\n    def assign_variant(self, test_id: str, user_id: str) -> VariantType:\r\n        """Assign user to control or treatment group"""\r\n        test = self.active_tests.get(test_id)\r\n        if not test:\r\n            return VariantType.CONTROL\r\n        \r\n        # Check if user already assigned\r\n        if user_id in test["participants"]:\r\n            return VariantType(test["participants"][user_id])\r\n        \r\n        # Assign randomly (50/50 split)\r\n        variant = VariantType.TREATMENT if random.random() < 0.5 else VariantType.CONTROL\r\n        test["participants"][user_id] = variant.value\r\n        \r\n        # Update stored test\r\n        key = "abtest:{test_id}".format(test_id)\r\n        self.memory.redis_client.setex(key, 604800, json.dumps(test))\r\n        \r\n        return variant\r\n    \r\n    def record_result(self, test_id: str, user_id: str, result_data: Dict[str, Any]) -> None:\r\n        """Record test result for analysis"""\r\n        test = self.active_tests.get(test_id)\r\n        if not test:\r\n            return\r\n        \r\n        variant = test["participants"].get(user_id)\r\n        if variant:\r\n            test["results"][variant].append({\r\n                "user_id": user_id,\r\n                "timestamp": datetime.utcnow().isoformat(),\r\n                "data": result_data\r\n            })\r\n            \r\n            # Update stored test\r\n            key = "abtest:{test_id}".format(test_id)\r\n            self.memory.redis_client.setex(key, 604800, json.dumps(test))\r\n    \r\n    def analyze_test_results(self, test_id: str) -> Dict[str, Any]:\r\n        """Analyze A/B test results for statistical significance"""\r\n        test = self.active_tests.get(test_id)\r\n        if not test:\r\n            return {"error": "Test not found"}\r\n        \r\n        control_results = test["results"]["control"]\r\n        treatment_results = test["results"]["treatment"]\r\n        \r\n        if len(control_results) < 10 or len(treatment_results) < 10:\r\n            return {"error": "Insufficient data for analysis", "min_required": 10}\r\n        \r\n        # Calculate key metrics\r\n        control_success_rate = self.calculate_success_rate(control_results)\r\n        treatment_success_rate = self.calculate_success_rate(treatment_results)\r\n        \r\n        control_avg_response_time = self.calculate_avg_response_time(control_results)\r\n        treatment_avg_response_time = self.calculate_avg_response_time(treatment_results)\r\n        \r\n        return {\r\n            "test_id": test_id,\r\n            "sample_sizes": {\r\n                "control": len(control_results),\r\n                "treatment": len(treatment_results)\r\n            },\r\n            "success_rates": {\r\n                "control": control_success_rate,\r\n                "treatment": treatment_success_rate,\r\n                "improvement": treatment_success_rate - control_success_rate\r\n            },\r\n            "response_times": {\r\n                "control": control_avg_response_time,\r\n                "treatment": treatment_avg_response_time,\r\n                "improvement": control_avg_response_time - treatment_avg_response_time\r\n            },\r\n            "recommendation": self.generate_test_recommendation(\r\n                control_success_rate, treatment_success_rate,\r\n                control_avg_response_time, treatment_avg_response_time\r\n            )\r\n        }\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83D\uDE80 Phase 7: Production Deployment & Monitoring"}),"\n",(0,t.jsx)(n.h3,{children:"Deployment Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# deployment/docker/Dockerfile\r\nFROM python:3.11-slim\r\n\r\nWORKDIR /app\r\n\r\n# Install system dependencies\r\nRUN apt-get update && apt-get install -y \\\r\n    build-essential \\\r\n    curl \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Copy requirements and install Python dependencies\r\nCOPY requirements.txt .\r\nRUN pip install --no-cache-dir -r requirements.txt\r\n\r\n# Copy application code\r\nCOPY src/ ./src/\r\nCOPY config/ ./config/\r\n\r\n# Create non-root user\r\nRUN useradd -m -u 1000 agent && chown -R agent:agent /app\r\nUSER agent\r\n\r\n# Health check\r\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\r\n    CMD curl -f http://localhost:8000/health || exit 1\r\n\r\n# Run application\r\nCMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Production API Server"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/main.py\r\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nfrom pydantic import BaseModel\r\nfrom typing import Dict, Any\r\nimport uvicorn\r\nfrom prometheus_client import make_asgi_app\r\n\r\nfrom src.agents.incident_agent import IncidentHandlingAgent\r\nfrom src.memory.memory_manager import AgentMemoryManager\r\nfrom src.evaluation.metrics import AgentEvaluator\r\nfrom config.settings import settings\r\n\r\napp = FastAPI(title="AI Incident Agent", version="1.0.0")\r\n\r\n# Add CORS middleware\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=["*"],\r\n    allow_credentials=True,\r\n    allow_methods=["*"],\r\n    allow_headers=["*"],\r\n)\r\n\r\n# Initialize components\r\nmemory_manager = AgentMemoryManager(settings.redis_url, settings.memory_ttl)\r\nagent = IncidentHandlingAgent(settings)\r\nevaluator = AgentEvaluator(memory_manager)\r\n\r\n# Add Prometheus metrics endpoint\r\nmetrics_app = make_asgi_app()\r\napp.mount("/metrics", metrics_app)\r\n\r\nclass AlertRequest(BaseModel):\r\n    severity: str\r\n    service: str\r\n    message: str\r\n    timestamp: str\r\n    metrics: Dict[str, Any] = {}\r\n\r\nclass AgentResponse(BaseModel):\r\n    agent_id: str\r\n    response: str\r\n    confidence_score: float\r\n    escalation_required: bool\r\n    actions_taken: list\r\n    processing_time: float\r\n\r\n@app.post("/process-alert", response_model=AgentResponse)\r\nasync def process_alert(alert: AlertRequest, background_tasks: BackgroundTasks):\r\n    """Process incoming alert through the incident agent"""\r\n    start_time = time.time()\r\n    \r\n    try:\r\n        # Convert to dict for processing\r\n        alert_data = alert.dict()\r\n        \r\n        # Process with agent\r\n        result = await agent.process(alert_data)\r\n        \r\n        # Calculate processing time\r\n        processing_time = time.time() - start_time\r\n        result["processing_time"] = processing_time\r\n        \r\n        # Record metrics\r\n        AGENT_REQUESTS.labels(agent_type="incident", status="success").inc()\r\n        AGENT_RESPONSE_TIME.observe(processing_time)\r\n        \r\n        # Schedule background evaluation\r\n        background_tasks.add_task(\r\n            evaluator.record_interaction, \r\n            agent.id, \r\n            alert_data, \r\n            result\r\n        )\r\n        \r\n        return AgentResponse(**result)\r\n        \r\n    except Exception as e:\r\n        processing_time = time.time() - start_time\r\n        AGENT_REQUESTS.labels(agent_type="incident", status="error").inc()\r\n        AGENT_RESPONSE_TIME.observe(processing_time)\r\n        \r\n        logger.error("Alert processing failed", error=str(e), alert=alert_data)\r\n        raise HTTPException(status_code=500, detail=str(e))\r\n\r\n@app.get("/agent/{agent_id}/status")\r\nasync def get_agent_status(agent_id: str):\r\n    """Get agent status and performance metrics"""\r\n    try:\r\n        performance_report = evaluator.generate_performance_report(agent_id)\r\n        conversation_history = memory_manager.get_conversation_history(agent_id, limit=5)\r\n        \r\n        return {\r\n            "agent_id": agent_id,\r\n            "status": "active",\r\n            "performance": performance_report,\r\n            "recent_interactions": len(conversation_history),\r\n            "uptime": str(datetime.utcnow() - agent.created_at)\r\n        }\r\n        \r\n    except Exception as e:\r\n        raise HTTPException(status_code=404, detail="Agent {agent_id} not found".format(agent_id))\r\n\r\n@app.get("/health")\r\nasync def health_check():\r\n    """Health check endpoint for monitoring"""\r\n    try:\r\n        # Check Redis connection\r\n        memory_manager.redis_client.ping()\r\n        \r\n        # Check agent status\r\n        agent_status = "healthy" if agent else "unhealthy"\r\n        \r\n        return {\r\n            "status": "healthy",\r\n            "agent_status": agent_status,\r\n            "timestamp": datetime.utcnow().isoformat(),\r\n            "version": "1.0.0"\r\n        }\r\n        \r\n    except Exception as e:\r\n        raise HTTPException(status_code=503, detail="Health check failed: {str(e)}".format(str(e)))\r\n\r\nif __name__ == "__main__":\r\n    uvicorn.run(\r\n        "main:app",\r\n        host="0.0.0.0",\r\n        port=settings.metrics_port,\r\n        log_level=settings.log_level.lower(),\r\n        access_log=True\r\n    )\n'})}),"\n",(0,t.jsx)(n.h3,{children:"Monitoring and Alerting"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# deployment/monitoring/docker-compose.monitoring.yml\r\nversion: '3.8'\r\n\r\nservices:\r\n  prometheus:\r\n    image: prom/prometheus:latest\r\n    container_name: prometheus\r\n    ports:\r\n      - \"9090:9090\"\r\n    volumes:\r\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\r\n      - prometheus_data:/prometheus\r\n    command:\r\n      - '--config.file=/etc/prometheus/prometheus.yml'\r\n      - '--storage.tsdb.path=/prometheus'\r\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\r\n      - '--web.console.templates=/etc/prometheus/consoles'\r\n\r\n  grafana:\r\n    image: grafana/grafana:latest\r\n    container_name: grafana\r\n    ports:\r\n      - \"3000:3000\"\r\n    environment:\r\n      - GF_SECURITY_ADMIN_PASSWORD=admin\r\n    volumes:\r\n      - grafana_data:/var/lib/grafana\r\n      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards\r\n      - ./grafana/datasources:/etc/grafana/provisioning/datasources\r\n\r\n  alertmanager:\r\n    image: prom/alertmanager:latest\r\n    container_name: alertmanager\r\n    ports:\r\n      - \"9093:9093\"\r\n    volumes:\r\n      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml\r\n\r\nvolumes:\r\n  prometheus_data:\r\n  grafana_data:\n"})}),"\n",(0,t.jsx)(n.h3,{children:"Production Checklist"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# scripts/production_checklist.py\r\n"""\r\nProduction Deployment Checklist for AI Agents\r\n"""\r\n\r\nPRODUCTION_CHECKLIST = {\r\n    "Security": [\r\n        "API keys stored in secure vault (not environment variables)",\r\n        "Rate limiting implemented on all endpoints", \r\n        "Input validation and sanitization",\r\n        "Authentication and authorization configured",\r\n        "Audit logging enabled for all agent actions",\r\n        "Network security groups configured"\r\n    ],\r\n    \r\n    "Monitoring": [\r\n        "Prometheus metrics collection configured",\r\n        "Grafana dashboards deployed",\r\n        "Alerting rules defined for critical metrics",\r\n        "Log aggregation and search configured", \r\n        "Health check endpoints implemented",\r\n        "Error tracking and notification setup"\r\n    ],\r\n    \r\n    "Performance": [\r\n        "Load testing completed",\r\n        "Response time targets defined and monitored",\r\n        "Resource limits and auto-scaling configured",\r\n        "Database connection pooling optimized",\r\n        "Caching strategy implemented",\r\n        "Background task queue configured"\r\n    ],\r\n    \r\n    "Reliability": [\r\n        "Circuit breakers implemented for external services",\r\n        "Retry logic with exponential backoff",\r\n        "Graceful degradation for tool failures",\r\n        "Database backup and recovery procedures",\r\n        "Disaster recovery plan documented",\r\n        "Rolling deployment strategy configured"\r\n    ],\r\n    \r\n    "Agent Quality": [\r\n        "A/B testing framework deployed",\r\n        "Performance benchmarks established",\r\n        "Human feedback collection implemented",\r\n        "Model version management configured",\r\n        "Prompt version control and testing",\r\n        "Escalation procedures documented"\r\n    ]\r\n}\r\n\r\ndef verify_production_readiness():\r\n    """Run production readiness checks"""\r\n    print("\uD83D\uDE80 Production Readiness Checklist")\r\n    print("=" * 50)\r\n    \r\n    for category, items in PRODUCTION_CHECKLIST.items():\r\n        print("\\n\uD83D\uDCCB {category}:".format(category))\r\n        for item in items:\r\n            # In a real implementation, these would be actual checks\r\n            status = "✅" if verify_item(item) else "❌"\r\n            print("  {status} {item}".format(item))\r\n\r\ndef verify_item(item: str) -> bool:\r\n    """Verify individual checklist item (placeholder)"""\r\n    # Implement actual verification logic\r\n    return True\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{children:"\uD83C\uDFAF Best Practices Summary"}),"\n",(0,t.jsx)(n.h3,{children:"Development Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Start Simple"}),": Begin with basic functionality and iterate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test Early"}),": Implement testing from the beginning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitor Everything"}),": Add observability at every layer"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Version Control"}),": Track prompts, configurations, and models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Security First"}),": Implement security controls from day one"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{children:"Production Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gradual Rollout"}),": Deploy to small percentage of traffic first"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human Oversight"}),": Always maintain human-in-the-loop for critical decisions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Continuous Evaluation"}),": Regularly assess and improve agent performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Documentation"}),": Maintain comprehensive operational documentation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Incident Response"}),": Have clear procedures for agent failures"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{children:"Performance Optimization"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Caching"}),": Cache frequently accessed data and responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Async Processing"}),": Use async operations for I/O bound tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Connection Pooling"}),": Optimize database and API connections"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource Management"}),": Monitor and limit resource usage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tool Optimization"}),": Regularly review and optimize tool performance"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"This comprehensive guide provides a solid foundation for developing production-ready AI agents. Remember that agent development is an iterative process - start with the basics, gather feedback, and continuously improve based on real-world performance and user needs."}),"\n",(0,t.jsxs)(n.p,{children:["In our next post, we'll explore ",(0,t.jsx)(n.strong,{children:"Multi-Agent Architectures"})," and how to coordinate multiple specialized agents for complex workflows."]})]})}function c(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},48105:function(e,n,r){r.r(n),r.d(n,{default:function(){return c},frontmatter:function(){return a},metadata:function(){return i}});var t=r(57437),s=r(52671);let a=void 0,i={postId:"261909b7-5dee-49e2-91c3-45fc3e702e2c",title:"System Design Interview: Step By Step Guide",date:"2024-03-24 23:37:07 +0530",excerpt:"Excerpt for System Design Interview: Step By Step Guide",author:"Abstract Algorithms",tags:["uncategorized"],status:"draft"};function o(e){return(0,t.jsx)(t.Fragment,{})}function c(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},55034:function(e,n,r){r.r(n),r.d(n,{default:function(){return c},frontmatter:function(){return a},metadata:function(){return i}});var t=r(57437),s=r(52671);let a=void 0,i={postId:"d8b23b58-8a45-4c21-8ebc-04795f556d30",title:"System Design Patterns: Change Data Capture Pattern",date:"2023-01-09 11:06:13 +0530",excerpt:"Excerpt for System Design Patterns: Change Data Capture Pattern",author:"Abstract Algorithms",tags:["uncategorized"],status:"draft"};function o(e){return(0,t.jsx)(t.Fragment,{})}function c(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},16526:function(e,n,r){r.r(n),r.d(n,{default:function(){return c},frontmatter:function(){return a},metadata:function(){return i}});var t=r(57437),s=r(52671);let a=void 0,i={postId:"2e8581f8-3d55-4686-a39e-83365a2b71a7",title:"Terraform: Getting Started",date:"2023-05-27 20:27:01 +0530",excerpt:"Excerpt for Terraform: Getting Started",author:"Abstract Algorithms",tags:["uncategorized"],status:"draft"};function o(e){return(0,t.jsx)(t.Fragment,{})}function c(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}}}]);