<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/posts/microservices-outbox-pattern/assets/overview.png" fetchPriority="high"/><link rel="stylesheet" href="/_next/static/css/275ed64cc4367444.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/c8b6ee85b5abc035.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b082bb110b1b278c.js"/><script src="/_next/static/chunks/vendors-aacc2dbb-3a7157755f8f47e3.js" async=""></script><script src="/_next/static/chunks/vendors-37a93c5f-7e2c02ed1d307f6c.js" async=""></script><script src="/_next/static/chunks/vendors-074c20c4-f873ace3944d24d6.js" async=""></script><script src="/_next/static/chunks/vendors-b9fa02b6-6171508ee50fe21a.js" async=""></script><script src="/_next/static/chunks/vendors-b0389ab8-170377b7b5b8a952.js" async=""></script><script src="/_next/static/chunks/vendors-3f88d8a8-dbdfd4398b8e8ee0.js" async=""></script><script src="/_next/static/chunks/vendors-052d92a9-e538202a8c5e3b10.js" async=""></script><script src="/_next/static/chunks/vendors-938ded93-ba94ff69859aa26a.js" async=""></script><script src="/_next/static/chunks/vendors-42f1a597-b22f03b7c25146af.js" async=""></script><script src="/_next/static/chunks/vendors-27f02048-4f94103112d37eb5.js" async=""></script><script src="/_next/static/chunks/vendors-4a7382ad-b399a3edfa9808ab.js" async=""></script><script src="/_next/static/chunks/vendors-362d063c-e6276985323a06ec.js" async=""></script><script src="/_next/static/chunks/vendors-9c587c8a-d2783e507f5d62a0.js" async=""></script><script src="/_next/static/chunks/vendors-05e245ef-1a4ab328b8ce9ef2.js" async=""></script><script src="/_next/static/chunks/vendors-d7c15829-2678d0470800ed7b.js" async=""></script><script src="/_next/static/chunks/vendors-6808aa01-9f52964abee5a964.js" async=""></script><script src="/_next/static/chunks/vendors-351e52ed-a9aa59bdfe53186c.js" async=""></script><script src="/_next/static/chunks/vendors-98a6762f-c2827647527b77c4.js" async=""></script><script src="/_next/static/chunks/vendors-bc692b9d-c55c35306d4d77bf.js" async=""></script><script src="/_next/static/chunks/vendors-e3e804e2-d9f06ce46a4dcab4.js" async=""></script><script src="/_next/static/chunks/vendors-a6f90180-ba8559790eb92e44.js" async=""></script><script src="/_next/static/chunks/vendors-d91c2bd6-e0f15c37863d1bdc.js" async=""></script><script src="/_next/static/chunks/vendors-2898f16f-8909755dc98c76f6.js" async=""></script><script src="/_next/static/chunks/vendors-6633164b-4565905b24af7fe3.js" async=""></script><script src="/_next/static/chunks/vendors-8cbd2506-fd05960a986f3395.js" async=""></script><script src="/_next/static/chunks/vendors-377fed06-65d4183f60271601.js" async=""></script><script src="/_next/static/chunks/main-app-fcbbb5bb13a4f03c.js" async=""></script><script src="/_next/static/chunks/common-8bc9aba88b3a5d2f.js" async=""></script><script src="/_next/static/chunks/app/layout-3d649b44ebe169dd.js" async=""></script><script src="/_next/static/chunks/app/error-1745ca505ccb7f84.js" async=""></script><script src="/_next/static/chunks/app/not-found-5aff7e7753541a4f.js" async=""></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-VZR168MHE2"></script><script src="/_next/static/chunks/app/posts/%5Bslug%5D/page-3331b62b71175ca5.js" async=""></script><link rel="manifest" href="/manifest.json"/><meta name="theme-color" content="#00D885"/><meta name="google-site-verification" content="D5v1M3nD8oO9DNaZKujCwBLNNqf35CTJo114uv8yMNU"/><title>Microservices: Outbox Pattern | Abstract Algorithms</title><meta name="description" content="Learn about Microservices: Outbox Pattern."/><meta name="author" content="Abstract Algorithms"/><meta name="keywords" content="algorithms,data structures,system design,software engineering,programming,computer science,performance optimization,big o notation,hash tables,database indexing"/><meta name="creator" content="Abstract Algorithms"/><meta name="publisher" content="Abstract Algorithms"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="Microservices: Outbox Pattern"/><meta property="og:description" content="Learn about Microservices: Outbox Pattern."/><meta property="og:type" content="article"/><meta property="article:published_time" content="2022-11-21 23:02:39 +0530"/><meta property="article:author" content="Abstract Algorithms"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Abstract Algorithms"/><meta name="twitter:description" content="A comprehensive blog about algorithms, data structures, system design, and software engineering best practices"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="icon" href="/icon.svg" type="image/svg+xml" sizes="32x32"/><link rel="apple-touch-icon" href="/apple-icon.svg" type="image/svg+xml" sizes="180x180"/><meta name="next-size-adjust"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"Abstract Algorithms","description":"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices","url":"https://abstractalgorithms.github.io","potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://abstractalgorithms.github.io/posts/{search_term_string}"},"query-input":"required name=search_term_string"},"publisher":{"@type":"Organization","name":"Abstract Algorithms","url":"https://abstractalgorithms.github.io"}}</script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-VZR168MHE2');
          </script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_e8ce0c"><div class="min-h-screen flex flex-col"><div class=""><header class="bg-white border-b border-gray-200 sticky top-0 z-50 backdrop-blur-sm bg-white/95"><div class="wide-container py-6"><div class="flex items-center justify-between"><a class="flex items-center space-x-3 group" href="/"><div class="w-10 h-10 bg-gradient-to-br from-green-500 to-emerald-600 rounded-xl flex items-center justify-center shadow-lg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-book-open w-6 h-6 text-white"><path d="M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z"></path><path d="M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z"></path></svg></div><span class="text-2xl font-bold text-gray-900 group-hover:text-green-600 transition-colors">Abstract Algorithms</span></a><nav class="hidden md:flex items-center space-x-12"><a class="text-gray-600 hover:text-gray-900 font-medium transition-colors text-lg" href="/">Home</a><a class="text-gray-600 hover:text-gray-900 font-medium transition-colors text-lg" href="/discover/">Discover</a><a class="text-gray-600 hover:text-gray-900 font-medium transition-colors text-lg" href="/posts/">Posts</a></nav><div class="flex items-center space-x-6"><button class="hidden md:flex items-center gap-3 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-xl transition-colors" title="Search posts (⌘K)"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search w-5 h-5"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg><span class="text-sm">Search</span><div class="flex items-center gap-1"><kbd class="px-1.5 py-0.5 text-xs bg-white border border-gray-300 rounded text-gray-500">⌘</kbd><kbd class="px-1.5 py-0.5 text-xs bg-white border border-gray-300 rounded text-gray-500">K</kbd></div></button><button class="md:hidden p-3 text-gray-600 hover:text-gray-900 rounded-xl hover:bg-gray-100 transition-colors" title="Search posts"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search w-6 h-6"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></button><div class="flex items-center min-w-[120px]"><div class="flex items-center "><div class="flex items-center gap-2 px-4 py-2 min-w-[100px] justify-center"><div class="w-6 h-6 bg-gray-200 rounded-full animate-pulse"></div><div class="w-12 h-4 bg-gray-200 rounded animate-pulse"></div></div></div></div><button class="md:hidden p-3 text-gray-600 hover:text-gray-900 rounded-xl hover:bg-gray-100 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu w-6 h-6"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button></div></div></div></header><main class="flex-grow"><article class="min-h-screen bg-gradient-to-br from-gray-50 to-gray-100"><header class="bg-white border-b border-gray-200"><div class="medium-container py-12"><div class="max-w-3xl mx-auto text-center"><div class="mb-6"><div class="flex flex-wrap justify-center gap-2 mb-6"><span class="tag">general</span></div><h1 class="text-4xl md:text-5xl font-bold text-gray-900 mb-6 leading-tight">Microservices: Outbox Pattern</h1><p class="text-xl text-gray-600 leading-relaxed">Learn about Microservices: Outbox Pattern.</p></div><div class="flex items-center justify-center space-x-6 text-gray-500"><div class="flex items-center space-x-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-user w-5 h-5"><path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></svg><span class="font-medium">Abstract Algorithms</span></div><div class="flex items-center space-x-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-5 h-5"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>over 2 years ago</span></div><div class="flex items-center space-x-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-5 h-5"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>1 min read</span></div></div><div class="flex items-center justify-center space-x-4 mt-8"><button class="flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share2 w-4 h-4"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" x2="15.42" y1="13.51" y2="17.49"></line><line x1="15.41" x2="8.59" y1="6.51" y2="10.49"></line></svg><span>Share</span></button><button class="flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-bookmark w-4 h-4"><path d="m19 21-7-4-7 4V5a2 2 0 0 1 2-2h10a2 2 0 0 1 2 2v16z"></path></svg><span>Save</span></button></div></div><div class="max-w-4xl mx-auto mt-12"><div class="relative aspect-video rounded-xl overflow-hidden shadow-lg"><img alt="Microservices: Outbox Pattern" fetchPriority="high" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/posts/microservices-outbox-pattern/assets/overview.png"/></div></div></div></header><div class="py-8"><div class="px-6"><div class="max-w-6xl mx-auto"><div class="grid grid-cols-1 lg:grid-cols-5 gap-6 lg:gap-8"><div class="hidden lg:block lg:col-span-1 order-2 lg:order-1"><div class="sticky top-32"></div></div><div class="lg:col-span-4 order-1 lg:order-2"><div class="lg:hidden mb-6"></div><div class="bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden"><div class="p-8 lg:p-12"><div class="prose prose-lg prose-gray max-w-none   prose-headings:text-gray-900 prose-headings:font-semibold   prose-h1:text-3xl prose-h1:mb-8 prose-h1:mt-0 prose-h1:leading-tight   prose-h2:text-2xl prose-h2:mt-12 prose-h2:mb-6 prose-h2:border-b prose-h2:border-gray-200 prose-h2:pb-3 prose-h2:leading-tight   prose-h3:text-xl prose-h3:mt-10 prose-h3:mb-5 prose-h3:leading-tight   prose-p:text-gray-700 prose-p:leading-relaxed prose-p:mb-6 prose-p:text-base   prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-a:font-medium   prose-strong:text-gray-900 prose-strong:font-semibold   prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-2 prose-code:py-1 prose-code:rounded prose-code:text-sm prose-code:font-medium   prose-pre:bg-gray-900 prose-pre:border prose-pre:border-gray-200 prose-pre:rounded-lg prose-pre:shadow-sm   prose-blockquote:border-l-blue-500 prose-blockquote:bg-blue-50 prose-blockquote:py-4 prose-blockquote:px-6 prose-blockquote:rounded-r-lg prose-blockquote:my-6   prose-ul:my-6 prose-ol:my-6 prose-ul:space-y-2 prose-ol:space-y-2   prose-li:my-1 prose-li:leading-relaxed   prose-table:text-sm prose-table:shadow-sm prose-table:border prose-table:border-gray-200 prose-table:rounded-lg prose-table:overflow-hidden   prose-th:bg-gray-50 prose-th:font-semibold prose-th:text-gray-800 prose-th:px-4 prose-th:py-3   prose-td:px-4 prose-td:py-3 prose-td:border-t prose-td:border-gray-200   prose-img:rounded-lg prose-img:shadow-md prose-img:my-8"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div class="animate-pulse h-64 bg-gray-100 rounded"></div><!--/$--></div></div></div></div></div></div></div><div class="max-w-6xl mx-auto px-6 mt-12"><div class="bg-white rounded-xl shadow-sm border border-gray-200 p-8"><section class="giscus-comments"></section></div></div></div><div class="bg-white border-t border-gray-200"><section class="bg-gray-50 py-16"><div class="medium-container"><div class="flex items-center justify-between mb-8"><div><h2 class="text-3xl font-bold text-gray-900 mb-2">More Articles</h2><p class="text-gray-600">Continue exploring algorithms and system design</p></div><div class="flex items-center space-x-2"><div class="sm:hidden text-xs text-gray-500 px-3 py-1 bg-gray-100 rounded-full">Swipe to explore →</div><div class="hidden sm:flex items-center space-x-2"><button class="p-2 rounded-full bg-white shadow-sm hover:shadow-md transition-shadow text-gray-600 hover:text-gray-900" aria-label="Scroll left"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-left w-5 h-5"><path d="m15 18-6-6 6-6"></path></svg></button><button class="p-2 rounded-full bg-white shadow-sm hover:shadow-md transition-shadow text-gray-600 hover:text-gray-900" aria-label="Scroll right"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right w-5 h-5"><path d="m9 18 6-6-6-6"></path></svg></button></div></div></div><div id="related-posts-carousel" class="flex gap-6 overflow-x-auto pb-4 scrollbar-hide snap-x snap-mandatory" style="scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch"><div class="snap-start"><article class="flex-shrink-0 w-80 bg-white rounded-xl shadow-sm hover:shadow-lg transition-all duration-300 overflow-hidden group"><div class="relative aspect-[16/10] overflow-hidden"><img alt="Little&#x27;s Law: Understanding Queue Performance" loading="lazy" decoding="async" data-nimg="fill" class="object-cover group-hover:scale-105 transition-transform duration-300" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/posts/little&#x27;s-law/assets/overview.png"/></div><div class="p-6 space-y-4"><h3 class="text-lg font-bold text-gray-900 leading-tight line-clamp-2"><a class="hover:text-green-600 transition-colors" href="/posts/little&#x27;s-law/">Little&#x27;s Law: Understanding Queue Performance</a></h3><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">Understanding Little&#x27;s Law and its applications in system performance analysis</p><div class="flex items-center justify-between pt-2"><div class="flex items-center space-x-3 text-xs text-gray-500"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-3 h-3"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>over 1 year ago</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-3 h-3"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>1 min read</span></div></div><a class="inline-flex items-center text-green-600 hover:text-green-700 font-medium text-sm group" href="/posts/little&#x27;s-law/">Read<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right ml-1 w-3 h-3 group-hover:translate-x-1 transition-transform"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a></div><div class="flex flex-wrap gap-2 pt-2"><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">queueing-theory</span><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">performance</span><span class="text-xs text-gray-500 px-2 py-1">+<!-- -->2</span></div></div></article></div><div class="snap-start"><article class="flex-shrink-0 w-80 bg-white rounded-xl shadow-sm hover:shadow-lg transition-all duration-300 overflow-hidden group"><div class="relative aspect-[16/10] overflow-hidden"><img alt="LLM Engineering Mastery: Part 3 - Production Deployment and Scaling" loading="lazy" decoding="async" data-nimg="fill" class="object-cover group-hover:scale-105 transition-transform duration-300" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/posts/llm-engineering-mastery-part-3/assets/production-deployment-cover.png"/></div><div class="p-6 space-y-4"><h3 class="text-lg font-bold text-gray-900 leading-tight line-clamp-2"><a class="hover:text-green-600 transition-colors" href="/posts/llm-engineering-mastery-part-3/">LLM Engineering Mastery: Part 3 - Production Deployment and Scaling</a></h3><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">Part 3 of the LLM Engineering Mastery series: Master production deployment, scaling strategies, monitoring, and security for enterprise-grade LLM applications.</p><div class="flex items-center justify-between pt-2"><div class="flex items-center space-x-3 text-xs text-gray-500"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-3 h-3"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>over 1 year ago</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-3 h-3"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>19 min read</span></div></div><a class="inline-flex items-center text-green-600 hover:text-green-700 font-medium text-sm group" href="/posts/llm-engineering-mastery-part-3/">Read<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right ml-1 w-3 h-3 group-hover:translate-x-1 transition-transform"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a></div><div class="flex flex-wrap gap-2 pt-2"><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">llm</span><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">production</span><span class="text-xs text-gray-500 px-2 py-1">+<!-- -->4</span></div></div></article></div><div class="snap-start"><article class="flex-shrink-0 w-80 bg-white rounded-xl shadow-sm hover:shadow-lg transition-all duration-300 overflow-hidden group"><div class="relative aspect-[16/10] overflow-hidden"><img alt="LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems" loading="lazy" decoding="async" data-nimg="fill" class="object-cover group-hover:scale-105 transition-transform duration-300" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/posts/llm-engineering-mastery-part-2/assets/prompt-engineering-rag-cover.png"/></div><div class="p-6 space-y-4"><h3 class="text-lg font-bold text-gray-900 leading-tight line-clamp-2"><a class="hover:text-green-600 transition-colors" href="/posts/llm-engineering-mastery-part-2/">LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems</a></h3><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">Part 2 of the LLM Engineering Mastery series: Master advanced prompt engineering techniques and build production-ready RAG systems for enhanced LLM applications.</p><div class="flex items-center justify-between pt-2"><div class="flex items-center space-x-3 text-xs text-gray-500"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-3 h-3"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>over 1 year ago</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-3 h-3"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>16 min read</span></div></div><a class="inline-flex items-center text-green-600 hover:text-green-700 font-medium text-sm group" href="/posts/llm-engineering-mastery-part-2/">Read<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right ml-1 w-3 h-3 group-hover:translate-x-1 transition-transform"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a></div><div class="flex flex-wrap gap-2 pt-2"><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">llm</span><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">prompt-engineering</span><span class="text-xs text-gray-500 px-2 py-1">+<!-- -->3</span></div></div></article></div><div class="snap-start"><article class="flex-shrink-0 w-80 bg-white rounded-xl shadow-sm hover:shadow-lg transition-all duration-300 overflow-hidden group"><div class="relative aspect-[16/10] overflow-hidden"><img alt="LLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models" loading="lazy" decoding="async" data-nimg="fill" class="object-cover group-hover:scale-105 transition-transform duration-300" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/posts/llm-engineering-mastery-part-1/assets/foundation-models-cover.png"/></div><div class="p-6 space-y-4"><h3 class="text-lg font-bold text-gray-900 leading-tight line-clamp-2"><a class="hover:text-green-600 transition-colors" href="/posts/llm-engineering-mastery-part-1/">LLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models</a></h3><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">Part 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.</p><div class="flex items-center justify-between pt-2"><div class="flex items-center space-x-3 text-xs text-gray-500"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-3 h-3"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>over 1 year ago</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-3 h-3"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>12 min read</span></div></div><a class="inline-flex items-center text-green-600 hover:text-green-700 font-medium text-sm group" href="/posts/llm-engineering-mastery-part-1/">Read<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right ml-1 w-3 h-3 group-hover:translate-x-1 transition-transform"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a></div><div class="flex flex-wrap gap-2 pt-2"><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">llm</span><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">genai</span><span class="text-xs text-gray-500 px-2 py-1">+<!-- -->3</span></div></div></article></div><div class="snap-start"><article class="flex-shrink-0 w-80 bg-white rounded-xl shadow-sm hover:shadow-lg transition-all duration-300 overflow-hidden group"><div class="relative aspect-[16/10] overflow-hidden"><img alt="LLM Engineering Mastery - Complete Series" loading="lazy" decoding="async" data-nimg="fill" class="object-cover group-hover:scale-105 transition-transform duration-300" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/posts/llm-engineering-mastery-series/assets/series-overview.png"/></div><div class="p-6 space-y-4"><h3 class="text-lg font-bold text-gray-900 leading-tight line-clamp-2"><a class="hover:text-green-600 transition-colors" href="/posts/llm-engineering-mastery-series/">LLM Engineering Mastery - Complete Series</a></h3><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">Complete LLM Engineering Mastery series with 3 parts covering Part 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.</p><div class="flex items-center justify-between pt-2"><div class="flex items-center space-x-3 text-xs text-gray-500"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-3 h-3"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>over 1 year ago</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-3 h-3"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>1 min read</span></div></div><a class="inline-flex items-center text-green-600 hover:text-green-700 font-medium text-sm group" href="/posts/llm-engineering-mastery-series/">Read<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right ml-1 w-3 h-3 group-hover:translate-x-1 transition-transform"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a></div><div class="flex flex-wrap gap-2 pt-2"><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">llm</span><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">genai</span><span class="text-xs text-gray-500 px-2 py-1">+<!-- -->3</span></div></div></article></div><div class="snap-start"><article class="flex-shrink-0 w-80 bg-white rounded-xl shadow-sm hover:shadow-lg transition-all duration-300 overflow-hidden group"><div class="relative aspect-[16/10] overflow-hidden"><img alt="Understanding Hash Tables: The Ultimate Guide" loading="lazy" decoding="async" data-nimg="fill" class="object-cover group-hover:scale-105 transition-transform duration-300" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/posts/understanding-hash-tables-ultimate-guide/assets/overview.png"/></div><div class="p-6 space-y-4"><h3 class="text-lg font-bold text-gray-900 leading-tight line-clamp-2"><a class="hover:text-green-600 transition-colors" href="/posts/understanding-hash-tables-ultimate-guide/">Understanding Hash Tables: The Ultimate Guide</a></h3><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">A comprehensive guide to hash tables, covering implementation details, collision resolution strategies, and performance analysis with practical examples.</p><div class="flex items-center justify-between pt-2"><div class="flex items-center space-x-3 text-xs text-gray-500"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-3 h-3"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>over 1 year ago</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-3 h-3"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>5 min read</span></div></div><a class="inline-flex items-center text-green-600 hover:text-green-700 font-medium text-sm group" href="/posts/understanding-hash-tables-ultimate-guide/">Read<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right ml-1 w-3 h-3 group-hover:translate-x-1 transition-transform"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a></div><div class="flex flex-wrap gap-2 pt-2"><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">data-structures</span><span class="text-xs bg-gray-100 text-gray-600 px-3 py-1 rounded-full">algorithms</span><span class="text-xs text-gray-500 px-2 py-1">+<!-- -->2</span></div></div></article></div></div><div class="text-center mt-8"><a class="inline-flex items-center px-6 py-3 bg-green-600 text-white font-semibold rounded-lg hover:bg-green-700 transition-colors" href="/posts/">View All Articles<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right ml-2 w-4 h-4"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a></div></div></section></div><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Microservices: Outbox Pattern","description":"Learn about Microservices: Outbox Pattern.","datePublished":"2022-11-21 23:02:39 +0530","dateModified":"2022-11-21 23:02:39 +0530","author":{"@type":"Person","name":"Abstract Algorithms"},"publisher":{"@type":"Organization","name":"Abstract Algorithms","url":"https://abstractalgorithms.github.io"},"url":"https://abstractalgorithms.github.io/posts/microservices-outbox-pattern","mainEntityOfPage":{"@type":"WebPage","@id":"https://abstractalgorithms.github.io/posts/microservices-outbox-pattern"},"image":{"@type":"ImageObject","url":"https://abstractalgorithms.github.io/posts/microservices-outbox-pattern/assets/overview.png"}}</script></article></main><footer class="bg-gray-50 border-t border-gray-200"><div class="medium-container py-12"><div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8"><div class="lg:col-span-2"><h3 class="text-lg font-semibold text-gray-900 mb-4">Abstract Algorithms</h3><p class="text-gray-600 mb-4 max-w-md">Exploring the fascinating world of algorithms, data structures, and software engineering through clear explanations and practical examples.</p><div class="flex space-x-4"><a href="https://github.com/abstractalgorithms" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-600 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github w-5 h-5"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://x.com/abstractalgs" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-600 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-twitter w-5 h-5"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a><a href="https://linkedin.com/company/abstractalgorithms" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-600 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin w-5 h-5"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><a href="mailto:contact@abstractalgorithms.dev" class="text-gray-400 hover:text-gray-600 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail w-5 h-5"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg></a></div></div><div><h4 class="text-sm font-semibold text-gray-900 mb-4">Navigation</h4><ul class="space-y-2"><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/">Home</a></li><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/discover/">Discover</a></li><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/posts/">Posts</a></li><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/badges/">Badges</a></li><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/search/">Search</a></li></ul></div><div><h4 class="text-sm font-semibold text-gray-900 mb-4">About</h4><ul class="space-y-2"><li><a class="text-gray-600 hover:text-gray-900 transition-colors font-medium" href="/about/">About Us</a></li><li><a class="text-gray-600 hover:text-gray-900 transition-colors font-medium" href="/contact/">Contact</a></li></ul></div><div><h4 class="text-sm font-semibold text-gray-900 mb-4">Topics</h4><ul class="space-y-2"><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/tag/algorithms/">Algorithms</a></li><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/tag/data-structures/">Data Structures</a></li><li><a class="text-gray-600 hover:text-gray-900 transition-colors" href="/tag/system-design/">System Design</a></li></ul></div></div><div class="mt-8 pt-8 border-t border-gray-200 text-center"><p class="text-gray-600 text-sm">© <!-- -->2025<!-- --> Abstract Algorithms. All rights reserved.</p></div></div></footer></div></div><script src="/_next/static/chunks/webpack-b082bb110b1b278c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/275ed64cc4367444.css\",\"style\"]\n3:HL[\"/_next/static/css/c8b6ee85b5abc035.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[2846,[],\"\"]\n7:I[4707,[],\"\"]\n9:I[6423,[],\"\"]\na:I[981,[\"8592\",\"static/chunks/common-8bc9aba88b3a5d2f.js\",\"3185\",\"static/chunks/app/layout-3d649b44ebe169dd.js\"],\"AuthProvider\"]\nb:I[8931,[\"8592\",\"static/chunks/common-8bc9aba88b3a5d2f.js\",\"3185\",\"static/chunks/app/layout-3d649b44ebe169dd.js\"],\"default\"]\nc:I[917,[\"7601\",\"static/chunks/app/error-1745ca505ccb7f84.js\"],\"default\"]\nd:I[5618,[\"9160\",\"static/chunks/app/not-found-5aff7e7753541a4f.js\"],\"default\"]\nf:I[1060,[],\"\"]\n8:[\"slug\",\"microservices-outbox-pattern\",\"d\"]\n10:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L4\",null,{\"buildId\":\"nMiW3GBlP4aSYdRxBmClw\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"posts\",\"microservices-outbox-pattern\",\"\"],\"initialTree\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"microservices-outbox-pattern\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"microservices-outbox-pattern\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"microservices-outbox-pattern\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L5\",\"$L6\",null],null],null]},[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"$8\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/275ed64cc4367444.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c8b6ee85b5abc035.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"Abstract Algorithms\\\",\\\"description\\\":\\\"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices\\\",\\\"url\\\":\\\"https://abstractalgorithms.github.io\\\",\\\"potentialAction\\\":{\\\"@type\\\":\\\"SearchAction\\\",\\\"target\\\":{\\\"@type\\\":\\\"EntryPoint\\\",\\\"urlTemplate\\\":\\\"https://abstractalgorithms.github.io/posts/{search_term_string}\\\"},\\\"query-input\\\":\\\"required name=search_term_string\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"Abstract Algorithms\\\",\\\"url\\\":\\\"https://abstractalgorithms.github.io\\\"}}\"}}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/manifest.json\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#00D885\"}],[\"$\",\"meta\",null,{\"name\":\"google-site-verification\",\"content\":\"D5v1M3nD8oO9DNaZKujCwBLNNqf35CTJo114uv8yMNU\"}],[\"$\",\"script\",null,{\"async\":true,\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-VZR168MHE2\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n            window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-VZR168MHE2');\\n          \"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_e8ce0c\",\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$c\",\"errorStyles\":[],\"errorScripts\":[],\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"$Ld\",null,{}],\"notFoundStyles\":[]}]}]}]}]]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Le\"],\"globalErrorComponent\":\"$f\",\"missingSlots\":\"$W10\"}]\n"])</script><script>self.__next_f.push([1,"11:I[5878,[\"8592\",\"static/chunks/common-8bc9aba88b3a5d2f.js\",\"333\",\"static/chunks/app/posts/%5Bslug%5D/page-3331b62b71175ca5.js\"],\"Image\"]\n12:I[4274,[\"8592\",\"static/chunks/common-8bc9aba88b3a5d2f.js\",\"333\",\"static/chunks/app/posts/%5Bslug%5D/page-3331b62b71175ca5.js\"],\"default\"]\n13:I[9798,[\"8592\",\"static/chunks/common-8bc9aba88b3a5d2f.js\",\"333\",\"static/chunks/app/posts/%5Bslug%5D/page-3331b62b71175ca5.js\"],\"default\"]\n14:I[6883,[\"8592\",\"static/chunks/common-8bc9aba88b3a5d2f.js\",\"333\",\"static/chunks/app/posts/%5Bslug%5D/page-3331b62b71175ca5.js\"],\"default\"]\n15:T497,\u003cp\u003eLittle's Law establishes a fundamental relationship in queueing systems:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eL = Î» Ã— W\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eL\u003c/strong\u003e: Average items in the system\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eÎ»\u003c/strong\u003e: Arrival rate\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eW\u003c/strong\u003e: Average waiting time\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhy Little's Law Matters\u003c/h2\u003e\n\u003cp\u003eBy understanding this relationship, engineers can:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePredict Throughput\u003c/strong\u003e: Estimate system capacity under varying loads.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOptimize Resources\u003c/strong\u003e: Allocate servers or threads to meet SLAs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnalyze Latency\u003c/strong\u003e: Correlate queue length with response times.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePractical Example\u003c/h2\u003e\n\u003cp\u003eAssume a web server receives 50 requests/second (Î») with an average response time of 0.2 seconds (W). Then:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eL = 50 Ã— 0.2 = 10 concurrent requests\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis simple insight guides capacity planning and performance tuning.\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eLittle's Law is a cornerstone in queueing theory, offering invaluable insights for system optimization. By mastering this principle, engineers can significantly enhance system performance and reliability.\u003c/p\u003e\n16:Tbfff,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eLLM Engineering Mastery: Part 3 - Production Deployment and Scaling\u003c/h1\u003e\n\u003cp\u003eIn this final part of the LLM Engineering Mastery series, we'll cover everything you need to deploy, scale, and maintain LLM applications in production environments. From infrastructure patterns to monitoring and security, this guide provides the practical knowledge needed for enterprise-grade deployments.\u003c/p\u003e\n\u003ch2\u003eInfrastructure Patterns for LLM Applications\u003c/h2\u003e\n\u003ch3\u003e1. Microservices Architecture for LLM Systems\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom fastapi import FastAPI, HTTPException, Depends\r\nfrom pydantic import BaseModel\r\nfrom typing import List, Optional\r\nimport asyncio\r\nimport httpx\r\nfrom datetime import datetime\r\nimport logging\r\n\r\n# Data models\r\nclass ChatRequest(BaseModel):\r\n    messages: List[dict]\r\n    model: str = \"gpt-3.5-turbo\"\r\n    temperature: float = 0.7\r\n    max_tokens: int = 1000\r\n\r\nclass RAGRequest(BaseModel):\r\n    query: str\r\n    collection: str = \"default\"\r\n    top_k: int = 5\r\n\r\nclass ChatResponse(BaseModel):\r\n    response: str\r\n    model_used: str\r\n    tokens_used: int\r\n    processing_time: float\r\n    request_id: str\r\n\r\n# LLM Service\r\nclass LLMService:\r\n    def __init__(self):\r\n        self.app = FastAPI(title=\"LLM Service\", version=\"1.0.0\")\r\n        self.setup_routes()\r\n        self.setup_middleware()\r\n    \r\n    def setup_middleware(self):\r\n        @self.app.middleware(\"http\")\r\n        async def log_requests(request, call_next):\r\n            start_time = datetime.now()\r\n            \r\n            response = await call_next(request)\r\n            \r\n            processing_time = (datetime.now() - start_time).total_seconds()\r\n            \r\n            logging.info(\r\n                \"Request processed\",\r\n                extra={\r\n                    \"method\": request.method,\r\n                    \"url\": str(request.url),\r\n                    \"status_code\": response.status_code,\r\n                    \"processing_time\": processing_time\r\n                }\r\n            )\r\n            \r\n            return response\r\n    \r\n    def setup_routes(self):\r\n        @self.app.post(\"/chat/completions\", response_model=ChatResponse)\r\n        async def chat_completion(request: ChatRequest):\r\n            start_time = datetime.now()\r\n            \r\n            try:\r\n                # Route to appropriate model provider\r\n                if request.model.startswith(\"gpt\"):\r\n                    result = await self._call_openai(request)\r\n                elif request.model.startswith(\"claude\"):\r\n                    result = await self._call_anthropic(request)\r\n                else:\r\n                    raise HTTPException(status_code=400, detail=\"Unsupported model\")\r\n                \r\n                processing_time = (datetime.now() - start_time).total_seconds()\r\n                \r\n                return ChatResponse(\r\n                    response=result[\"content\"],\r\n                    model_used=request.model,\r\n                    tokens_used=result[\"tokens\"],\r\n                    processing_time=processing_time,\r\n                    request_id=result[\"request_id\"]\r\n                )\r\n                \r\n            except Exception as e:\r\n                logging.error(\"Chat completion failed\", extra={\"error\": str(e)})\r\n                raise HTTPException(status_code=500, detail=\"Internal server error\")\r\n        \r\n        @self.app.get(\"/health\")\r\n        async def health_check():\r\n            return {\"status\": \"healthy\", \"timestamp\": datetime.now().isoformat()}\r\n        \r\n        @self.app.get(\"/models\")\r\n        async def list_models():\r\n            return {\r\n                \"available_models\": [\r\n                    \"gpt-3.5-turbo\",\r\n                    \"gpt-4-turbo\", \r\n                    \"claude-3-sonnet\",\r\n                    \"claude-3-haiku\"\r\n                ]\r\n            }\r\n    \r\n    async def _call_openai(self, request: ChatRequest) -\u003e dict:\r\n        # Implementation for OpenAI API calls\r\n        # This would include the robust client from Part 1\r\n        pass\r\n    \r\n    async def _call_anthropic(self, request: ChatRequest) -\u003e dict:\r\n        # Implementation for Anthropic API calls\r\n        pass\r\n\r\n# RAG Service\r\nclass RAGService:\r\n    def __init__(self, llm_service_url: str):\r\n        self.app = FastAPI(title=\"RAG Service\", version=\"1.0.0\")\r\n        self.llm_service_url = llm_service_url\r\n        self.setup_routes()\r\n    \r\n    def setup_routes(self):\r\n        @self.app.post(\"/rag/query\")\r\n        async def rag_query(request: RAGRequest):\r\n            try:\r\n                # Retrieve relevant documents\r\n                relevant_docs = await self._retrieve_documents(\r\n                    request.query, \r\n                    request.collection, \r\n                    request.top_k\r\n                )\r\n                \r\n                # Build context\r\n                context = self._build_context(relevant_docs)\r\n                \r\n                # Generate response using LLM service\r\n                llm_request = ChatRequest(\r\n                    messages=[\r\n                        {\r\n                            \"role\": \"system\",\r\n                            \"content\": \"Answer based on the provided context.\"\r\n                        },\r\n                        {\r\n                            \"role\": \"user\", \r\n                            \"content\": \"Context:\\n\" + context + \"\\n\\nQuestion: \" + request.query\r\n                        }\r\n                    ]\r\n                )\r\n                \r\n                async with httpx.AsyncClient() as client:\r\n                    response = await client.post(\r\n                        self.llm_service_url + \"/chat/completions\",\r\n                        json=llm_request.dict()\r\n                    )\r\n                    response.raise_for_status()\r\n                    llm_response = response.json()\r\n                \r\n                return {\r\n                    \"answer\": llm_response[\"response\"],\r\n                    \"sources\": relevant_docs,\r\n                    \"tokens_used\": llm_response[\"tokens_used\"]\r\n                }\r\n                \r\n            except Exception as e:\r\n                logging.error(\"RAG query failed\", extra={\"error\": str(e)})\r\n                raise HTTPException(status_code=500, detail=\"RAG processing failed\")\r\n    \r\n    async def _retrieve_documents(self, query: str, collection: str, top_k: int):\r\n        # Implementation for document retrieval\r\n        # This would use the vector store from Part 2\r\n        pass\r\n    \r\n    def _build_context(self, documents: List[dict]) -\u003e str:\r\n        context_parts = []\r\n        for i, doc in enumerate(documents, 1):\r\n            context_parts.append(\"Document \" + str(i) + \":\")\r\n            context_parts.append(doc[\"content\"])\r\n            context_parts.append(\"\")\r\n        return \"\\n\".join(context_parts)\r\n\r\n# API Gateway\r\nclass APIGateway:\r\n    def __init__(self, llm_service_url: str, rag_service_url: str):\r\n        self.app = FastAPI(title=\"LLM API Gateway\", version=\"1.0.0\")\r\n        self.llm_service_url = llm_service_url\r\n        self.rag_service_url = rag_service_url\r\n        self.setup_routes()\r\n        self.setup_middleware()\r\n    \r\n    def setup_middleware(self):\r\n        # Rate limiting, authentication, etc.\r\n        pass\r\n    \r\n    def setup_routes(self):\r\n        @self.app.post(\"/v1/chat/completions\")\r\n        async def proxy_chat(request: ChatRequest):\r\n            async with httpx.AsyncClient() as client:\r\n                response = await client.post(\r\n                    self.llm_service_url + \"/chat/completions\",\r\n                    json=request.dict(),\r\n                    timeout=60.0\r\n                )\r\n                response.raise_for_status()\r\n                return response.json()\r\n        \r\n        @self.app.post(\"/v1/rag/query\")\r\n        async def proxy_rag(request: RAGRequest):\r\n            async with httpx.AsyncClient() as client:\r\n                response = await client.post(\r\n                    self.rag_service_url + \"/rag/query\",\r\n                    json=request.dict(),\r\n                    timeout=60.0\r\n                )\r\n                response.raise_for_status()\r\n                return response.json()\r\n\r\n# Docker Compose for local development\r\ndocker_compose_content = \"\"\"\r\nversion: '3.8'\r\n\r\nservices:\r\n  llm-service:\r\n    build: ./llm-service\r\n    ports:\r\n      - \"8001:8000\"\r\n    environment:      - OPENAI_API_KEY=\\$\\{OPENAI_API_KEY\\}\r\n      - ANTHROPIC_API_KEY=\\$\\{ANTHROPIC_API_KEY\\}\r\n    healthcheck:\r\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\r\n      interval: 30s\r\n      timeout: 10s\r\n      retries: 3\r\n\r\n  rag-service:\r\n    build: ./rag-service\r\n    ports:\r\n      - \"8002:8000\"\r\n    environment:\r\n      - LLM_SERVICE_URL=http://llm-service:8000\r\n      - VECTOR_DB_URL=\\$\\{VECTOR_DB_URL\\}\r\n    depends_on:\r\n      - llm-service\r\n      - vector-db\r\n    healthcheck:\r\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\r\n      interval: 30s\r\n      timeout: 10s\r\n      retries: 3\r\n\r\n  api-gateway:\r\n    build: ./api-gateway\r\n    ports:\r\n      - \"8000:8000\"\r\n    environment:\r\n      - LLM_SERVICE_URL=http://llm-service:8000\r\n      - RAG_SERVICE_URL=http://rag-service:8000\r\n    depends_on:\r\n      - llm-service\r\n      - rag-service\r\n\r\n  vector-db:\r\n    image: chromadb/chroma:latest\r\n    ports:\r\n      - \"8003:8000\"\r\n    volumes:\r\n      - vector_data:/chroma/chroma\r\n\r\n  redis:\r\n    image: redis:alpine\r\n    ports:\r\n      - \"6379:6379\"\r\n\r\n  prometheus:\r\n    image: prom/prometheus:latest\r\n    ports:\r\n      - \"9090:9090\"\r\n    volumes:\r\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\r\n\r\n  grafana:\r\n    image: grafana/grafana:latest\r\n    ports:\r\n      - \"3000:3000\"\r\n    environment:\r\n      - GF_SECURITY_ADMIN_PASSWORD=admin\r\n\r\nvolumes:\r\n  vector_data:\r\n\"\"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Kubernetes Deployment Configuration\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003e# llm-deployment.yaml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: llm-service\r\n  labels:\r\n    app: llm-service\r\nspec:\r\n  replicas: 3\r\n  selector:\r\n    matchLabels:\r\n      app: llm-service\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: llm-service\r\n    spec:\r\n      containers:\r\n      - name: llm-service\r\n        image: your-registry/llm-service:latest\r\n        ports:\r\n        - containerPort: 8000\r\n        env:\r\n        - name: OPENAI_API_KEY\r\n          valueFrom:\r\n            secretKeyRef:\r\n              name: api-secrets\r\n              key: openai-api-key\r\n        - name: ANTHROPIC_API_KEY\r\n          valueFrom:\r\n            secretKeyRef:\r\n              name: api-secrets\r\n              key: anthropic-api-key\r\n        resources:\r\n          requests:\r\n            memory: \"512Mi\"\r\n            cpu: \"250m\"\r\n          limits:\r\n            memory: \"1Gi\"\r\n            cpu: \"500m\"\r\n        livenessProbe:\r\n          httpGet:\r\n            path: /health\r\n            port: 8000\r\n          initialDelaySeconds: 30\r\n          periodSeconds: 10\r\n        readinessProbe:\r\n          httpGet:\r\n            path: /health\r\n            port: 8000\r\n          initialDelaySeconds: 5\r\n          periodSeconds: 5\r\n\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: llm-service\r\nspec:\r\n  selector:\r\n    app: llm-service\r\n  ports:\r\n  - port: 80\r\n    targetPort: 8000\r\n  type: ClusterIP\r\n\r\n---\r\napiVersion: autoscaling/v2\r\nkind: HorizontalPodAutoscaler\r\nmetadata:\r\n  name: llm-service-hpa\r\nspec:\r\n  scaleTargetRef:\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    name: llm-service\r\n  minReplicas: 2\r\n  maxReplicas: 10\r\n  metrics:\r\n  - type: Resource\r\n    resource:\r\n      name: cpu\r\n      target:\r\n        type: Utilization\r\n        averageUtilization: 70\r\n  - type: Resource\r\n    resource:\r\n      name: memory\r\n      target:\r\n        type: Utilization\r\n        averageUtilization: 80\r\n\r\n---\r\n# Ingress for external access\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: llm-ingress\r\n  annotations:\r\n    nginx.ingress.kubernetes.io/rate-limit: \"100\"\r\n    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\r\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\r\nspec:\r\n  tls:\r\n  - hosts:\r\n    - api.yourdomain.com\r\n    secretName: llm-tls\r\n  rules:\r\n  - host: api.yourdomain.com\r\n    http:\r\n      paths:\r\n      - path: /v1\r\n        pathType: Prefix\r\n        backend:\r\n          service:\r\n            name: api-gateway\r\n            port:\r\n              number: 80\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eMonitoring and Observability\u003c/h2\u003e\n\u003ch3\u003e1. Comprehensive Monitoring System\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport logging\r\nimport time\r\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\r\nfrom functools import wraps\r\nimport structlog\r\nfrom typing import Any, Callable\r\nimport asyncio\r\n\r\n# Prometheus metrics\r\nREQUEST_COUNT = Counter(\r\n    'llm_requests_total',\r\n    'Total number of LLM requests',\r\n    ['model', 'endpoint', 'status']\r\n)\r\n\r\nREQUEST_DURATION = Histogram(\r\n    'llm_request_duration_seconds',\r\n    'Time spent processing LLM requests',\r\n    ['model', 'endpoint']\r\n)\r\n\r\nTOKEN_USAGE = Counter(\r\n    'llm_tokens_total',\r\n    'Total number of tokens processed',\r\n    ['model', 'type']  # type: input/output\r\n)\r\n\r\nCOST_TRACKING = Counter(\r\n    'llm_cost_total_usd',\r\n    'Total cost in USD',\r\n    ['model', 'provider']\r\n)\r\n\r\nACTIVE_REQUESTS = Gauge(\r\n    'llm_active_requests',\r\n    'Number of currently active requests',\r\n    ['model']\r\n)\r\n\r\nERROR_RATE = Counter(\r\n    'llm_errors_total',\r\n    'Total number of errors',\r\n    ['model', 'error_type']\r\n)\r\n\r\nclass MetricsCollector:\r\n    def __init__(self):\r\n        self.logger = structlog.get_logger()\r\n    \r\n    def record_request(self, model: str, endpoint: str, status: str):\r\n        \"\"\"Record a request with its status\"\"\"\r\n        REQUEST_COUNT.labels(model=model, endpoint=endpoint, status=status).inc()\r\n    \r\n    def record_duration(self, model: str, endpoint: str, duration: float):\r\n        \"\"\"Record request duration\"\"\"\r\n        REQUEST_DURATION.labels(model=model, endpoint=endpoint).observe(duration)\r\n    \r\n    def record_token_usage(self, model: str, input_tokens: int, output_tokens: int):\r\n        \"\"\"Record token usage\"\"\"\r\n        TOKEN_USAGE.labels(model=model, type='input').inc(input_tokens)\r\n        TOKEN_USAGE.labels(model=model, type='output').inc(output_tokens)\r\n    \r\n    def record_cost(self, model: str, provider: str, cost: float):\r\n        \"\"\"Record cost\"\"\"\r\n        COST_TRACKING.labels(model=model, provider=provider).inc(cost)\r\n    \r\n    def record_error(self, model: str, error_type: str):\r\n        \"\"\"Record error\"\"\"\r\n        ERROR_RATE.labels(model=model, error_type=error_type).inc()\r\n    \r\n    def track_active_request(self, model: str, increment: bool = True):\r\n        \"\"\"Track active requests\"\"\"\r\n        if increment:\r\n            ACTIVE_REQUESTS.labels(model=model).inc()\r\n        else:\r\n            ACTIVE_REQUESTS.labels(model=model).dec()\r\n\r\n# Monitoring decorator\r\ndef monitor_llm_request(model: str, endpoint: str):\r\n    def decorator(func: Callable) -\u003e Callable:\r\n        @wraps(func)\r\n        async def async_wrapper(*args, **kwargs) -\u003e Any:\r\n            metrics = MetricsCollector()\r\n            start_time = time.time()\r\n            \r\n            metrics.track_active_request(model, increment=True)\r\n            \r\n            try:\r\n                result = await func(*args, **kwargs)\r\n                \r\n                # Record success metrics\r\n                duration = time.time() - start_time\r\n                metrics.record_request(model, endpoint, 'success')\r\n                metrics.record_duration(model, endpoint, duration)\r\n                \r\n                # Record token usage if available\r\n                if hasattr(result, 'tokens_used'):\r\n                    metrics.record_token_usage(\r\n                        model, \r\n                        result.input_tokens, \r\n                        result.output_tokens\r\n                    )\r\n                \r\n                return result\r\n                \r\n            except Exception as e:\r\n                # Record error metrics\r\n                duration = time.time() - start_time\r\n                metrics.record_request(model, endpoint, 'error')\r\n                metrics.record_duration(model, endpoint, duration)\r\n                metrics.record_error(model, type(e).__name__)\r\n                \r\n                # Log structured error\r\n                structlog.get_logger().error(\r\n                    \"LLM request failed\",\r\n                    model=model,\r\n                    endpoint=endpoint,\r\n                    error=str(e),\r\n                    duration=duration\r\n                )\r\n                \r\n                raise\r\n            \r\n            finally:\r\n                metrics.track_active_request(model, increment=False)\r\n        \r\n        return async_wrapper\r\n    return decorator\r\n\r\n# Usage example\r\nclass MonitoredLLMClient:\r\n    def __init__(self, model: str):\r\n        self.model = model\r\n        self.metrics = MetricsCollector()\r\n    \r\n    @monitor_llm_request(\"gpt-3.5-turbo\", \"chat_completion\")\r\n    async def chat_completion(self, messages: list, **kwargs):\r\n        # Your LLM API call implementation\r\n        pass\r\n\r\n# Structured logging configuration\r\ndef setup_logging():\r\n    structlog.configure(\r\n        processors=[\r\n            structlog.stdlib.filter_by_level,\r\n            structlog.stdlib.add_logger_name,\r\n            structlog.stdlib.add_log_level,\r\n            structlog.stdlib.PositionalArgumentsFormatter(),\r\n            structlog.processors.TimeStamper(fmt=\"iso\"),\r\n            structlog.processors.StackInfoRenderer(),\r\n            structlog.processors.format_exc_info,\r\n            structlog.processors.UnicodeDecoder(),\r\n            structlog.processors.JSONRenderer()\r\n        ],\r\n        context_class=dict,\r\n        logger_factory=structlog.stdlib.LoggerFactory(),\r\n        wrapper_class=structlog.stdlib.BoundLogger,\r\n        cache_logger_on_first_use=True,\r\n    )\r\n\r\n# Health check endpoint with detailed diagnostics\r\nclass HealthChecker:\r\n    def __init__(self, llm_client, vector_store):\r\n        self.llm_client = llm_client\r\n        self.vector_store = vector_store\r\n    \r\n    async def comprehensive_health_check(self) -\u003e dict:\r\n        \"\"\"Perform comprehensive health check\"\"\"\r\n        checks = {}\r\n        overall_healthy = True\r\n        \r\n        # Check LLM service connectivity\r\n        try:\r\n            test_response = await self.llm_client.complete([\r\n                {\"role\": \"user\", \"content\": \"Health check test\"}\r\n            ], max_tokens=5)\r\n            \r\n            checks[\"llm_service\"] = {\r\n                \"status\": \"healthy\",\r\n                \"response_time\": 0.5,  # Calculate actual response time\r\n                \"last_check\": time.time()\r\n            }\r\n        except Exception as e:\r\n            checks[\"llm_service\"] = {\r\n                \"status\": \"unhealthy\",\r\n                \"error\": str(e),\r\n                \"last_check\": time.time()\r\n            }\r\n            overall_healthy = False\r\n        \r\n        # Check vector store connectivity\r\n        try:\r\n            # Test vector store query\r\n            test_results = self.vector_store.search(\"health check\", top_k=1)\r\n            \r\n            checks[\"vector_store\"] = {\r\n                \"status\": \"healthy\",\r\n                \"documents_count\": len(test_results),\r\n                \"last_check\": time.time()\r\n            }\r\n        except Exception as e:\r\n            checks[\"vector_store\"] = {\r\n                \"status\": \"unhealthy\", \r\n                \"error\": str(e),\r\n                \"last_check\": time.time()\r\n            }\r\n            overall_healthy = False\r\n        \r\n        # Check system resources\r\n        import psutil\r\n        \r\n        checks[\"system_resources\"] = {\r\n            \"cpu_percent\": psutil.cpu_percent(),\r\n            \"memory_percent\": psutil.virtual_memory().percent,\r\n            \"disk_percent\": psutil.disk_usage('/').percent\r\n        }\r\n        \r\n        # Check if resources are within acceptable limits\r\n        if (checks[\"system_resources\"][\"cpu_percent\"] \u003e 90 or \r\n            checks[\"system_resources\"][\"memory_percent\"] \u003e 90):\r\n            overall_healthy = False\r\n        \r\n        return {\r\n            \"status\": \"healthy\" if overall_healthy else \"unhealthy\",\r\n            \"timestamp\": time.time(),\r\n            \"checks\": checks\r\n        }\r\n\r\n# Start metrics server\r\ndef start_metrics_server(port: int = 8080):\r\n    start_http_server(port)\r\n    print(\"Metrics server started on port \" + str(port))\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Custom Dashboards and Alerting\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Grafana dashboard configuration (JSON)\r\ngrafana_dashboard = {\r\n    \"dashboard\": {\r\n        \"title\": \"LLM Application Monitoring\",\r\n        \"panels\": [\r\n            {\r\n                \"title\": \"Request Rate\",\r\n                \"type\": \"graph\",\r\n                \"targets\": [\r\n                    {\r\n                        \"expr\": \"rate(llm_requests_total[5m])\",\r\n                        \"legendFormat\": \"\\\\{\\\\{model\\\\}\\\\} - \\\\{\\\\{endpoint\\\\}\\\\}\"\r\n                    }\r\n                ]\r\n            },\r\n            {\r\n                \"title\": \"Response Time\",\r\n                \"type\": \"graph\", \r\n                \"targets\": [\r\n                    {\r\n                        \"expr\": \"histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m]))\",\r\n                        \"legendFormat\": \"95th percentile\"\r\n                    },\r\n                    {\r\n                        \"expr\": \"histogram_quantile(0.50, rate(llm_request_duration_seconds_bucket[5m]))\",\r\n                        \"legendFormat\": \"50th percentile\"\r\n                    }\r\n                ]\r\n            },\r\n            {\r\n                \"title\": \"Error Rate\",\r\n                \"type\": \"graph\",\r\n                \"targets\": [\r\n                    {\r\n                        \"expr\": \"rate(llm_errors_total[5m]) / rate(llm_requests_total[5m])\",\r\n                        \"legendFormat\": \"Error Rate\"\r\n                    }\r\n                ]\r\n            },\r\n            {\r\n                \"title\": \"Token Usage\",\r\n                \"type\": \"graph\",\r\n                \"targets\": [\r\n                    {\r\n                        \"expr\": \"rate(llm_tokens_total[5m])\",\r\n                        \"legendFormat\": \"\\\\{\\\\{type\\\\}\\\\} tokens\"\r\n                    }\r\n                ]\r\n            },\r\n            {\r\n                \"title\": \"Cost Tracking\",\r\n                \"type\": \"singlestat\",\r\n                \"targets\": [\r\n                    {\r\n                        \"expr\": \"sum(llm_cost_total_usd)\",\r\n                        \"legendFormat\": \"Total Cost (USD)\"\r\n                    }\r\n                ]\r\n            }\r\n        ]\r\n    }\r\n}\r\n\r\n# Alerting rules for Prometheus\r\nalerting_rules = \"\"\"\r\ngroups:\r\n- name: llm_application_alerts\r\n  rules:\r\n  - alert: HighErrorRate\r\n    expr: rate(llm_errors_total[5m]) / rate(llm_requests_total[5m]) \u003e 0.1\r\n    for: 2m\r\n    labels:\r\n      severity: warning\r\n    annotations:\r\n      summary: \"High error rate detected\"\r\n      description: \"Error rate is \\\\{\\\\{ $value | humanizePercentage \\\\}\\\\} for the last 5 minutes\"\r\n\r\n  - alert: HighResponseTime\r\n    expr: histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m])) \u003e 10\r\n    for: 5m\r\n    labels:\r\n      severity: warning\r\n    annotations:\r\n      summary: \"High response time detected\"\r\n      description: \"95th percentile response time is \\\\{\\\\{ $value \\\\}\\\\}s\"\r\n\r\n  - alert: ServiceDown\r\n    expr: up{job=\"llm-service\"} == 0\r\n    for: 1m\r\n    labels:\r\n      severity: critical\r\n    annotations:\r\n      summary: \"LLM service is down\"\r\n      description: \"LLM service has been down for more than 1 minute\"\r\n\r\n  - alert: HighCostBurn\r\n    expr: increase(llm_cost_total_usd[1h]) \u003e 50\r\n    for: 0m\r\n    labels:\r\n      severity: warning\r\n    annotations:\r\n      summary: \"High cost burn rate\"\r\n      description: \"Cost increased by $\\\\{\\\\{ $value \\\\}\\\\} in the last hour\"\r\n\"\"\"\r\n\r\n# Slack alerting integration\r\nimport requests\r\nimport json\r\n\r\nclass SlackAlerter:\r\n    def __init__(self, webhook_url: str, channel: str = \"#alerts\"):\r\n        self.webhook_url = webhook_url\r\n        self.channel = channel\r\n    \r\n    def send_alert(self, title: str, message: str, severity: str = \"warning\"):\r\n        \"\"\"Send alert to Slack\"\"\"\r\n        \r\n        color_map = {\r\n            \"info\": \"#36a64f\",     # green\r\n            \"warning\": \"#ffaa00\",  # orange  \r\n            \"critical\": \"#ff0000\"  # red\r\n        }\r\n        \r\n        payload = {\r\n            \"channel\": self.channel,\r\n            \"username\": \"LLM Monitor\",\r\n            \"attachments\": [\r\n                {\r\n                    \"color\": color_map.get(severity, \"#808080\"),\r\n                    \"title\": title,\r\n                    \"text\": message,\r\n                    \"fields\": [\r\n                        {\r\n                            \"title\": \"Severity\",\r\n                            \"value\": severity.upper(),\r\n                            \"short\": True\r\n                        },\r\n                        {\r\n                            \"title\": \"Timestamp\", \r\n                            \"value\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n                            \"short\": True\r\n                        }\r\n                    ]\r\n                }\r\n            ]\r\n        }\r\n        \r\n        try:\r\n            response = requests.post(\r\n                self.webhook_url,\r\n                data=json.dumps(payload),\r\n                headers={'Content-Type': 'application/json'},\r\n                timeout=10\r\n            )\r\n            response.raise_for_status()\r\n        except Exception as e:\r\n            logging.error(\"Failed to send Slack alert\", extra={\"error\": str(e)})\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eSecurity and Compliance\u003c/h2\u003e\n\u003ch3\u003e1. Authentication and Authorization\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom fastapi import FastAPI, Depends, HTTPException, status\r\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\r\nimport jwt\r\nfrom datetime import datetime, timedelta\r\nimport hashlib\r\nimport secrets\r\nfrom typing import Optional, List\r\nimport redis\r\nimport asyncio\r\n\r\nclass SecurityManager:\r\n    def __init__(self, secret_key: str, redis_client: redis.Redis):\r\n        self.secret_key = secret_key\r\n        self.redis_client = redis_client\r\n        self.security = HTTPBearer()\r\n    \r\n    def create_access_token(self, user_id: str, scopes: List[str]) -\u003e str:\r\n        \"\"\"Create JWT access token with scopes\"\"\"\r\n        to_encode = {\r\n            \"sub\": user_id,\r\n            \"scopes\": scopes,\r\n            \"exp\": datetime.utcnow() + timedelta(hours=24),\r\n            \"iat\": datetime.utcnow(),\r\n            \"type\": \"access\"\r\n        }\r\n        \r\n        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm=\"HS256\")\r\n        return encoded_jwt\r\n    \r\n    def create_api_key(self, user_id: str, name: str, scopes: List[str]) -\u003e tuple:\r\n        \"\"\"Create API key for service-to-service communication\"\"\"\r\n        api_key = \"ak_\" + secrets.token_urlsafe(32)\r\n        api_secret = secrets.token_urlsafe(64)\r\n        \r\n        # Hash the secret for storage\r\n        secret_hash = hashlib.sha256(api_secret.encode()).hexdigest()\r\n        \r\n        # Store in Redis\r\n        key_data = {\r\n            \"user_id\": user_id,\r\n            \"name\": name,\r\n            \"scopes\": \",\".join(scopes),\r\n            \"secret_hash\": secret_hash,\r\n            \"created_at\": datetime.utcnow().isoformat(),\r\n            \"last_used\": None\r\n        }\r\n        \r\n        self.redis_client.hset(\"api_keys:\" + api_key, mapping=key_data)\r\n        \r\n        return api_key, api_secret\r\n    \r\n    async def verify_token(self, credentials: HTTPAuthorizationCredentials) -\u003e dict:\r\n        \"\"\"Verify JWT token\"\"\"\r\n        try:\r\n            payload = jwt.decode(\r\n                credentials.credentials, \r\n                self.secret_key, \r\n                algorithms=[\"HS256\"]\r\n            )\r\n            \r\n            user_id = payload.get(\"sub\")\r\n            scopes = payload.get(\"scopes\", [])\r\n            \r\n            if user_id is None:\r\n                raise HTTPException(\r\n                    status_code=status.HTTP_401_UNAUTHORIZED,\r\n                    detail=\"Invalid token\"\r\n                )\r\n            \r\n            return {\"user_id\": user_id, \"scopes\": scopes}\r\n            \r\n        except jwt.ExpiredSignatureError:\r\n            raise HTTPException(\r\n                status_code=status.HTTP_401_UNAUTHORIZED,\r\n                detail=\"Token has expired\"\r\n            )\r\n        except jwt.JWTError:\r\n            raise HTTPException(\r\n                status_code=status.HTTP_401_UNAUTHORIZED,\r\n                detail=\"Invalid token\"\r\n            )\r\n    \r\n    async def verify_api_key(self, api_key: str, api_secret: str) -\u003e dict:\r\n        \"\"\"Verify API key and secret\"\"\"\r\n        key_data = self.redis_client.hgetall(\"api_keys:\" + api_key)\r\n        \r\n        if not key_data:\r\n            raise HTTPException(\r\n                status_code=status.HTTP_401_UNAUTHORIZED,\r\n                detail=\"Invalid API key\"\r\n            )\r\n        \r\n        # Verify secret\r\n        secret_hash = hashlib.sha256(api_secret.encode()).hexdigest()\r\n        if secret_hash != key_data[b\"secret_hash\"].decode():\r\n            raise HTTPException(\r\n                status_code=status.HTTP_401_UNAUTHORIZED,\r\n                detail=\"Invalid API secret\"\r\n            )\r\n        \r\n        # Update last used timestamp\r\n        self.redis_client.hset(\r\n            \"api_keys:\" + api_key, \r\n            \"last_used\", \r\n            datetime.utcnow().isoformat()\r\n        )\r\n        \r\n        return {\r\n            \"user_id\": key_data[b\"user_id\"].decode(),\r\n            \"scopes\": key_data[b\"scopes\"].decode().split(\",\")\r\n        }\r\n    \r\n    def require_scope(self, required_scope: str):\r\n        \"\"\"Decorator to require specific scope\"\"\"\r\n        def decorator(func):\r\n            @wraps(func)\r\n            async def wrapper(*args, **kwargs):\r\n                # Extract auth info from kwargs or dependency injection\r\n                auth_info = kwargs.get(\"auth_info\")\r\n                if not auth_info or required_scope not in auth_info.get(\"scopes\", []):\r\n                    raise HTTPException(\r\n                        status_code=status.HTTP_403_FORBIDDEN,\r\n                        detail=\"Insufficient permissions\"\r\n                    )\r\n                return await func(*args, **kwargs)\r\n            return wrapper\r\n        return decorator\r\n\r\n# Rate limiting\r\nclass RateLimiter:\r\n    def __init__(self, redis_client: redis.Redis):\r\n        self.redis_client = redis_client\r\n    \r\n    async def is_allowed(\r\n        self, \r\n        key: str, \r\n        limit: int, \r\n        window_seconds: int\r\n    ) -\u003e tuple[bool, dict]:\r\n        \"\"\"Check if request is allowed under rate limit\"\"\"\r\n        \r\n        current_time = int(time.time())\r\n        window_start = current_time - window_seconds\r\n        \r\n        pipe = self.redis_client.pipeline()\r\n        \r\n        # Remove old entries\r\n        pipe.zremrangebyscore(key, 0, window_start)\r\n        \r\n        # Count current requests\r\n        pipe.zcard(key)\r\n        \r\n        # Add current request\r\n        pipe.zadd(key, {str(current_time): current_time})\r\n        \r\n        # Set expiry\r\n        pipe.expire(key, window_seconds)\r\n        \r\n        results = pipe.execute()\r\n        current_requests = results[1]\r\n        \r\n        allowed = current_requests \u0026#x3C; limit\r\n        \r\n        return allowed, {\r\n            \"limit\": limit,\r\n            \"current\": current_requests,\r\n            \"remaining\": max(0, limit - current_requests - 1),\r\n            \"reset_time\": current_time + window_seconds\r\n        }\r\n\r\n# Secure FastAPI application\r\ndef create_secure_app() -\u003e FastAPI:\r\n    app = FastAPI(title=\"Secure LLM API\")\r\n    \r\n    redis_client = redis.Redis(host='localhost', port=6379, db=0)\r\n    security_manager = SecurityManager(\"your-secret-key\", redis_client)\r\n    rate_limiter = RateLimiter(redis_client)\r\n    \r\n    @app.middleware(\"http\")\r\n    async def security_middleware(request, call_next):\r\n        # Add security headers\r\n        response = await call_next(request)\r\n        response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\r\n        response.headers[\"X-Frame-Options\"] = \"DENY\"\r\n        response.headers[\"X-XSS-Protection\"] = \"1; mode=block\"\r\n        response.headers[\"Strict-Transport-Security\"] = \"max-age=31536000; includeSubDomains\"\r\n        return response\r\n    \r\n    async def get_current_user(\r\n        credentials: HTTPAuthorizationCredentials = Depends(security_manager.security)\r\n    ):\r\n        return await security_manager.verify_token(credentials)\r\n    \r\n    @app.post(\"/v1/chat/completions\")\r\n    @security_manager.require_scope(\"llm:chat\")\r\n    async def secure_chat_completion(\r\n        request: ChatRequest,\r\n        auth_info: dict = Depends(get_current_user)\r\n    ):\r\n        user_id = auth_info[\"user_id\"]\r\n        \r\n        # Apply rate limiting\r\n        allowed, rate_info = await rate_limiter.is_allowed(\r\n            \"user:\" + user_id,\r\n            limit=100,  # 100 requests per hour\r\n            window_seconds=3600\r\n        )\r\n        \r\n        if not allowed:\r\n            raise HTTPException(\r\n                status_code=status.HTTP_429_TOO_MANY_REQUESTS,\r\n                detail=\"Rate limit exceeded\",\r\n                headers={\r\n                    \"X-RateLimit-Limit\": str(rate_info[\"limit\"]),\r\n                    \"X-RateLimit-Remaining\": str(rate_info[\"remaining\"]),\r\n                    \"X-RateLimit-Reset\": str(rate_info[\"reset_time\"])\r\n                }\r\n            )\r\n        \r\n        # Process the request\r\n        # ... your chat completion logic here\r\n        \r\n        return {\"message\": \"Chat completion processed securely\"}\r\n    \r\n    return app\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Data Privacy and Compliance\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport hashlib\r\nimport hmac\r\nfrom datetime import datetime, timedelta\r\nfrom typing import Dict, Any, Optional\r\nimport json\r\nimport asyncio\r\n\r\nclass DataPrivacyManager:\r\n    def __init__(self, encryption_key: str):\r\n        self.encryption_key = encryption_key.encode()\r\n    \r\n    def anonymize_user_data(self, user_id: str) -\u003e str:\r\n        \"\"\"Create anonymous user identifier\"\"\"\r\n        return hmac.new(\r\n            self.encryption_key,\r\n            user_id.encode(),\r\n            hashlib.sha256\r\n        ).hexdigest()[:16]\r\n    \r\n    def sanitize_conversation(self, messages: List[dict]) -\u003e List[dict]:\r\n        \"\"\"Remove PII from conversation data\"\"\"\r\n        sanitized = []\r\n        \r\n        pii_patterns = [\r\n            r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # SSN\r\n            r'\\b\\d{4}\\s?\\d{4}\\s?\\d{4}\\s?\\d{4}\\b',  # Credit card\r\n            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email\r\n            r'\\b\\d{3}-\\d{3}-\\d{4}\\b',  # Phone number\r\n        ]\r\n        \r\n        for message in messages:\r\n            content = message.get(\"content\", \"\")\r\n            \r\n            # Replace PII patterns with placeholders\r\n            for pattern in pii_patterns:\r\n                content = re.sub(pattern, \"[REDACTED]\", content)\r\n            \r\n            sanitized.append({\r\n                **message,\r\n                \"content\": content\r\n            })\r\n        \r\n        return sanitized\r\n    \r\n    def log_data_access(self, user_id: str, data_type: str, purpose: str):\r\n        \"\"\"Log data access for compliance\"\"\"\r\n        access_log = {\r\n            \"timestamp\": datetime.utcnow().isoformat(),\r\n            \"user_id\": self.anonymize_user_data(user_id),\r\n            \"data_type\": data_type,\r\n            \"purpose\": purpose,\r\n            \"access_granted\": True\r\n        }\r\n        \r\n        # Store in compliance log (implement your storage mechanism)\r\n        self._store_compliance_log(access_log)\r\n    \r\n    def handle_data_deletion_request(self, user_id: str) -\u003e bool:\r\n        \"\"\"Handle GDPR/CCPA deletion requests\"\"\"\r\n        try:\r\n            # Delete user conversations\r\n            # Delete user preferences\r\n            # Delete user analytics data\r\n            # Update logs to reflect deletion\r\n            \r\n            deletion_log = {\r\n                \"timestamp\": datetime.utcnow().isoformat(),\r\n                \"user_id\": self.anonymize_user_data(user_id),\r\n                \"action\": \"data_deletion\",\r\n                \"status\": \"completed\"\r\n            }\r\n            \r\n            self._store_compliance_log(deletion_log)\r\n            return True\r\n            \r\n        except Exception as e:\r\n            logging.error(\"Data deletion failed\", extra={\"error\": str(e)})\r\n            return False\r\n    \r\n    def _store_compliance_log(self, log_entry: dict):\r\n        \"\"\"Store compliance log entry\"\"\"\r\n        # Implement your preferred storage mechanism\r\n        # Could be database, file system, or external compliance service\r\n        pass\r\n\r\n# Content filtering for safety\r\nclass ContentFilter:\r\n    def __init__(self):\r\n        self.harmful_patterns = [\r\n            r'\\b(kill|murder|suicide)\\b',\r\n            r'\\b(bomb|explosive|weapon)\\b',\r\n            r'\\b(hack|exploit|vulnerability)\\b',\r\n            # Add more patterns based on your safety requirements\r\n        ]\r\n    \r\n    async def filter_content(self, content: str) -\u003e tuple[bool, List[str]]:\r\n        \"\"\"Filter content for harmful patterns\"\"\"\r\n        violations = []\r\n        \r\n        for pattern in self.harmful_patterns:\r\n            if re.search(pattern, content, re.IGNORECASE):\r\n                violations.append(pattern)\r\n        \r\n        is_safe = len(violations) == 0\r\n        return is_safe, violations\r\n    \r\n    async def filter_request(self, request: ChatRequest) -\u003e ChatRequest:\r\n        \"\"\"Filter incoming request\"\"\"\r\n        filtered_messages = []\r\n        \r\n        for message in request.messages:\r\n            content = message.get(\"content\", \"\")\r\n            is_safe, violations = await self.filter_content(content)\r\n            \r\n            if not is_safe:\r\n                # Log the violation\r\n                logging.warning(\r\n                    \"Content violation detected\",\r\n                    extra={\r\n                        \"violations\": violations,\r\n                        \"content_preview\": content[:100]\r\n                    }\r\n                )\r\n                \r\n                # Replace with safe content or reject\r\n                message[\"content\"] = \"[Content filtered for safety]\"\r\n            \r\n            filtered_messages.append(message)\r\n        \r\n        return ChatRequest(\r\n            **{**request.dict(), \"messages\": filtered_messages}\r\n        )\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eScaling Strategies and Performance Optimization\u003c/h2\u003e\n\u003ch3\u003e1. Caching Strategies\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport redis\r\nimport json\r\nimport hashlib\r\nfrom typing import Optional, Any\r\nimport asyncio\r\n\r\nclass LLMCache:\r\n    def __init__(self, redis_client: redis.Redis):\r\n        self.redis_client = redis_client\r\n        self.default_ttl = 3600  # 1 hour\r\n    \r\n    def _generate_cache_key(self, messages: List[dict], model: str, **kwargs) -\u003e str:\r\n        \"\"\"Generate deterministic cache key\"\"\"\r\n        # Create a deterministic representation\r\n        cache_data = {\r\n            \"messages\": messages,\r\n            \"model\": model,\r\n            **{k: v for k, v in kwargs.items() if k in [\"temperature\", \"max_tokens\"]}\r\n        }\r\n        \r\n        # Sort for deterministic ordering\r\n        cache_string = json.dumps(cache_data, sort_keys=True)\r\n        \r\n        # Hash for compact key\r\n        return \"llm_cache:\" + hashlib.md5(cache_string.encode()).hexdigest()\r\n    \r\n    async def get(self, messages: List[dict], model: str, **kwargs) -\u003e Optional[dict]:\r\n        \"\"\"Get cached response\"\"\"\r\n        cache_key = self._generate_cache_key(messages, model, **kwargs)\r\n        \r\n        try:\r\n            cached_data = self.redis_client.get(cache_key)\r\n            if cached_data:\r\n                return json.loads(cached_data)\r\n        except Exception as e:\r\n            logging.warning(\"Cache retrieval failed\", extra={\"error\": str(e)})\r\n        \r\n        return None\r\n    \r\n    async def set(\r\n        self, \r\n        messages: List[dict], \r\n        model: str, \r\n        response: dict, \r\n        ttl: Optional[int] = None,\r\n        **kwargs\r\n    ):\r\n        \"\"\"Cache response\"\"\"\r\n        cache_key = self._generate_cache_key(messages, model, **kwargs)\r\n        ttl = ttl or self.default_ttl\r\n        \r\n        try:\r\n            self.redis_client.setex(\r\n                cache_key,\r\n                ttl,\r\n                json.dumps(response)\r\n            )\r\n        except Exception as e:\r\n            logging.warning(\"Cache storage failed\", extra={\"error\": str(e)})\r\n    \r\n    async def invalidate_pattern(self, pattern: str):\r\n        \"\"\"Invalidate cache entries matching pattern\"\"\"\r\n        try:\r\n            keys = self.redis_client.keys(pattern)\r\n            if keys:\r\n                self.redis_client.delete(*keys)\r\n        except Exception as e:\r\n            logging.warning(\"Cache invalidation failed\", extra={\"error\": str(e)})\r\n\r\nclass CachedLLMClient:\r\n    def __init__(self, llm_client, cache: LLMCache):\r\n        self.llm_client = llm_client\r\n        self.cache = cache\r\n    \r\n    async def complete(self, messages: List[dict], **kwargs) -\u003e dict:\r\n        \"\"\"Complete with caching\"\"\"\r\n        \r\n        # Check cache first\r\n        cached_response = await self.cache.get(messages, self.llm_client.model, **kwargs)\r\n        if cached_response:\r\n            logging.info(\"Cache hit\", extra={\"cache_key\": \"hit\"})\r\n            return cached_response\r\n        \r\n        # Call LLM API\r\n        response = await self.llm_client.complete(messages, **kwargs)\r\n        \r\n        # Cache the response\r\n        await self.cache.set(messages, self.llm_client.model, response, **kwargs)\r\n        \r\n        return response\r\n\r\n# Connection pooling and load balancing\r\nclass LLMLoadBalancer:\r\n    def __init__(self, providers: List[dict]):\r\n        \"\"\"\r\n        providers: [\r\n            {\"name\": \"openai\", \"client\": openai_client, \"weight\": 0.7},\r\n            {\"name\": \"anthropic\", \"client\": anthropic_client, \"weight\": 0.3}\r\n        ]\r\n        \"\"\"\r\n        self.providers = providers\r\n        self.current_loads = {p[\"name\"]: 0 for p in providers}\r\n    \r\n    async def select_provider(self, request_type: str = \"chat\") -\u003e dict:\r\n        \"\"\"Select provider based on load and weights\"\"\"\r\n        \r\n        # Calculate weighted scores based on current load\r\n        best_provider = None\r\n        best_score = float('in')\r\n        \r\n        for provider in self.providers:\r\n            current_load = self.current_loads[provider[\"name\"]]\r\n            weight = provider[\"weight\"]\r\n            \r\n            # Score = load / weight (lower is better)\r\n            score = current_load / weight\r\n            \r\n            if score \u0026#x3C; best_score:\r\n                best_score = score\r\n                best_provider = provider\r\n        \r\n        # Update load tracking\r\n        if best_provider:\r\n            self.current_loads[best_provider[\"name\"]] += 1\r\n        \r\n        return best_provider\r\n    \r\n    async def complete_with_load_balancing(self, messages: List[dict], **kwargs) -\u003e dict:\r\n        \"\"\"Complete request with load balancing\"\"\"\r\n        \r\n        provider = await self.select_provider()\r\n        \r\n        try:\r\n            response = await provider[\"client\"].complete(messages, **kwargs)\r\n            return response\r\n        except Exception as e:\r\n            logging.error(\r\n                \"Provider failed, attempting fallback\",\r\n                extra={\"provider\": provider[\"name\"], \"error\": str(e)}\r\n            )\r\n            \r\n            # Try other providers as fallback\r\n            for fallback_provider in self.providers:\r\n                if fallback_provider[\"name\"] != provider[\"name\"]:\r\n                    try:\r\n                        return await fallback_provider[\"client\"].complete(messages, **kwargs)\r\n                    except Exception as fe:\r\n                        logging.error(\r\n                            \"Fallback provider failed\",\r\n                            extra={\"provider\": fallback_provider[\"name\"], \"error\": str(fe)}\r\n                        )\r\n            \r\n            # If all providers fail, raise the original exception\r\n            raise e\r\n        \r\n        finally:\r\n            # Decrease load counter\r\n            self.current_loads[provider[\"name\"]] -= 1\r\n\r\n# Async request batching\r\nclass RequestBatcher:\r\n    def __init__(self, batch_size: int = 10, max_wait_time: float = 0.1):\r\n        self.batch_size = batch_size\r\n        self.max_wait_time = max_wait_time\r\n        self.pending_requests = []\r\n        self.batch_timer = None\r\n    \r\n    async def add_request(self, request: dict, response_future: asyncio.Future):\r\n        \"\"\"Add request to batch\"\"\"\r\n        self.pending_requests.append({\r\n            \"request\": request,\r\n            \"future\": response_future\r\n        })\r\n        \r\n        # Start timer if this is the first request\r\n        if len(self.pending_requests) == 1:\r\n            self.batch_timer = asyncio.create_task(\r\n                self._wait_and_process_batch()\r\n            )\r\n        \r\n        # Process immediately if batch is full\r\n        if len(self.pending_requests) \u003e= self.batch_size:\r\n            if self.batch_timer:\r\n                self.batch_timer.cancel()\r\n            await self._process_batch()\r\n    \r\n    async def _wait_and_process_batch(self):\r\n        \"\"\"Wait for max_wait_time then process batch\"\"\"\r\n        try:\r\n            await asyncio.sleep(self.max_wait_time)\r\n            await self._process_batch()\r\n        except asyncio.CancelledError:\r\n            pass\r\n    \r\n    async def _process_batch(self):\r\n        \"\"\"Process current batch of requests\"\"\"\r\n        if not self.pending_requests:\r\n            return\r\n        \r\n        batch = self.pending_requests.copy()\r\n        self.pending_requests.clear()\r\n        \r\n        # Process batch requests\r\n        try:\r\n            # Implement batch processing logic here\r\n            # This could involve parallel API calls or optimized batch API endpoints\r\n            \r\n            responses = await self._execute_batch([req[\"request\"] for req in batch])\r\n            \r\n            # Resolve futures with responses\r\n            for i, batch_item in enumerate(batch):\r\n                batch_item[\"future\"].set_result(responses[i])\r\n                \r\n        except Exception as e:\r\n            # Reject all futures with the error\r\n            for batch_item in batch:\r\n                batch_item[\"future\"].set_exception(e)\r\n    \r\n    async def _execute_batch(self, requests: List[dict]) -\u003e List[dict]:\r\n        \"\"\"Execute batch of requests\"\"\"\r\n        # Implement parallel execution\r\n        tasks = []\r\n        for request in requests:\r\n            task = asyncio.create_task(self._execute_single_request(request))\r\n            tasks.append(task)\r\n        \r\n        return await asyncio.gather(*tasks)\r\n    \r\n    async def _execute_single_request(self, request: dict) -\u003e dict:\r\n        \"\"\"Execute single request (implement your LLM client call here)\"\"\"\r\n        # This is where you'.format(\r\n            \"request\": request,\r\n            \"future\": response_future\r\n        )d call your actual LLM client\r\n        pass\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eKey Takeaways for Part 3\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInfrastructure Patterns\u003c/strong\u003e: Use microservices architecture with proper service separation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMonitoring is Essential\u003c/strong\u003e: Implement comprehensive monitoring with metrics, logging, and alerting\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSecurity First\u003c/strong\u003e: Implement authentication, authorization, rate limiting, and content filtering\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance Optimization\u003c/strong\u003e: Use caching, load balancing, and request batching for scale\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompliance Matters\u003c/strong\u003e: Handle data privacy, PII protection, and regulatory requirements\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eSeries Conclusion\u003c/h2\u003e\n\u003cp\u003eCongratulations! You've completed the \u003cstrong\u003eLLM Engineering Mastery\u003c/strong\u003e series. You now have the practical knowledge to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSelect and integrate foundation models effectively\u003c/li\u003e\n\u003cli\u003eBuild advanced RAG systems with proper evaluation\u003c/li\u003e\n\u003cli\u003eDeploy and scale LLM applications in production\u003c/li\u003e\n\u003cli\u003eMonitor and maintain enterprise-grade systems\u003c/li\u003e\n\u003cli\u003eImplement security and compliance best practices\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe field of LLM engineering is rapidly evolving, but these foundational patterns and practices will serve you well as you build the next generation of AI-powered applications.\u003c/p\u003e\n\u003ch3\u003eNext Steps\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePractice\u003c/strong\u003e: Implement these patterns in your own projects\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStay Updated\u003c/strong\u003e: Follow LLM research and new model releases\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommunity\u003c/strong\u003e: Join LLM engineering communities and share your experiences\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExperiment\u003c/strong\u003e: Try new techniques and optimization strategies\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScale Gradually\u003c/strong\u003e: Start small and scale based on real usage patterns\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eThis concludes the LLM Engineering Mastery series. Keep building amazing AI applications!\u003c/em\u003e\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"17:T9244,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eLLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems\u003c/h1\u003e\n\u003cp\u003eBuilding on the foundation model integration from Part 1, we now dive deep into advanced prompt engineering techniques and Retrieval-Augmented Generation (RAG) systems that can dramatically enhance your LLM applications' capabilities and reliability.\u003c/p\u003e\n\u003ch2\u003eAdvanced Prompt Engineering Techniques\u003c/h2\u003e\n\u003ch3\u003e1. Few-Shot Learning Patterns\u003c/h3\u003e\n\u003cp\u003eFew-shot prompting provides examples to guide the model's behavior and output format.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass FewShotPromptBuilder:\r\n    def __init__(self):\r\n        self.examples = {}\r\n    \r\n    def add_example(self, category: str, input_text: str, output_text: str):\r\n        \"\"\"Add an example for few-shot learning\"\"\"\r\n        if category not in self.examples:\r\n            self.examples[category] = []\r\n        \r\n        self.examples[category].append({\r\n            \"input\": input_text,\r\n            \"output\": output_text\r\n        })\r\n    \r\n    def build_prompt(self, category: str, query: str, max_examples: int = 3) -\u003e str:\r\n        \"\"\"Build a few-shot prompt with examples\"\"\"\r\n        if category not in self.examples:\r\n            return query\r\n        \r\n        examples = self.examples[category][:max_examples]\r\n        \r\n        prompt_parts = [\r\n            \"Here are some examples of the expected format:\",\r\n            \"\"\r\n        ]\r\n        \r\n        for i, example in enumerate(examples, 1):\r\n            prompt_parts.extend([\r\n                \"Example \" + str(i) + \":\",\r\n                \"Input: \" + example[\"input\"],\r\n                \"Output: \" + example[\"output\"],\r\n                \"\"\r\n            ])\r\n        \r\n        prompt_parts.extend([\r\n            \"Now, please process this input:\",\r\n            \"Input: \" + query,\r\n            \"Output:\"\r\n        ])\r\n        \r\n        return \"\\n\".join(prompt_parts)\r\n\r\n# Usage for code generation\r\nprompt_builder = FewShotPromptBuilder()\r\n\r\n# Add examples for Python function generation\r\nprompt_builder.add_example(\r\n    \"python_function\",\r\n    \"Create a function to calculate factorial\",\r\n    \"\"\"def factorial(n):\r\n    if n \u0026#x3C;= 1:\r\n        return 1\r\n    return n * factorial(n - 1)\"\"\"\r\n)\r\n\r\nprompt_builder.add_example(\r\n    \"python_function\", \r\n    \"Create a function to check if a string is palindrome\",\r\n    \"\"\"def is_palindrome(s):\r\n    s = s.lower().replace(' ', '')\r\n    return s == s[::-1]\"\"\"\r\n)\r\n\r\n# Generate prompt for new task\r\nprompt = prompt_builder.build_prompt(\r\n    \"python_function\",\r\n    \"Create a function to find the maximum element in a list\"\r\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Chain-of-Thought (CoT) Reasoning\u003c/h3\u003e\n\u003cp\u003eChain-of-thought prompting encourages step-by-step reasoning for complex problems.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass ChainOfThoughtPrompt:\r\n    def __init__(self):\r\n        self.reasoning_templates = {\r\n            \"problem_solving\": \"\"\"Let's solve this step by step:\r\n\r\n1. First, I need to understand what the problem is asking\r\n2. Then, I'll identify the key information given\r\n3. Next, I'll determine what approach to use\r\n4. Finally, I'll work through the solution step by step\r\n\r\nProblem: {problem}\r\n\r\nStep-by-step solution:\"\"\",\r\n            \r\n            \"code_debugging\": \"\"\"Let me debug this code systematically:\r\n\r\n1. First, I'll read through the code to understand its purpose\r\n2. Then, I'll identify potential issues or errors\r\n3. Next, I'll analyze the logic flow\r\n4. Finally, I'll provide the corrected version with explanations\r\n\r\nCode to debug: {code}\r\n\r\nDebugging analysis:\"\"\",\r\n            \r\n            \"data_analysis\": \"\"\"Let me analyze this data step by step:\r\n\r\n1. First, I'll examine the data structure and format\r\n2. Then, I'll identify patterns and key metrics\r\n3. Next, I'll consider what insights can be drawn\r\n4. Finally, I'll provide conclusions and recommendations\r\n\r\nData: {data}\r\n\r\nAnalysis:\"\"\"\r\n        }\r\n    \r\n    def generate_cot_prompt(self, template_type: str, **kwargs) -\u003e str:\r\n        \"\"\"Generate a chain-of-thought prompt\"\"\"\r\n        if template_type not in self.reasoning_templates:\r\n            raise ValueError(\"Unknown template type: \" + template_type)\r\n        \r\n        return self.reasoning_templates[template_type].format(**kwargs)\r\n    \r\n    def create_custom_cot(self, problem_description: str, steps: list) -\u003e str:\r\n        \"\"\"Create a custom chain-of-thought prompt\"\"\"\r\n        prompt_parts = [\r\n            \"Let's approach this systematically:\",\r\n            \"\"\r\n        ]\r\n        \r\n        for i, step in enumerate(steps, 1):\r\n            prompt_parts.append(str(i) + \". \" + step)\r\n        \r\n        prompt_parts.extend([\r\n            \"\",\r\n            \"Problem: \" + problem_description,\r\n            \"\",\r\n            \"Step-by-step solution:\"\r\n        ])\r\n        \r\n        return \"\\n\".join(prompt_parts)\r\n\r\n# Usage example\r\ncot = ChainOfThoughtPrompt()\r\n\r\n# For complex problem solving\r\nmath_prompt = cot.generate_cot_prompt(\r\n    \"problem_solving\",\r\n    problem=\"A company's revenue increased by 25% in Q1, decreased by 15% in Q2, and increased by 30% in Q3. If the Q3 revenue was $169,000, what was the initial revenue?\"\r\n)\r\n\r\n# For code debugging\r\ndebug_prompt = cot.generate_cot_prompt(\r\n    \"code_debugging\",\r\n    code=\"\"\"def find_average(numbers):\r\n    total = 0\r\n    for num in numbers:\r\n        total += num\r\n    return total / len(numbers)\r\n\r\nresult = find_average([])\"\"\"\r\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Tree-of-Thought for Complex Decision Making\u003c/h3\u003e\n\u003cp\u003eTree-of-thought explores multiple reasoning paths and evaluates them.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass TreeOfThoughtPrompt:\r\n    def __init__(self, llm_client):\r\n        self.client = llm_client\r\n    \r\n    async def generate_thoughts(self, problem: str, num_thoughts: int = 3) -\u003e list:\r\n        \"\"\"Generate multiple initial thought paths\"\"\"\r\n        prompt = \"\"\"Problem: {problem}\r\n\r\nGenerate {num_thoughts} different approaches or initial thoughts for solving this problem. \r\nFormat each as:\r\nThought X: [brief approach description]\r\n\r\nThoughts:\"\"\".format(problem=problem, num_thoughts=num_thoughts)\r\n        \r\n        response = await self.client.complete([\r\n            {\"role\": \"user\", \"content\": prompt}\r\n        ], temperature=0.8)\r\n        \r\n        # Parse thoughts from response\r\n        content = response[\"choices\"][0][\"message\"][\"content\"]\r\n        thoughts = []\r\n        \r\n        for line in content.split('\\n'):\r\n            if line.strip().startswith('Thought'):\r\n                thought = line.split(':', 1)[1].strip() if ':' in line else line.strip()\r\n                thoughts.append(thought)\r\n        \r\n        return thoughts[:num_thoughts]\r\n    \r\n    async def evaluate_thought(self, problem: str, thought: str) -\u003e float:\r\n        \"\"\"Evaluate the quality/feasibility of a thought\"\"\"\r\n        eval_prompt = \"\"\"Problem: {problem}\r\n\r\nProposed approach: {thought}\r\n\r\nEvaluate this approach on a scale of 1-10 considering:\r\n- Feasibility (can it actually work?)\r\n- Efficiency (is it a good use of resources?)\r\n- Completeness (does it address the full problem?)\r\n\r\nProvide only a numeric score (1-10):\"\"\".format(problem=problem, thought=thought)\r\n        \r\n        response = await self.client.complete([\r\n            {\"role\": \"user\", \"content\": eval_prompt}\r\n        ], temperature=0.1, max_tokens=10)\r\n        \r\n        try:\r\n            score = float(response[\"choices\"][0][\"message\"][\"content\"].strip())\r\n            return min(max(score, 1), 10)  # Clamp between 1-10\r\n        except ValueError:\r\n            return 5.0  # Default score if parsing fails\r\n    \r\n    async def expand_thought(self, problem: str, thought: str) -\u003e str:\r\n        \"\"\"Expand a thought into detailed steps\"\"\"\r\n        expand_prompt = \"\"\"Problem: {problem}\r\n\r\nApproach: {thought}\r\n\r\nExpand this approach into detailed, actionable steps. Be specific and practical:\r\n\r\nDetailed steps:\"\"\".format(problem=problem, thought=thought)\r\n        \r\n        response = await self.client.complete([\r\n            {\"role\": \"user\", \"content\": expand_prompt}\r\n        ], temperature=0.3)\r\n        \r\n        return response[\"choices\"][0][\"message\"][\"content\"]\r\n    \r\n    async def solve_with_tot(self, problem: str) -\u003e dict:\r\n        \"\"\"Solve a problem using tree-of-thought approach\"\"\"\r\n        # Generate initial thoughts\r\n        thoughts = await self.generate_thoughts(problem)\r\n        \r\n        # Evaluate each thought\r\n        evaluations = []\r\n        for thought in thoughts:\r\n            score = await self.evaluate_thought(problem, thought)\r\n            evaluations.append((thought, score))\r\n        \r\n        # Sort by score and select best thoughts\r\n        evaluations.sort(key=lambda x: x[1], reverse=True)\r\n        best_thoughts = evaluations[:2]  # Top 2 thoughts\r\n        \r\n        # Expand the best thoughts\r\n        expanded_solutions = []\r\n        for thought, score in best_thoughts:\r\n            expanded = await self.expand_thought(problem, thought)\r\n            expanded_solutions.append({\r\n                \"approach\": thought,\r\n                \"score\": score,\r\n                \"detailed_solution\": expanded\r\n            })\r\n        \r\n        return {\r\n            \"problem\": problem,\r\n            \"all_thoughts\": evaluations,\r\n            \"best_solutions\": expanded_solutions\r\n        }\r\n\r\n# Usage example\r\nasync def main():\r\n    # Assuming you have an LLM client\r\n    tot = TreeOfThoughtPrompt(llm_client)\r\n    \r\n    result = await tot.solve_with_tot(\r\n        \"Design a system to handle 1 million concurrent users for a social media platform\"\r\n    )\r\n    \r\n    print(\"Best Solutions:\")\r\n    for i, solution in enumerate(result[\"best_solutions\"], 1):\r\n        print(\"Solution \" + str(i) + \" (Score: \" + str(solution[\"score\"]) + \"):\")\r\n        print(solution[\"approach\"])\r\n        print(solution[\"detailed_solution\"])\r\n        print(\"-\" * 50)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eBuilding Production-Ready RAG Systems\u003c/h2\u003e\n\u003ch3\u003e1. RAG Architecture and Components\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport numpy as np\r\nfrom typing import List, Dict, Any, Optional\r\nimport chromadb\r\nfrom sentence_transformers import SentenceTransformer\r\nimport asyncio\r\n\r\nclass DocumentChunker:\r\n    def __init__(self, chunk_size: int = 1000, overlap: int = 200):\r\n        self.chunk_size = chunk_size\r\n        self.overlap = overlap\r\n    \r\n    def chunk_text(self, text: str, metadata: dict = None) -\u003e List[dict]:\r\n        \"\"\"Split text into overlapping chunks\"\"\"\r\n        words = text.split()\r\n        chunks = []\r\n        \r\n        for i in range(0, len(words), self.chunk_size - self.overlap):\r\n            chunk_words = words[i:i + self.chunk_size]\r\n            chunk_text = ' '.join(chunk_words)\r\n            \r\n            chunk_metadata = {\r\n                \"chunk_index\": len(chunks),\r\n                \"start_word\": i,\r\n                \"end_word\": i + len(chunk_words),\r\n                **(metadata or {})\r\n            }\r\n            \r\n            chunks.append({\r\n                \"content\": chunk_text,\r\n                \"metadata\": chunk_metadata\r\n            })\r\n        \r\n        return chunks\r\n    \r\n    def semantic_chunking(self, text: str, encoder, similarity_threshold: float = 0.8) -\u003e List[dict]:\r\n        \"\"\"Chunk text based on semantic similarity\"\"\"\r\n        sentences = text.split('. ')\r\n        if len(sentences) \u0026#x3C; 2:\r\n            return [{\"content\": text, \"metadata\": {\"chunk_index\": 0}}]\r\n        \r\n        # Encode sentences\r\n        embeddings = encoder.encode(sentences)\r\n        \r\n        chunks = []\r\n        current_chunk = [sentences[0]]\r\n        \r\n        for i in range(1, len(sentences)):\r\n            # Calculate similarity with current chunk\r\n            current_embedding = np.mean([embeddings[j] for j in range(len(current_chunk))], axis=0)\r\n            similarity = np.dot(current_embedding, embeddings[i]) / (\r\n                np.linalg.norm(current_embedding) * np.linalg.norm(embeddings[i])\r\n            )\r\n            \r\n            if similarity \u003e similarity_threshold and len(' '.join(current_chunk)) \u0026#x3C; self.chunk_size:\r\n                current_chunk.append(sentences[i])\r\n            else:\r\n                # Finalize current chunk and start new one\r\n                chunks.append({\r\n                    \"content\": '. '.join(current_chunk),\r\n                    \"metadata\": {\"chunk_index\": len(chunks)}\r\n                })\r\n                current_chunk = [sentences[i]]\r\n        \r\n        # Add final chunk\r\n        if current_chunk:\r\n            chunks.append({\r\n                \"content\": '. '.join(current_chunk),\r\n                \"metadata\": {\"chunk_index\": len(chunks)}\r\n            })\r\n        \r\n        return chunks\r\n\r\nclass VectorStore:\r\n    def __init__(self, collection_name: str = \"documents\"):\r\n        self.client = chromadb.Client()\r\n        self.collection = self.client.create_collection(collection_name)\r\n        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\r\n    \r\n    def add_documents(self, documents: List[dict]):\r\n        \"\"\"Add documents to the vector store\"\"\"\r\n        contents = [doc[\"content\"] for doc in documents]\r\n        metadatas = [doc[\"metadata\"] for doc in documents]\r\n        ids = [str(i) for i in range(len(documents))]\r\n        \r\n        # Generate embeddings\r\n        embeddings = self.encoder.encode(contents).tolist()\r\n        \r\n        self.collection.add(\r\n            embeddings=embeddings,\r\n            documents=contents,\r\n            metadatas=metadatas,\r\n            ids=ids\r\n        )\r\n    \r\n    def search(self, query: str, top_k: int = 5) -\u003e List[dict]:\r\n        \"\"\"Search for relevant documents\"\"\"\r\n        query_embedding = self.encoder.encode([query]).tolist()\r\n        \r\n        results = self.collection.query(\r\n            query_embeddings=query_embedding,\r\n            n_results=top_k\r\n        )\r\n        \r\n        documents = []\r\n        for i in range(len(results[\"documents\"][0])):\r\n            documents.append({\r\n                \"content\": results[\"documents\"][0][i],\r\n                \"metadata\": results[\"metadatas\"][0][i],\r\n                \"distance\": results[\"distances\"][0][i]\r\n            })\r\n        \r\n        return documents\r\n\r\nclass RAGSystem:\r\n    def __init__(self, llm_client, vector_store: VectorStore):\r\n        self.llm_client = llm_client\r\n        self.vector_store = vector_store\r\n        self.chunker = DocumentChunker()\r\n    \r\n    def ingest_document(self, content: str, metadata: dict = None):\r\n        \"\"\"Ingest a document into the RAG system\"\"\"\r\n        chunks = self.chunker.chunk_text(content, metadata)\r\n        self.vector_store.add_documents(chunks)\r\n    \r\n    async def retrieve_and_generate(\r\n        self, \r\n        query: str, \r\n        top_k: int = 5,\r\n        system_prompt: str = None\r\n    ) -\u003e dict:\r\n        \"\"\"Retrieve relevant documents and generate response\"\"\"\r\n        \r\n        # Retrieve relevant documents\r\n        relevant_docs = self.vector_store.search(query, top_k=top_k)\r\n        \r\n        # Build context from retrieved documents\r\n        context_parts = []\r\n        for i, doc in enumerate(relevant_docs, 1):\r\n            context_parts.append(\"Document \" + str(i) + \":\")\r\n            context_parts.append(doc[\"content\"])\r\n            context_parts.append(\"\")\r\n        \r\n        context = \"\\n\".join(context_parts)\r\n        \r\n        # Build RAG prompt\r\n        default_system = \"\"\"You are a helpful assistant that answers questions based on the provided context. \r\nUse only the information from the context to answer questions. If the answer cannot be found in the context, say so clearly.\"\"\"\r\n        \r\n        system_message = system_prompt or default_system\r\n        \r\n        user_prompt = \"\"\"Context:\r\n{context}\r\n\r\nQuestion: {query}\r\n\r\nPlease provide a detailed answer based on the context above:\"\"\".format(\r\n            context=context,\r\n            query=query\r\n        )\r\n        \r\n        # Generate response\r\n        response = await self.llm_client.complete([\r\n            {\"role\": \"system\", \"content\": system_message},\r\n            {\"role\": \"user\", \"content\": user_prompt}\r\n        ])\r\n        \r\n        return {\r\n            \"query\": query,\r\n            \"answer\": response[\"choices\"][0][\"message\"][\"content\"],\r\n            \"sources\": relevant_docs,\r\n            \"context_used\": context\r\n        }\r\n    \r\n    async def conversational_rag(\r\n        self, \r\n        query: str, \r\n        conversation_history: List[dict],\r\n        top_k: int = 5\r\n    ) -\u003e dict:\r\n        \"\"\"RAG with conversation history\"\"\"\r\n        \r\n        # Create a comprehensive query including conversation context\r\n        history_context = \"\"\r\n        if conversation_history:\r\n            recent_history = conversation_history[-3:]  # Last 3 exchanges\r\n            history_parts = []\r\n            for exchange in recent_history:\r\n                if exchange[\"role\"] == \"user\":\r\n                    history_parts.append(\"User: \" + exchange[\"content\"])\r\n                elif exchange[\"role\"] == \"assistant\":\r\n                    history_parts.append(\"Assistant: \" + exchange[\"content\"])\r\n            \r\n            history_context = \"\\n\".join(history_parts)\r\n        \r\n        # Enhanced query for better retrieval\r\n        enhanced_query = query\r\n        if history_context:\r\n            enhanced_query = \"Previous conversation:\\n\" + history_context + \"\\n\\nCurrent question: \" + query\r\n        \r\n        # Use the enhanced query for retrieval\r\n        relevant_docs = self.vector_store.search(enhanced_query, top_k=top_k)\r\n        \r\n        # Build context\r\n        context_parts = []\r\n        for i, doc in enumerate(relevant_docs, 1):\r\n            context_parts.append(\"Document \" + str(i) + \":\")\r\n            context_parts.append(doc[\"content\"])\r\n            context_parts.append(\"\")\r\n        \r\n        context = \"\\n\".join(context_parts)\r\n        \r\n        # Build conversational RAG prompt\r\n        messages = [\r\n            {\r\n                \"role\": \"system\", \r\n                \"content\": \"\"\"You are a helpful assistant that answers questions based on provided context and conversation history. \r\nUse the context and previous conversation to provide coherent, contextual responses.\"\"\"\r\n            }\r\n        ]\r\n        \r\n        # Add conversation history\r\n        messages.extend(conversation_history[-5:])  # Last 5 messages\r\n        \r\n        # Add current query with context\r\n        current_prompt = \"\"\"Context:\r\n{context}\r\n\r\nQuestion: {query}\r\n\r\nAnswer:\"\"\".format(context=context, query=query)\r\n        \r\n        messages.append({\"role\": \"user\", \"content\": current_prompt})\r\n        \r\n        response = await self.llm_client.complete(messages)\r\n        \r\n        return {\r\n            \"query\": query,\r\n            \"answer\": response[\"choices\"][0][\"message\"][\"content\"],\r\n            \"sources\": relevant_docs,\r\n            \"enhanced_query\": enhanced_query\r\n        }\r\n\r\n# Usage example\r\nasync def main():\r\n    # Initialize components\r\n    vector_store = VectorStore(\"technical_docs\")\r\n    rag_system = RAGSystem(llm_client, vector_store)\r\n    \r\n    # Ingest documents\r\n    documents = [\r\n        \"Python is a high-level programming language known for its simplicity and readability...\",\r\n        \"Machine learning is a subset of artificial intelligence that enables computers to learn...\",\r\n        \"REST APIs are architectural style for designing networked applications...\"\r\n    ]\r\n    \r\n    for doc in documents:\r\n        rag_system.ingest_document(doc, {\"source\": \"technical_guide\"})\r\n    \r\n    # Query the system\r\n    result = await rag_system.retrieve_and_generate(\r\n        \"What are the benefits of Python programming?\",\r\n        top_k=3\r\n    )\r\n    \r\n    print(\"Answer:\", result[\"answer\"])\r\n    print(\"Sources used:\", len(result[\"sources\"]))\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Advanced RAG Techniques\u003c/h3\u003e\n\u003ch4\u003eHybrid Search (Keyword + Semantic)\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom elasticsearch import Elasticsearch\r\nimport numpy as np\r\n\r\nclass HybridSearchRAG:\r\n    def __init__(self, llm_client, es_host: str = \"localhost:9200\"):\r\n        self.llm_client = llm_client\r\n        self.es_client = Elasticsearch([es_host])\r\n        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\r\n        self.index_name = \"hybrid_docs\"\r\n    \r\n    def create_index(self):\r\n        \"\"\"Create Elasticsearch index with dense vector support\"\"\"\r\n        mapping = {\r\n            \"mappings\": {\r\n                \"properties\": {\r\n                    \"content\": {\"type\": \"text\"},\r\n                    \"embedding\": {\r\n                        \"type\": \"dense_vector\",\r\n                        \"dims\": 384  # all-MiniLM-L6-v2 dimension\r\n                    },\r\n                    \"metadata\": {\"type\": \"object\"}\r\n                }\r\n            }\r\n        }\r\n        \r\n        if self.es_client.indices.exists(index=self.index_name):\r\n            self.es_client.indices.delete(index=self.index_name)\r\n        \r\n        self.es_client.indices.create(index=self.index_name, body=mapping)\r\n    \r\n    def add_document(self, content: str, metadata: dict = None):\r\n        \"\"\"Add document with both text and vector representation\"\"\"\r\n        embedding = self.encoder.encode(content).tolist()\r\n        \r\n        doc = {\r\n            \"content\": content,\r\n            \"embedding\": embedding,\r\n            \"metadata\": metadata or {}\r\n        }\r\n        \r\n        self.es_client.index(index=self.index_name, body=doc)\r\n    \r\n    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -\u003e List[dict]:\r\n        \"\"\"\r\n        Perform hybrid search combining keyword and semantic search\r\n        alpha: weight for semantic search (1-alpha for keyword search)\r\n        \"\"\"\r\n        \r\n        # Keyword search\r\n        keyword_query = {\r\n            \"query\": {\r\n                \"match\": {\r\n                    \"content\": query\r\n                }\r\n            },\r\n            \"size\": top_k * 2  # Get more results for reranking\r\n        }\r\n        \r\n        keyword_results = self.es_client.search(index=self.index_name, body=keyword_query)\r\n        \r\n        # Semantic search\r\n        query_embedding = self.encoder.encode(query).tolist()\r\n        semantic_query = {\r\n            \"query\": {\r\n                \"script_score\": {\r\n                    \"query\": {\"match_all\": {}},\r\n                    \"script\": {\r\n                        \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\r\n                        \"params\": {\"query_vector\": query_embedding}\r\n                    }\r\n                }\r\n            },\r\n            \"size\": top_k * 2\r\n        }\r\n        \r\n        semantic_results = self.es_client.search(index=self.index_name, body=semantic_query)\r\n        \r\n        # Combine and rerank results\r\n        combined_scores = {}\r\n        \r\n        # Add keyword scores\r\n        for hit in keyword_results[\"hits\"][\"hits\"]:\r\n            doc_id = hit[\"_id\"]\r\n            keyword_score = hit[\"_score\"]\r\n            combined_scores[doc_id] = {\r\n                \"keyword_score\": keyword_score,\r\n                \"semantic_score\": 0,\r\n                \"doc\": hit[\"_source\"]\r\n            }\r\n        \r\n        # Add semantic scores\r\n        for hit in semantic_results[\"hits\"][\"hits\"]:\r\n            doc_id = hit[\"_id\"]\r\n            semantic_score = hit[\"_score\"]\r\n            \r\n            if doc_id in combined_scores:\r\n                combined_scores[doc_id][\"semantic_score\"] = semantic_score\r\n            else:\r\n                combined_scores[doc_id] = {\r\n                    \"keyword_score\": 0,\r\n                    \"semantic_score\": semantic_score,\r\n                    \"doc\": hit[\"_source\"]\r\n                }\r\n        \r\n        # Calculate final scores and rank\r\n        final_results = []\r\n        for doc_id, scores in combined_scores.items():\r\n            # Normalize scores (simple min-max normalization)\r\n            keyword_normalized = scores[\"keyword_score\"] / 10.0  # Adjust based on your data\r\n            semantic_normalized = (scores[\"semantic_score\"] - 1.0) / 1.0  # Cosine similarity range\r\n            \r\n            final_score = alpha * semantic_normalized + (1 - alpha) * keyword_normalized\r\n            \r\n            final_results.append({\r\n                \"content\": scores[\"doc\"][\"content\"],\r\n                \"metadata\": scores[\"doc\"][\"metadata\"],\r\n                \"final_score\": final_score,\r\n                \"keyword_score\": scores[\"keyword_score\"],\r\n                \"semantic_score\": scores[\"semantic_score\"]\r\n            })\r\n        \r\n        # Sort by final score and return top k\r\n        final_results.sort(key=lambda x: x[\"final_score\"], reverse=True)\r\n        return final_results[:top_k]\r\n    \r\n    async def query_with_hybrid_search(self, query: str, top_k: int = 5) -\u003e dict:\r\n        \"\"\"Query using hybrid search and generate response\"\"\"\r\n        relevant_docs = self.hybrid_search(query, top_k)\r\n        \r\n        # Build context\r\n        context_parts = []\r\n        for i, doc in enumerate(relevant_docs, 1):\r\n            context_parts.append(\"Document \" + str(i) + \" (Score: \" + str(round(doc[\"final_score\"], 3)) + \"):\")\r\n            context_parts.append(doc[\"content\"])\r\n            context_parts.append(\"\")\r\n        \r\n        context = \"\\n\".join(context_parts)\r\n        \r\n        # Generate response\r\n        prompt = \"\"\"Context:\r\n{context}\r\n\r\nQuestion: {query}\r\n\r\nBased on the context above, provide a comprehensive answer:\"\"\".format(\r\n            context=context,\r\n            query=query\r\n        )\r\n        \r\n        response = await self.llm_client.complete([\r\n            {\"role\": \"user\", \"content\": prompt}\r\n        ])\r\n        \r\n        return {\r\n            \"query\": query,\r\n            \"answer\": response[\"choices\"][0][\"message\"][\"content\"],\r\n            \"sources\": relevant_docs\r\n        }\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eMulti-Query RAG\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass MultiQueryRAG:\r\n    def __init__(self, llm_client, vector_store: VectorStore):\r\n        self.llm_client = llm_client\r\n        self.vector_store = vector_store\r\n    \r\n    async def generate_query_variations(self, original_query: str, num_variations: int = 3) -\u003e List[str]:\r\n        \"\"\"Generate variations of the original query for better retrieval\"\"\"\r\n        prompt = \"\"\"Given the following question, generate {num_variations} different ways to ask the same question. \r\nThese variations should help retrieve more comprehensive information.\r\n\r\nOriginal question: {query}\r\n\r\nGenerate {num_variations} question variations (one per line):\"\"\".format(\r\n            query=original_query,\r\n            num_variations=num_variations\r\n        )\r\n        \r\n        response = await self.llm_client.complete([\r\n            {\"role\": \"user\", \"content\": prompt}\r\n        ], temperature=0.7)\r\n        \r\n        variations = []\r\n        lines = response[\"choices\"][0][\"message\"][\"content\"].strip().split('\\n')\r\n        \r\n        for line in lines:\r\n            line = line.strip()\r\n            if line and not line.startswith('Original'):\r\n                # Remove numbering if present\r\n                if line[0].isdigit() and '.' in line[:3]:\r\n                    line = line.split('.', 1)[1].strip()\r\n                variations.append(line)\r\n        \r\n        return variations[:num_variations]\r\n    \r\n    async def multi_query_retrieve(\r\n        self, \r\n        query: str, \r\n        num_variations: int = 3,\r\n        docs_per_query: int = 3\r\n    ) -\u003e List[dict]:\r\n        \"\"\"Retrieve documents using multiple query variations\"\"\"\r\n        \r\n        # Generate query variations\r\n        query_variations = await self.generate_query_variations(query, num_variations)\r\n        all_queries = [query] + query_variations\r\n        \r\n        # Retrieve documents for each query\r\n        all_docs = []\r\n        seen_content = set()\r\n        \r\n        for q in all_queries:\r\n            docs = self.vector_store.search(q, top_k=docs_per_query)\r\n            \r\n            for doc in docs:\r\n                # Avoid duplicates based on content\r\n                content_hash = hash(doc[\"content\"])\r\n                if content_hash not in seen_content:\r\n                    doc[\"retrieved_by_query\"] = q\r\n                    all_docs.append(doc)\r\n                    seen_content.add(content_hash)\r\n        \r\n        # Sort by relevance score and return top documents\r\n        all_docs.sort(key=lambda x: x[\"distance\"])\r\n        return all_docs[:docs_per_query * len(all_queries)]\r\n    \r\n    async def answer_with_multi_query(self, query: str) -\u003e dict:\r\n        \"\"\"Answer using multi-query RAG approach\"\"\"\r\n        \r\n        # Retrieve using multiple queries\r\n        relevant_docs = await self.multi_query_retrieve(query)\r\n        \r\n        # Build enhanced context\r\n        context_parts = []\r\n        context_parts.append(\"Retrieved information from multiple search perspectives:\")\r\n        context_parts.append(\"\")\r\n        \r\n        for i, doc in enumerate(relevant_docs, 1):\r\n            context_parts.append(\"Source \" + str(i) + \" (found via: '\" + doc[\"retrieved_by_query\"] + \"'):\")\r\n            context_parts.append(doc[\"content\"])\r\n            context_parts.append(\"\")\r\n        \r\n        context = \"\\n\".join(context_parts)\r\n        \r\n        # Generate comprehensive response\r\n        prompt = \"\"\"You have been provided with information retrieved using multiple search approaches for better coverage.\r\n\r\n{context}\r\n\r\nOriginal question: {query}\r\n\r\nProvide a comprehensive answer that synthesizes information from all the sources:\"\"\".format(\r\n            context=context,\r\n            query=query\r\n        )\r\n        \r\n        response = await self.llm_client.complete([\r\n            {\"role\": \"user\", \"content\": prompt}\r\n        ])\r\n        \r\n        return {\r\n            \"query\": query,\r\n            \"answer\": response[\"choices\"][0][\"message\"][\"content\"],\r\n            \"sources\": relevant_docs,\r\n            \"num_sources\": len(relevant_docs)\r\n        }\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eEvaluation and Quality Assurance\u003c/h2\u003e\n\u003ch3\u003eRAG Evaluation Framework\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass RAGEvaluator:\r\n    def __init__(self, llm_client):\r\n        self.llm_client = llm_client\r\n    \r\n    async def evaluate_relevance(self, query: str, retrieved_docs: List[dict]) -\u003e List[float]:\r\n        \"\"\"Evaluate relevance of retrieved documents to the query\"\"\"\r\n        relevance_scores = []\r\n        \r\n        for doc in retrieved_docs:\r\n            prompt = \"\"\"Evaluate how relevant this document is to the given query on a scale of 1-10.\r\n\r\nQuery: {query}\r\n\r\nDocument: {document}\r\n\r\nConsider:\r\n- Does the document contain information that helps answer the query?\r\n- How directly related is the content to the query?\r\n- Would this document be useful for someone trying to answer the query?\r\n\r\nProvide only a numeric score (1-10):\"\"\".format(\r\n                query=query,\r\n                document=doc[\"content\"]\r\n            )\r\n            \r\n            response = await self.llm_client.complete([\r\n                {\"role\": \"user\", \"content\": prompt}\r\n            ], temperature=0.1, max_tokens=5)\r\n            \r\n            try:\r\n                score = float(response[\"choices\"][0][\"message\"][\"content\"].strip())\r\n                relevance_scores.append(min(max(score, 1), 10))\r\n            except ValueError:\r\n                relevance_scores.append(5.0)  # Default score\r\n        \r\n        return relevance_scores\r\n    \r\n    async def evaluate_answer_quality(\r\n        self, \r\n        query: str, \r\n        generated_answer: str, \r\n        ground_truth: str = None\r\n    ) -\u003e dict:\r\n        \"\"\"Evaluate the quality of the generated answer\"\"\"\r\n        \r\n        evaluation_criteria = [\r\n            \"Accuracy: Is the information factually correct?\",\r\n            \"Completeness: Does it fully address the query?\", \r\n            \"Clarity: Is it easy to understand?\",\r\n            \"Relevance: Does it stay focused on the query?\"\r\n        ]\r\n        \r\n        evaluation_results = {}\r\n        \r\n        for criterion in evaluation_criteria:\r\n            prompt = \"\"\"Evaluate the following answer based on this criterion: {criterion}\r\n\r\nQuery: {query}\r\nAnswer: {answer}\r\n\r\nRate on a scale of 1-10 and provide a brief explanation.\r\n\r\nFormat: Score: X/10\r\nExplanation: [brief explanation]\"\"\".format(\r\n                criterion=criterion,\r\n                query=query,\r\n                answer=generated_answer\r\n            )\r\n            \r\n            response = await self.llm_client.complete([\r\n                {\"role\": \"user\", \"content\": prompt}\r\n            ], temperature=0.2)\r\n            \r\n            content = response[\"choices\"][0][\"message\"][\"content\"]\r\n            \r\n            # Parse score and explanation\r\n            score = 5.0  # default\r\n            explanation = content\r\n            \r\n            if \"Score:\" in content:\r\n                try:\r\n                    score_line = [line for line in content.split('\\n') if 'Score:' in line][0]\r\n                    score = float(score_line.split('Score:')[1].split('/')[0].strip())\r\n                except:\r\n                    pass\r\n            \r\n            criterion_name = criterion.split(':')[0].lower()\r\n            evaluation_results[criterion_name] = {\r\n                \"score\": score,\r\n                \"explanation\": explanation\r\n            }\r\n        \r\n        # Calculate overall score\r\n        overall_score = sum(result[\"score\"] for result in evaluation_results.values()) / len(evaluation_results)\r\n        evaluation_results[\"overall\"] = {\"score\": overall_score}\r\n        \r\n        return evaluation_results\r\n    \r\n    async def evaluate_rag_system(\r\n        self, \r\n        test_queries: List[dict],  # [{\"query\": \"...\", \"expected_answer\": \"...\"}]\r\n        rag_system\r\n    ) -\u003e dict:\r\n        \"\"\"Comprehensive evaluation of RAG system\"\"\"\r\n        \r\n        results = {\r\n            \"total_queries\": len(test_queries),\r\n            \"average_relevance\": 0,\r\n            \"average_quality\": 0,\r\n            \"detailed_results\": []\r\n        }\r\n        \r\n        total_relevance = 0\r\n        total_quality = 0\r\n        \r\n        for test_case in test_queries:\r\n            query = test_case[\"query\"]\r\n            expected = test_case.get(\"expected_answer\", \"\")\r\n            \r\n            # Get RAG response\r\n            rag_response = await rag_system.retrieve_and_generate(query)\r\n            \r\n            # Evaluate retrieval relevance\r\n            relevance_scores = await self.evaluate_relevance(query, rag_response[\"sources\"])\r\n            avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0\r\n            \r\n            # Evaluate answer quality\r\n            quality_eval = await self.evaluate_answer_quality(\r\n                query, \r\n                rag_response[\"answer\"], \r\n                expected\r\n            )\r\n            \r\n            result = {\r\n                \"query\": query,\r\n                \"answer\": rag_response[\"answer\"],\r\n                \"relevance_score\": avg_relevance,\r\n                \"quality_score\": quality_eval[\"overall\"][\"score\"],\r\n                \"sources_count\": len(rag_response[\"sources\"]),\r\n                \"detailed_quality\": quality_eval\r\n            }\r\n            \r\n            results[\"detailed_results\"].append(result)\r\n            total_relevance += avg_relevance\r\n            total_quality += quality_eval[\"overall\"][\"score\"]\r\n        \r\n        results[\"average_relevance\"] = total_relevance / len(test_queries)\r\n        results[\"average_quality\"] = total_quality / len(test_queries)\r\n        \r\n        return results\r\n\r\n# Usage example\r\nasync def main():\r\n    evaluator = RAGEvaluator(llm_client)\r\n    \r\n    test_queries = [\r\n        {\r\n            \"query\": \"What are the benefits of using Python for data science?\",\r\n            \"expected_answer\": \"Python offers libraries like pandas, numpy, excellent community support...\"\r\n        },\r\n        {\r\n            \"query\": \"How do you implement a REST API?\",\r\n            \"expected_answer\": \"REST APIs can be implemented using frameworks like Flask, FastAPI...\"\r\n        }\r\n    ]\r\n    \r\n    evaluation_results = await evaluator.evaluate_rag_system(test_queries, rag_system)\r\n    \r\n    print(\"Average Relevance Score:\", evaluation_results[\"average_relevance\"])\r\n    print(\"Average Quality Score:\", evaluation_results[\"average_quality\"])\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eKey Takeaways for Part 2\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eAdvanced Prompting\u003c/strong\u003e: Use few-shot, chain-of-thought, and tree-of-thought techniques for better results\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRAG Architecture\u003c/strong\u003e: Build robust retrieval systems with proper chunking and vector storage\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHybrid Search\u003c/strong\u003e: Combine keyword and semantic search for better retrieval\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Approach\u003c/strong\u003e: Use query variations to capture more relevant information\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation is Critical\u003c/strong\u003e: Implement systematic evaluation for both retrieval and generation quality\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eWhat's Next?\u003c/h2\u003e\n\u003cp\u003eIn \u003cstrong\u003ePart 3\u003c/strong\u003e, we'll focus on production deployment and scaling of LLM applications, covering infrastructure patterns, monitoring, security, and performance optimization strategies.\u003c/p\u003e\n\u003cp\u003eWe'll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInfrastructure and deployment patterns\u003c/li\u003e\n\u003cli\u003eMonitoring and observability for LLM applications\u003c/li\u003e\n\u003cli\u003eSecurity, safety, and compliance considerations\u003c/li\u003e\n\u003cli\u003eScaling strategies and performance optimization\u003c/li\u003e\n\u003cli\u003eCost optimization and resource management\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eThis series provides practical, implementation-focused guidance for engineers building production LLM applications.\u003c/em\u003e\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"18:T6a01,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eLLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models\u003c/h1\u003e\n\u003cp\u003eWelcome to the \u003cstrong\u003eLLM Engineering Mastery\u003c/strong\u003e series! This focused 3-part series is designed for engineers who want to master Large Language Models from a practical, implementation-oriented perspective.\u003c/p\u003e\n\u003ch2\u003eSeries Overview\u003c/h2\u003e\n\u003cp\u003eThis series focuses on the \u003cstrong\u003eengineering perspective\u003c/strong\u003e of working with LLMs, emphasizing practical usage, integration, and optimization rather than theoretical underpinnings.\u003c/p\u003e\n\u003ch3\u003eWhat We'll Cover in This 3-Part Series\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePart 1: Understanding and Leveraging Foundation Models\u003c/strong\u003e (This part)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFoundation model ecosystem and selection\u003c/li\u003e\n\u003cli\u003eAPI integration patterns and best practices\u003c/li\u003e\n\u003cli\u003ePerformance optimization and cost management\u003c/li\u003e\n\u003cli\u003eUnderstanding model capabilities and limitations\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePart 2: Advanced Prompt Engineering and RAG Systems\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced prompting techniques and optimization\u003c/li\u003e\n\u003cli\u003eBuilding production-ready RAG systems\u003c/li\u003e\n\u003cli\u003eContext management and information retrieval\u003c/li\u003e\n\u003cli\u003eEvaluation and quality assurance\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePart 3: Production Deployment and Scaling\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInfrastructure patterns for LLM applications\u003c/li\u003e\n\u003cli\u003eMonitoring, observability, and debugging\u003c/li\u003e\n\u003cli\u003eSecurity, safety, and compliance\u003c/li\u003e\n\u003cli\u003eScaling strategies and performance optimization\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003ePart 1: Understanding and Leveraging Foundation Models\u003c/h2\u003e\n\u003cp\u003eAs an LLM engineer, your first challenge is understanding the landscape of available models and how to effectively integrate them into your applications.\u003c/p\u003e\n\u003ch3\u003eThe Foundation Model Ecosystem\u003c/h3\u003e\n\u003ch4\u003eMajor Model Families and Their Sweet Spots\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI GPT Family\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGPT-4 Turbo\u003c/strong\u003e: Best for complex reasoning, coding, analysis\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGPT-3.5 Turbo\u003c/strong\u003e: Cost-effective for most conversational tasks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUse Cases\u003c/strong\u003e: Customer support, content generation, code assistance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAnthropic Claude Family\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eClaude-3 Opus\u003c/strong\u003e: Superior for safety-critical applications\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eClaude-3 Sonnet\u003c/strong\u003e: Balanced performance and cost\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUse Cases\u003c/strong\u003e: Content moderation, research assistance, ethical AI applications\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eGoogle PaLM/Gemini Family\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGemini Pro\u003c/strong\u003e: Strong multimodal capabilities\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePaLM 2\u003c/strong\u003e: Excellent for multilingual applications\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUse Cases\u003c/strong\u003e: Translation, multimodal applications, search enhancement\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eOpen Source Models\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLlama 2/Code Llama\u003c/strong\u003e: Self-hosted deployment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMistral\u003c/strong\u003e: European alternative with strong performance\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUse Cases\u003c/strong\u003e: On-premises deployment, customization, cost control\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eModel Selection Framework\u003c/h3\u003e\n\u003ch4\u003ePerformance vs. Cost Analysis\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass ModelSelectionFramework:\r\n    def __init__(self):\r\n        self.models = {\r\n            \"gpt-4-turbo\": {\r\n                \"cost_per_1k_tokens\": {\"input\": 0.01, \"output\": 0.03},\r\n                \"context_window\": 128000,\r\n                \"strengths\": [\"reasoning\", \"coding\", \"analysis\"],\r\n                \"latency_ms\": 2000\r\n            },\r\n            \"gpt-3.5-turbo\": {\r\n                \"cost_per_1k_tokens\": {\"input\": 0.0015, \"output\": 0.002},\r\n                \"context_window\": 16000,\r\n                \"strengths\": [\"speed\", \"cost\", \"general\"],\r\n                \"latency_ms\": 800\r\n            },\r\n            \"claude-3-sonnet\": {\r\n                \"cost_per_1k_tokens\": {\"input\": 0.003, \"output\": 0.015},\r\n                \"context_window\": 200000,\r\n                \"strengths\": [\"safety\", \"long_context\", \"reasoning\"],\r\n                \"latency_ms\": 1500\r\n            }\r\n        }\r\n    \r\n    def calculate_cost(self, model_name, input_tokens, output_tokens):\r\n        model = self.models[model_name]\r\n        input_cost = (input_tokens / 1000) * model[\"cost_per_1k_tokens\"][\"input\"]\r\n        output_cost = (output_tokens / 1000) * model[\"cost_per_1k_tokens\"][\"output\"]\r\n        return input_cost + output_cost\r\n    \r\n    def recommend_model(self, requirements):\r\n        \"\"\"\r\n        Recommend model based on requirements:\r\n        - latency_sensitive: bool\r\n        - cost_sensitive: bool\r\n        - context_length: int\r\n        - task_type: str\r\n        \"\"\"\r\n        scores = {}\r\n        for model_name, specs in self.models.items():\r\n            score = 0\r\n            \r\n            # Latency scoring\r\n            if requirements.get(\"latency_sensitive\", False):\r\n                score += 10 if specs[\"latency_ms\"] \u0026#x3C; 1000 else 5\r\n            \r\n            # Cost scoring\r\n            if requirements.get(\"cost_sensitive\", False):\r\n                avg_cost = (specs[\"cost_per_1k_tokens\"][\"input\"] + \r\n                           specs[\"cost_per_1k_tokens\"][\"output\"]) / 2\r\n                score += 10 if avg_cost \u0026#x3C; 0.005 else 5\r\n            \r\n            # Context length scoring\r\n            if requirements.get(\"context_length\", 0) \u003e specs[\"context_window\"]:\r\n                score = 0  # Disqualify if context too long\r\n            \r\n            # Task type scoring\r\n            task_type = requirements.get(\"task_type\", \"\")\r\n            if task_type in specs[\"strengths\"]:\r\n                score += 15\r\n            \r\n            scores[model_name] = score\r\n        \r\n        return max(scores, key=scores.get) if scores else None\r\n\r\n# Usage example\r\nframework = ModelSelectionFramework()\r\nrecommendation = framework.recommend_model({\r\n    \"latency_sensitive\": True,\r\n    \"cost_sensitive\": True,\r\n    \"context_length\": 8000,\r\n    \"task_type\": \"general\"\r\n})\r\nprint(\"Recommended model:\", recommendation)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eAPI Integration Patterns\u003c/h3\u003e\n\u003ch4\u003e1. Robust Client Implementation\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport asyncio\r\nimport aiohttp\r\nimport backoff\r\nfrom typing import Optional, Dict, Any\r\nimport logging\r\n\r\nclass LLMClient:\r\n    def __init__(self, api_key: str, base_url: str, model: str):\r\n        self.api_key = api_key\r\n        self.base_url = base_url\r\n        self.model = model\r\n        self.session = None\r\n        self.logger = logging.getLogger(__name__)\r\n    \r\n    async def __aenter__(self):\r\n        self.session = aiohttp.ClientSession(\r\n            headers={\"Authorization\": \"Bearer {self.api_key}\".format(self.api_key)},\r\n            timeout=aiohttp.ClientTimeout(total=60)\r\n        )\r\n        return self\r\n    \r\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\r\n        if self.session:\r\n            await self.session.close()\r\n    \r\n    @backoff.on_exception(\r\n        backoff.expo,\r\n        (aiohttp.ClientError, asyncio.TimeoutError),\r\n        max_tries=3,\r\n        max_time=300\r\n    )\r\n    async def complete(\r\n        self, \r\n        messages: list,\r\n        temperature: float = 0.7,\r\n        max_tokens: int = 1000,\r\n        **kwargs\r\n    ) -\u003e Dict[str, Any]:\r\n        \"\"\"\r\n        Complete a chat conversation with robust error handling\r\n        \"\"\"\r\n        payload = {\r\n            \"model\": self.model,\r\n            \"messages\": messages,\r\n            \"temperature\": temperature,\r\n            \"max_tokens\": max_tokens,\r\n            **kwargs\r\n        }\r\n        \r\n        try:\r\n            async with self.session.post(\r\n                \"{self.base_url}/chat/completions\".format(self.base_url),\r\n                json=payload\r\n            ) as response:\r\n                response.raise_for_status()\r\n                result = await response.json()\r\n                \r\n                # Log usage for monitoring\r\n                usage = result.get(\"usage\", {})\r\n                self.logger.info(\r\n                    \"API call completed\",\r\n                    extra={\r\n                        \"model\": self.model,\r\n                        \"input_tokens\": usage.get(\"prompt_tokens\", 0),\r\n                        \"output_tokens\": usage.get(\"completion_tokens\", 0),\r\n                        \"total_tokens\": usage.get(\"total_tokens\", 0)\r\n                    }\r\n                )\r\n                \r\n                return result\r\n                \r\n        except aiohttp.ClientResponseError as e:\r\n            if e.status == 429:  # Rate limit\r\n                self.logger.warning(\"Rate limited, backing off\")\r\n                raise\r\n            elif e.status == 400:  # Bad request\r\n                self.logger.error(\"Bad request\", extra={\"payload\": payload})\r\n                raise ValueError(\"Invalid request parameters\")\r\n            else:\r\n                self.logger.error(\"API error\", extra={\"status\": e.status})\r\n                raise\r\n    \r\n    async def stream_complete(\r\n        self,\r\n        messages: list,\r\n        **kwargs\r\n    ):\r\n        \"\"\"\r\n        Stream completion for real-time applications\r\n        \"\"\"\r\n        payload = {\r\n            \"model\": self.model,\r\n            \"messages\": messages,\r\n            \"stream\": True,\r\n            **kwargs\r\n        }\r\n        \r\n        async with self.session.post(\r\n            \"{self.base_url}/chat/completions\".format(self.base_url),\r\n            json=payload\r\n        ) as response:\r\n            response.raise_for_status()\r\n            \r\n            async for line in response.content:\r\n                line = line.decode('utf-8').strip()\r\n                if line.startswith('data: '):\r\n                    data = line[6:]\r\n                    if data == '[DONE]':\r\n                        break\r\n                    try:\r\n                        yield json.loads(data)\r\n                    except json.JSONDecodeError:\r\n                        continue\r\n\r\n# Usage example\r\nasync def main():\r\n    async with LLMClient(\r\n        api_key=\"your-api-key\",\r\n        base_url=\"https://api.openai.com/v1\",\r\n        model=\"gpt-3.5-turbo\"\r\n    ) as client:\r\n        \r\n        response = await client.complete(\r\n            messages=[\r\n                {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\r\n            ],\r\n            temperature=0.3\r\n        )\r\n        \r\n        print(response[\"choices\"][0][\"message\"][\"content\"])\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e2. Multi-Provider Abstraction Layer\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom abc import ABC, abstractmethod\r\nfrom enum import Enum\r\n\r\nclass Provider(Enum):\r\n    OPENAI = \"openai\"\r\n    ANTHROPIC = \"anthropic\"\r\n    GOOGLE = \"google\"\r\n\r\nclass LLMProvider(ABC):\r\n    @abstractmethod\r\n    async def complete(self, messages: list, **kwargs) -\u003e Dict[str, Any]:\r\n        pass\r\n    \r\n    @abstractmethod\r\n    def estimate_tokens(self, text: str) -\u003e int:\r\n        pass\r\n\r\nclass OpenAIProvider(LLMProvider):\r\n    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\r\n        self.client = LLMClient(api_key, \"https://api.openai.com/v1\", model)\r\n    \r\n    async def complete(self, messages: list, **kwargs) -\u003e Dict[str, Any]:\r\n        async with self.client as client:\r\n            return await client.complete(messages, **kwargs)\r\n    \r\n    def estimate_tokens(self, text: str) -\u003e int:\r\n        # Rough estimation: 1 token ≈ 4 characters\r\n        return len(text) // 4\r\n\r\nclass AnthropicProvider(LLMProvider):\r\n    def __init__(self, api_key: str, model: str = \"claude-3-sonnet-20240229\"):\r\n        self.api_key = api_key\r\n        self.model = model\r\n    \r\n    async def complete(self, messages: list, **kwargs) -\u003e Dict[str, Any]:\r\n        # Implement Anthropic-specific API calls\r\n        # Convert messages format, handle different response structure\r\n        pass\r\n    \r\n    def estimate_tokens(self, text: str) -\u003e int:\r\n        # Anthropic-specific token estimation\r\n        return len(text) // 4\r\n\r\nclass LLMManager:\r\n    def __init__(self):\r\n        self.providers = {}\r\n    \r\n    def register_provider(self, name: str, provider: LLMProvider):\r\n        self.providers[name] = provider\r\n    \r\n    async def complete(\r\n        self, \r\n        provider_name: str, \r\n        messages: list, \r\n        fallback_providers: list = None,\r\n        **kwargs\r\n    ) -\u003e Dict[str, Any]:\r\n        \"\"\"\r\n        Complete with primary provider, fallback to alternatives on failure\r\n        \"\"\"\r\n        providers_to_try = [provider_name] + (fallback_providers or [])\r\n        \r\n        for provider in providers_to_try:\r\n            if provider not in self.providers:\r\n                continue\r\n                \r\n            try:\r\n                return await self.providers[provider].complete(messages, **kwargs)\r\n            except Exception as e:\r\n                logging.warning(\"Provider {provider} failed: {e}\".format(e))\r\n                if provider == providers_to_try[-1]:  # Last provider\r\n                    raise\r\n                continue\r\n\r\n# Usage\r\nmanager = LLMManager()\r\nmanager.register_provider(\"openai\", OpenAIProvider(\"openai-key\"))\r\nmanager.register_provider(\"anthropic\", AnthropicProvider(\"anthropic-key\"))\r\n\r\nresponse = await manager.complete(\r\n    \"openai\",\r\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\r\n    fallback_providers=[\"anthropic\"]\r\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003ePerformance Optimization and Cost Management\u003c/h3\u003e\n\u003ch4\u003eToken Usage Optimization\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass TokenOptimizer:\r\n    def __init__(self, provider: LLMProvider):\r\n        self.provider = provider\r\n    \r\n    def compress_conversation_history(\r\n        self, \r\n        messages: list, \r\n        max_tokens: int = 4000\r\n    ) -\u003e list:\r\n        \"\"\"\r\n        Intelligently compress conversation history to fit token limits\r\n        \"\"\"\r\n        # Always keep system message and last user message\r\n        if len(messages) \u0026#x3C;= 2:\r\n            return messages\r\n        \r\n        system_msg = messages[0] if messages[0][\"role\"] == \"system\" else None\r\n        recent_messages = messages[-2:]  # Last user + assistant\r\n        middle_messages = messages[1:-2] if len(messages) \u003e 2 else []\r\n        \r\n        # Estimate current token usage\r\n        current_tokens = sum(\r\n            self.provider.estimate_tokens(msg[\"content\"]) \r\n            for msg in messages\r\n        )\r\n        \r\n        if current_tokens \u0026#x3C;= max_tokens:\r\n            return messages\r\n        \r\n        # Compress middle messages by summarizing them\r\n        if middle_messages:\r\n            summary_prompt = self._create_summary_prompt(middle_messages)\r\n            # Use cheaper model for summarization\r\n            summary_response = await self.provider.complete(\r\n                [{\"role\": \"user\", \"content\": summary_prompt}],\r\n                model=\"gpt-3.5-turbo\",  # Cheaper model\r\n                max_tokens=200,\r\n                temperature=0.1\r\n            )\r\n            \r\n            summary_message = {\r\n                \"role\": \"assistant\",\r\n                \"content\": \"[Previous conversation summary: \" + summary_response['choices'][0]['message']['content'] + \"]\"\r\n            }\r\n            \r\n            compressed = [system_msg, summary_message] + recent_messages\r\n            return [msg for msg in compressed if msg is not None]\r\n        \r\n        return ([system_msg] if system_msg else []) + recent_messages\r\n    \r\n    def _create_summary_prompt(self, messages: list) -\u003e str:\r\n        conversation = \"\\n\".join([\r\n            msg['role'] + \": \" + msg['content'] for msg in messages\r\n        ])\r\n        return \"\"\"Summarize this conversation concisely, preserving key context and decisions made:\r\n\r\n\"\"\" + conversation + \"\"\"\r\n\r\nSummary (max 150 words):\"\"\"\r\n\r\n    async def optimize_prompt(self, prompt: str, task_type: str = \"general\") -\u003e str:\r\n        \"\"\"\r\n        Optimize prompt for clarity and token efficiency\r\n        \"\"\"\r\n        optimization_prompts = {\r\n            \"general\": \"Rewrite this prompt to be more concise while preserving meaning\",\r\n            \"coding\": \"Rewrite this coding prompt to be clear and specific\",\r\n            \"analysis\": \"Rewrite this analysis prompt to be focused and actionable\"\r\n        }\r\n        \r\n        opt_prompt = optimization_prompts.get(task_type, optimization_prompts[\"general\"])\r\n        \r\n        response = await self.provider.complete([\r\n            {\r\n                \"role\": \"user\", \r\n                \"content\": opt_prompt + \":\\n\\n\" + prompt + \"\\n\\nOptimized prompt:\"\r\n            }\r\n        ], max_tokens=300, temperature=0.1)\r\n        \r\n        return response[\"choices\"][0][\"message\"][\"content\"].strip()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eCost Monitoring and Budgeting\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport asyncio\r\nfrom datetime import datetime, timedelta\r\nfrom dataclasses import dataclass\r\nfrom typing import Dict, List\r\n\r\n@dataclass\r\nclass UsageRecord:\r\n    timestamp: datetime\r\n    model: str\r\n    input_tokens: int\r\n    output_tokens: int\r\n    cost: float\r\n    operation: str\r\n\r\nclass CostMonitor:\r\n    def __init__(self, daily_budget: float = 100.0):\r\n        self.daily_budget = daily_budget\r\n        self.usage_records: List[UsageRecord] = []\r\n        self.model_costs = {\r\n            \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\r\n            \"gpt-3.5-turbo\": {\"input\": 0.0015, \"output\": 0.002},\r\n            \"claude-3-sonnet\": {\"input\": 0.003, \"output\": 0.015}\r\n        }\r\n    \r\n    def log_usage(\r\n        self, \r\n        model: str, \r\n        input_tokens: int, \r\n        output_tokens: int,\r\n        operation: str = \"completion\"\r\n    ):\r\n        \"\"\"Log API usage for cost tracking\"\"\"\r\n        cost = self.calculate_cost(model, input_tokens, output_tokens)\r\n        \r\n        record = UsageRecord(\r\n            timestamp=datetime.now(),\r\n            model=model,\r\n            input_tokens=input_tokens,\r\n            output_tokens=output_tokens,\r\n            cost=cost,\r\n            operation=operation\r\n        )\r\n        \r\n        self.usage_records.append(record)\r\n        \r\n        # Check if approaching budget\r\n        daily_spend = self.get_daily_spend()\r\n        if daily_spend \u003e self.daily_budget * 0.8:\r\n            logging.warning(\r\n                \"Approaching daily budget: $\" + str(round(daily_spend, 2)) + \" / $\" + str(self.daily_budget)\r\n            )\r\n    \r\n    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -\u003e float:\r\n        \"\"\"Calculate cost for API call\"\"\"\r\n        if model not in self.model_costs:\r\n            return 0.0\r\n        \r\n        costs = self.model_costs[model]\r\n        input_cost = (input_tokens / 1000) * costs[\"input\"]\r\n        output_cost = (output_tokens / 1000) * costs[\"output\"]\r\n        \r\n        return input_cost + output_cost\r\n    \r\n    def get_daily_spend(self, date: datetime = None) -\u003e float:\r\n        \"\"\"Get total spending for a specific day\"\"\"\r\n        if date is None:\r\n            date = datetime.now()\r\n        \r\n        start_of_day = date.replace(hour=0, minute=0, second=0, microsecond=0)\r\n        end_of_day = start_of_day + timedelta(days=1)\r\n        \r\n        daily_records = [\r\n            record for record in self.usage_records\r\n            if start_of_day \u0026#x3C;= record.timestamp \u0026#x3C; end_of_day\r\n        ]\r\n        \r\n        return sum(record.cost for record in daily_records)\r\n    \r\n    def get_model_breakdown(self, days: int = 7) -\u003e Dict[str, float]:\r\n        \"\"\"Get cost breakdown by model for the last N days\"\"\"\r\n        cutoff_date = datetime.now() - timedelta(days=days)\r\n        recent_records = [\r\n            record for record in self.usage_records\r\n            if record.timestamp \u003e= cutoff_date\r\n        ]\r\n        \r\n        breakdown = {}\r\n        for record in recent_records:\r\n            breakdown[record.model] = breakdown.get(record.model, 0) + record.cost\r\n        \r\n        return breakdown\r\n    \r\n    def should_throttle(self) -\u003e bool:\r\n        \"\"\"Check if we should throttle requests due to budget\"\"\"\r\n        return self.get_daily_spend() \u003e= self.daily_budget\r\n\r\n# Integration with LLM client\r\nclass MonitoredLLMClient(LLMClient):\r\n    def __init__(self, *args, cost_monitor: CostMonitor = None, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.cost_monitor = cost_monitor or CostMonitor()\r\n    \r\n    async def complete(self, messages: list, **kwargs) -\u003e Dict[str, Any]:\r\n        # Check budget before making request\r\n        if self.cost_monitor.should_throttle():\r\n            raise Exception(\"Daily budget exceeded\")\r\n        \r\n        response = await super().complete(messages, **kwargs)\r\n        \r\n        # Log usage after successful request\r\n        usage = response.get(\"usage\", {})\r\n        self.cost_monitor.log_usage(\r\n            model=self.model,\r\n            input_tokens=usage.get(\"prompt_tokens\", 0),\r\n            output_tokens=usage.get(\"completion_tokens\", 0),\r\n            operation=\"chat_completion\"\r\n        )\r\n        \r\n        return response\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eUnderstanding Model Capabilities and Limitations\u003c/h3\u003e\n\u003ch4\u003eCapability Assessment Framework\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport time\r\n\r\nclass CapabilityTester:\r\n    def __init__(self, llm_client: LLMClient):\r\n        self.client = llm_client\r\n        self.test_suite = {\r\n            \"reasoning\": [\r\n                \"If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\",\r\n                \"A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\"\r\n            ],\r\n            \"coding\": [\r\n                \"Write a Python function to find the longest palindromic substring\",\r\n                \"Implement a basic LRU cache in Python\"\r\n            ],\r\n            \"math\": [\r\n                \"Calculate the derivative of x^3 + 2x^2 - 5x + 3\",\r\n                \"Solve the system: 2x + 3y = 7, x - y = 1\"\r\n            ],\r\n            \"creativity\": [\r\n                \"Write a haiku about debugging code\",\r\n                \"Create a metaphor explaining machine learning to a 5-year-old\"\r\n            ],\r\n            \"analysis\": [\r\n                \"Analyze the pros and cons of microservices vs monolithic architecture\",\r\n                \"Compare the trade-offs between SQL and NoSQL databases\"\r\n            ]\r\n        }\r\n    \r\n    async def run_capability_assessment(self) -\u003e Dict[str, Dict[str, Any]]:\r\n        \"\"\"Run comprehensive capability assessment\"\"\"\r\n        results = {}\r\n        \r\n        for category, prompts in self.test_suite.items():\r\n            category_results = {\r\n                \"scores\": [],\r\n                \"responses\": [],\r\n                \"avg_latency\": 0,\r\n                \"consistency\": 0\r\n            }\r\n            \r\n            latencies = []\r\n            responses = []\r\n            \r\n            for prompt in prompts:\r\n                start_time = time.time()\r\n                \r\n                # Test multiple times for consistency\r\n                test_responses = []\r\n                for _ in range(3):\r\n                    response = await self.client.complete([\r\n                        {\"role\": \"user\", \"content\": prompt}\r\n                    ], temperature=0.1)\r\n                    \r\n                    content = response[\"choices\"][0][\"message\"][\"content\"]\r\n                    test_responses.append(content)\r\n                \r\n                end_time = time.time()\r\n                latencies.append(end_time - start_time)\r\n                responses.append(test_responses)\r\n                \r\n                # Score quality (simplified - in practice, use more sophisticated scoring)\r\n                quality_score = self._score_response(prompt, test_responses[0], category)\r\n                category_results[\"scores\"].append(quality_score)\r\n                category_results[\"responses\"].append(test_responses[0])\r\n            \r\n            category_results[\"avg_latency\"] = sum(latencies) / len(latencies)\r\n            category_results[\"consistency\"] = self._calculate_consistency(responses)\r\n            \r\n            results[category] = category_results\r\n        \r\n        return results\r\n    \r\n    def _score_response(self, prompt: str, response: str, category: str) -\u003e float:\r\n        \"\"\"Score response quality (simplified scoring)\"\"\"\r\n        # In practice, implement category-specific scoring logic\r\n        # This is a placeholder\r\n        if category == \"reasoning\":\r\n            # Check for logical structure, correct answer if verifiable\r\n            return 8.5 if len(response) \u003e 50 and \"because\" in response.lower() else 6.0\r\n        elif category == \"coding\":\r\n            # Check for code blocks, proper syntax\r\n            return 9.0 if \"def \" in response or \"function\" in response else 5.0\r\n        elif category == \"math\":\r\n            # Check for mathematical notation, step-by-step solution\r\n            return 8.0 if any(char in response for char in \"=+-*/\") else 4.0\r\n        else:\r\n            # General quality based on length and coherence\r\n            return 7.0 if len(response) \u003e 30 else 4.0\r\n    \r\n    def _calculate_consistency(self, responses: List[List[str]]) -\u003e float:\r\n        \"\"\"Calculate consistency across multiple runs\"\"\"\r\n        # Simplified consistency calculation\r\n        # In practice, use semantic similarity metrics\r\n        total_similarity = 0\r\n        count = 0\r\n        \r\n        for response_group in responses:\r\n            for i in range(len(response_group)):\r\n                for j in range(i + 1, len(response_group)):\r\n                    # Simple similarity based on length and word overlap\r\n                    r1, r2 = response_group[i], response_group[j]\r\n                    similarity = len(set(r1.split()) \u0026#x26; set(r2.split())) / max(len(r1.split()), len(r2.split()))\r\n                    total_similarity += similarity\r\n                    count += 1\r\n        \r\n        return total_similarity / count if count \u003e 0 else 0\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eKey Takeaways for Part 1\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eModel Selection is Critical\u003c/strong\u003e: Choose based on specific requirements (cost, latency, capabilities)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobust Integration\u003c/strong\u003e: Implement proper error handling, retries, and monitoring\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCost Management\u003c/strong\u003e: Track usage actively and implement budget controls\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUnderstand Limitations\u003c/strong\u003e: Test capabilities systematically and plan accordingly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAbstraction Layers\u003c/strong\u003e: Build provider-agnostic systems for flexibility\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eWhat's Next?\u003c/h2\u003e\n\u003cp\u003eIn \u003cstrong\u003ePart 2\u003c/strong\u003e, we'll dive deep into advanced prompt engineering techniques and building production-ready RAG (Retrieval-Augmented Generation) systems that can enhance your LLM applications with external knowledge.\u003c/p\u003e\n\u003cp\u003eWe'll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced prompting strategies (few-shot, chain-of-thought, tree-of-thought)\u003c/li\u003e\n\u003cli\u003eBuilding robust RAG architectures\u003c/li\u003e\n\u003cli\u003eVector databases and embedding strategies\u003c/li\u003e\n\u003cli\u003eContext optimization and retrieval quality\u003c/li\u003e\n\u003cli\u003eEvaluation frameworks for prompt and RAG performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eThis series is designed for practicing engineers who want to master LLM integration and deployment. Each part builds upon the previous while remaining practical and implementation-focused.\u003c/em\u003e\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"19:T646,\u003ch1\u003eLLM Engineering Mastery\u003c/h1\u003e\n\u003cp\u003ePart 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.\u003c/p\u003e\n\u003ch2\u003eSeries Overview\u003c/h2\u003e\n\u003cp\u003eThis comprehensive 3-part series covers:\u003c/p\u003e\n\u003ch3\u003e1. LLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models\u003c/h3\u003e\n\u003cp\u003ePart 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/llm-engineering-mastery-part-1/\"\u003eRead Part 1 →\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e2. LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems\u003c/h3\u003e\n\u003cp\u003ePart 2 of the LLM Engineering Mastery series: Master advanced prompt engineering techniques and build production-ready RAG systems for enhanced LLM applications.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/llm-engineering-mastery-part-2/\"\u003eRead Part 2 →\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e3. LLM Engineering Mastery: Part 3 - Production Deployment and Scaling\u003c/h3\u003e\n\u003cp\u003ePart 3 of the LLM Engineering Mastery series: Master production deployment, scaling strategies, monitoring, and security for enterprise-grade LLM applications.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/llm-engineering-mastery-part-3/\"\u003eRead Part 3 →\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eReady to dive in? Start with Part 1 and work your way through the series:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/llm-engineering-mastery-part-1/\"\u003eBegin with Part 1 →\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eThis series is designed to be read sequentially for the best learning experience.\u003c/em\u003e\u003c/p\u003e\n1a:T1c03,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eUnderstanding Hash Tables: The Ultimate Guide\u003c/h1\u003e\n\u003cp\u003eHash tables are one of the most fundamental and powerful data structures in computer science, offering average-case O(1) time complexity for basic operations. This comprehensive guide explores hash tables from the ground up.\u003c/p\u003e\n\u003ch2\u003eWhat Are Hash Tables?\u003c/h2\u003e\n\u003cp\u003eA hash table (also known as a hash map) is a data structure that implements an associative array abstract data type, mapping keys to values. It uses a hash function to compute an index into an array of buckets or slots.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"./assets/overview.png\" alt=\"Hash Table Overview\"\u003e\u003c/p\u003e\n\u003ch3\u003eKey Components\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eHash Function\u003c/strong\u003e: Converts keys into array indices\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuckets\u003c/strong\u003e: Array slots that store key-value pairs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCollision Resolution\u003c/strong\u003e: Strategy for handling multiple keys mapping to the same index\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"./assets/anatomy.png\" alt=\"Hash Table Anatomy\"\u003e\u003c/p\u003e\n\u003ch2\u003eHash Functions\u003c/h2\u003e\n\u003cp\u003eA good hash function should:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBe deterministic\u003c/li\u003e\n\u003cli\u003eDistribute keys uniformly\u003c/li\u003e\n\u003cli\u003eBe fast to compute\u003c/li\u003e\n\u003cli\u003eMinimize collisions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCommon Hash Functions\u003c/h3\u003e\n\u003ch4\u003eDivision Method\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003efunction hashDivision(key, tableSize) {\r\n  return key % tableSize;\r\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eMultiplication Method\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003efunction hashMultiplication(key, tableSize) {\r\n  const A = 0.6180339887; // (sqrt(5) - 1) / 2\r\n  return Math.floor(tableSize * ((key * A) % 1));\r\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCollision Resolution\u003c/h2\u003e\n\u003cp\u003eWhen two keys hash to the same index, we need collision resolution strategies:\u003c/p\u003e\n\u003ch3\u003e1. Chaining (Separate Chaining)\u003c/h3\u003e\n\u003cp\u003eEach bucket contains a linked list of entries:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"./assets/chaining.png\" alt=\"Chaining Collision Resolution\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003eclass HashTableChaining {\r\n  constructor(size = 53) {\r\n    this.keyMap = new Array(size);\r\n  }\r\n  \r\n  hash(key) {\r\n    let total = 0;\r\n    let WEIRD_PRIME = 31;\r\n    for (let i = 0; i \u0026#x3C; Math.min(key.length, 100); i++) {\r\n      let char = key[i];\r\n      let value = char.charCodeAt(0) - 96;\r\n      total = (total * WEIRD_PRIME + value) % this.keyMap.length;\r\n    }\r\n    return total;\r\n  }\r\n  \r\n  set(key, value) {\r\n    let index = this.hash(key);\r\n    if (!this.keyMap[index]) {\r\n      this.keyMap[index] = [];\r\n    }\r\n    this.keyMap[index].push([key, value]);\r\n  }\r\n  \r\n  get(key) {\r\n    let index = this.hash(key);\r\n    if (this.keyMap[index]) {\r\n      for (let i = 0; i \u0026#x3C; this.keyMap[index].length; i++) {\r\n        if (this.keyMap[index][i][0] === key) {\r\n          return this.keyMap[index][i][1];\r\n        }\r\n      }\r\n    }\r\n    return undefined;\r\n  }\r\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Open Addressing\u003c/h3\u003e\n\u003cp\u003eAll entries are stored directly in the hash table array:\u003c/p\u003e\n\u003ch4\u003eLinear Probing\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003eclass HashTableLinearProbing {\r\n  constructor(size = 53) {\r\n    this.keyMap = new Array(size);\r\n    this.values = new Array(size);\r\n  }\r\n  \r\n  hash(key) {\r\n    let total = 0;\r\n    let WEIRD_PRIME = 31;\r\n    for (let i = 0; i \u0026#x3C; Math.min(key.length, 100); i++) {\r\n      let char = key[i];\r\n      let value = char.charCodeAt(0) - 96;\r\n      total = (total * WEIRD_PRIME + value) % this.keyMap.length;\r\n    }\r\n    return total;\r\n  }\r\n  \r\n  set(key, value) {\r\n    let index = this.hash(key);\r\n    while (this.keyMap[index] !== undefined) {\r\n      if (this.keyMap[index] === key) {\r\n        this.values[index] = value;\r\n        return;\r\n      }\r\n      index = (index + 1) % this.keyMap.length;\r\n    }\r\n    this.keyMap[index] = key;\r\n    this.values[index] = value;\r\n  }\r\n  \r\n  get(key) {\r\n    let index = this.hash(key);\r\n    while (this.keyMap[index] !== undefined) {\r\n      if (this.keyMap[index] === key) {\r\n        return this.values[index];\r\n      }\r\n      index = (index + 1) % this.keyMap.length;\r\n    }\r\n    return undefined;\r\n  }\r\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003ePerformance Analysis\u003c/h2\u003e\n\u003ch3\u003eTime Complexity\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOperation\u003c/th\u003e\n\u003cth\u003eAverage Case\u003c/th\u003e\n\u003cth\u003eWorst Case\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eInsert\u003c/td\u003e\n\u003ctd\u003eO(1)\u003c/td\u003e\n\u003ctd\u003eO(n)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDelete\u003c/td\u003e\n\u003ctd\u003eO(1)\u003c/td\u003e\n\u003ctd\u003eO(n)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSearch\u003c/td\u003e\n\u003ctd\u003eO(1)\u003c/td\u003e\n\u003ctd\u003eO(n)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eSpace Complexity\u003c/h3\u003e\n\u003cp\u003eO(n) where n is the number of key-value pairs.\u003c/p\u003e\n\u003ch3\u003eLoad Factor\u003c/h3\u003e\n\u003cp\u003eThe load factor α = n/m where:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003en = number of stored elements\u003c/li\u003e\n\u003cli\u003em = number of buckets\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOptimal load factors:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eChaining\u003c/strong\u003e: α ≤ 1\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpen Addressing\u003c/strong\u003e: α ≤ 0.7\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAdvanced Topics\u003c/h2\u003e\n\u003ch3\u003eDynamic Resizing\u003c/h3\u003e\n\u003cp\u003eWhen load factor exceeds threshold, resize the hash table:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003eresize() {\r\n  let oldKeyMap = this.keyMap;\r\n  let oldValues = this.values;\r\n  \r\n  this.keyMap = new Array(oldKeyMap.length * 2);\r\n  this.values = new Array(oldValues.length * 2);\r\n  \r\n  for (let i = 0; i \u0026#x3C; oldKeyMap.length; i++) {\r\n    if (oldKeyMap[i] !== undefined) {\r\n      this.set(oldKeyMap[i], oldValues[i]);\r\n    }\r\n  }\r\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eConsistent Hashing\u003c/h3\u003e\n\u003cp\u003eUsed in distributed systems to minimize rehashing when nodes are added/removed.\u003c/p\u003e\n\u003ch2\u003eReal-World Applications\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDatabase Indexing\u003c/strong\u003e: Fast record lookup\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCaching\u003c/strong\u003e: Web browsers, CDNs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSymbol Tables\u003c/strong\u003e: Compilers and interpreters\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSets\u003c/strong\u003e: Unique element storage\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRouting Tables\u003c/strong\u003e: Network packet routing\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eBest Practices\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eChoose appropriate hash function\u003c/strong\u003e for your key type\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMonitor load factor\u003c/strong\u003e and resize when necessary\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHandle collisions efficiently\u003c/strong\u003e based on usage patterns\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConsider memory vs. time tradeoffs\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUse prime numbers\u003c/strong\u003e for table sizes to reduce clustering\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eCommon Pitfalls\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePoor hash function\u003c/strong\u003e leading to clustering\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIgnoring load factor\u003c/strong\u003e causing performance degradation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNot handling edge cases\u003c/strong\u003e like null keys\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory leaks\u003c/strong\u003e in chaining implementations\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eHash tables are essential for building efficient software systems. Understanding their internals helps you:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChoose the right implementation for your use case\u003c/li\u003e\n\u003cli\u003eDebug performance issues\u003c/li\u003e\n\u003cli\u003eDesign better algorithms\u003c/li\u003e\n\u003cli\u003eOptimize memory usage\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe key to effective hash table usage is balancing simplicity, performance, and memory consumption based on your specific requirements.\u003c/p\u003e\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://example.com\"\u003eIntroduction to Algorithms by Cormen et al.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://example.com\"\u003eHash Table Visualization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://example.com\"\u003ePerformance Analysis of Hash Functions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://example.com\"\u003eDistributed Hash Tables\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"article\",null,{\"className\":\"min-h-screen bg-gradient-to-br from-gray-50 to-gray-100\",\"children\":[[\"$\",\"header\",null,{\"className\":\"bg-white border-b border-gray-200\",\"children\":[\"$\",\"div\",null,{\"className\":\"medium-container py-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-3xl mx-auto text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-2 mb-6\",\"children\":[[\"$\",\"span\",\"general\",{\"className\":\"tag\",\"children\":\"general\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl font-bold text-gray-900 mb-6 leading-tight\",\"children\":\"Microservices: Outbox Pattern\"}],[\"$\",\"p\",null,{\"className\":\"text-xl text-gray-600 leading-relaxed\",\"children\":\"Learn about Microservices: Outbox Pattern.\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center space-x-6 text-gray-500\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-2\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-user w-5 h-5\",\"children\":[[\"$\",\"path\",\"975kel\",{\"d\":\"M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2\"}],[\"$\",\"circle\",\"17ys0d\",{\"cx\":\"12\",\"cy\":\"7\",\"r\":\"4\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"font-medium\",\"children\":\"Abstract Algorithms\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-2\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-calendar w-5 h-5\",\"children\":[[\"$\",\"rect\",\"eu3xkr\",{\"width\":\"18\",\"height\":\"18\",\"x\":\"3\",\"y\":\"4\",\"rx\":\"2\",\"ry\":\"2\"}],[\"$\",\"line\",\"m3sa8f\",{\"x1\":\"16\",\"x2\":\"16\",\"y1\":\"2\",\"y2\":\"6\"}],[\"$\",\"line\",\"18kwsl\",{\"x1\":\"8\",\"x2\":\"8\",\"y1\":\"2\",\"y2\":\"6\"}],[\"$\",\"line\",\"xt86sb\",{\"x1\":\"3\",\"x2\":\"21\",\"y1\":\"10\",\"y2\":\"10\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"over 2 years ago\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-2\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-clock w-5 h-5\",\"children\":[[\"$\",\"circle\",\"1mglay\",{\"cx\":\"12\",\"cy\":\"12\",\"r\":\"10\"}],[\"$\",\"polyline\",\"68esgv\",{\"points\":\"12 6 12 12 16 14\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"1 min read\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center space-x-4 mt-8\",\"children\":[[\"$\",\"button\",null,{\"className\":\"flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-share2 w-4 h-4\",\"children\":[[\"$\",\"circle\",\"gq8acd\",{\"cx\":\"18\",\"cy\":\"5\",\"r\":\"3\"}],[\"$\",\"circle\",\"w7nqdw\",{\"cx\":\"6\",\"cy\":\"12\",\"r\":\"3\"}],[\"$\",\"circle\",\"1xt0gg\",{\"cx\":\"18\",\"cy\":\"19\",\"r\":\"3\"}],[\"$\",\"line\",\"47mynk\",{\"x1\":\"8.59\",\"x2\":\"15.42\",\"y1\":\"13.51\",\"y2\":\"17.49\"}],[\"$\",\"line\",\"1n3mei\",{\"x1\":\"15.41\",\"x2\":\"8.59\",\"y1\":\"6.51\",\"y2\":\"10.49\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"Share\"}]]}],[\"$\",\"button\",null,{\"className\":\"flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-bookmark w-4 h-4\",\"children\":[[\"$\",\"path\",\"1fy3hk\",{\"d\":\"m19 21-7-4-7 4V5a2 2 0 0 1 2-2h10a2 2 0 0 1 2 2v16z\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"Save\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto mt-12\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative aspect-video rounded-xl overflow-hidden shadow-lg\",\"children\":[\"$\",\"$L11\",null,{\"src\":\"/posts/microservices-outbox-pattern/assets/overview.png\",\"alt\":\"Microservices: Outbox Pattern\",\"fill\":true,\"className\":\"object-cover\",\"priority\":true}]}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"py-8\",\"children\":[[\"$\",\"$L12\",null,{\"slug\":\"microservices-outbox-pattern\"}],[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-6 mt-12\",\"children\":[\"$\",\"div\",null,{\"className\":\"bg-white rounded-xl shadow-sm border border-gray-200 p-8\",\"children\":[\"$\",\"$L13\",null,{}]}]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-white border-t border-gray-200\",\"children\":[\"$\",\"$L14\",null,{\"posts\":[{\"slug\":\"little's-law\",\"title\":\"Little's Law: Understanding Queue Performance\",\"date\":\"2024-03-05\",\"excerpt\":\"Understanding Little's Law and its applications in system performance analysis\",\"content\":\"$15\",\"author\":\"Abstract Algorithms\",\"tags\":[\"queueing-theory\",\"performance\",\"system-design\",\"mathematics\"],\"readingTime\":\"1 min read\",\"coverImage\":\"/posts/little's-law/assets/overview.png\",\"fixedUrl\":\"$undefined\",\"series\":\"$undefined\"},{\"slug\":\"llm-engineering-mastery-part-3\",\"title\":\"LLM Engineering Mastery: Part 3 - Production Deployment and Scaling\",\"date\":\"2024-02-10\",\"excerpt\":\"Part 3 of the LLM Engineering Mastery series: Master production deployment, scaling strategies, monitoring, and security for enterprise-grade LLM applications.\",\"content\":\"$16\",\"author\":\"Abstract Algorithms\",\"tags\":[\"llm\",\"production\",\"deployment\",\"scaling\",\"monitoring\",\"security\"],\"readingTime\":\"19 min read\",\"coverImage\":\"/posts/llm-engineering-mastery-part-3/assets/production-deployment-cover.png\",\"fixedUrl\":\"$undefined\",\"series\":{\"name\":\"LLM Engineering Mastery\",\"order\":3,\"total\":3,\"prev\":\"/posts/llm-engineering-mastery-part-2/\",\"next\":null}},{\"slug\":\"llm-engineering-mastery-part-2\",\"title\":\"LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems\",\"date\":\"2024-02-03\",\"excerpt\":\"Part 2 of the LLM Engineering Mastery series: Master advanced prompt engineering techniques and build production-ready RAG systems for enhanced LLM applications.\",\"content\":\"$17\",\"author\":\"Abstract Algorithms\",\"tags\":[\"llm\",\"prompt-engineering\",\"rag\",\"vector-databases\",\"retrieval\"],\"readingTime\":\"16 min read\",\"coverImage\":\"/posts/llm-engineering-mastery-part-2/assets/prompt-engineering-rag-cover.png\",\"fixedUrl\":\"$undefined\",\"series\":{\"name\":\"LLM Engineering Mastery\",\"order\":2,\"total\":3,\"prev\":\"/posts/llm-engineering-mastery-part-1/\",\"next\":\"/posts/llm-engineering-mastery-part-3/\"}},{\"slug\":\"llm-engineering-mastery-part-1\",\"title\":\"LLM Engineering Mastery: Part 1 - Understanding and Leveraging Foundation Models\",\"date\":\"2024-01-27\",\"excerpt\":\"Part 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.\",\"content\":\"$18\",\"author\":\"Abstract Algorithms\",\"tags\":[\"llm\",\"genai\",\"engineering\",\"foundation-models\",\"practical-ai\"],\"readingTime\":\"12 min read\",\"coverImage\":\"/posts/llm-engineering-mastery-part-1/assets/foundation-models-cover.png\",\"fixedUrl\":\"$undefined\",\"series\":{\"name\":\"LLM Engineering Mastery\",\"order\":1,\"total\":3,\"prev\":null,\"next\":\"/posts/llm-engineering-mastery-part-2/\"}},{\"slug\":\"llm-engineering-mastery-series\",\"title\":\"LLM Engineering Mastery - Complete Series\",\"date\":\"2024-01-27\",\"excerpt\":\"Complete LLM Engineering Mastery series with 3 parts covering Part 1 of the LLM Engineering Mastery series: Master foundation models from an engineering perspective - understanding capabilities, limitations, and practical integration strategies.\",\"content\":\"$19\",\"author\":\"Abstract Algorithms\",\"tags\":[\"llm\",\"genai\",\"engineering\",\"foundation-models\",\"practical-ai\"],\"readingTime\":\"1 min read\",\"coverImage\":\"/posts/llm-engineering-mastery-series/assets/series-overview.png\",\"fixedUrl\":\"$undefined\",\"series\":{\"name\":\"LLM Engineering Mastery\",\"order\":\"$undefined\",\"total\":3,\"prev\":null,\"next\":null}},{\"slug\":\"understanding-hash-tables-ultimate-guide\",\"title\":\"Understanding Hash Tables: The Ultimate Guide\",\"date\":\"2024-01-15\",\"excerpt\":\"A comprehensive guide to hash tables, covering implementation details, collision resolution strategies, and performance analysis with practical examples.\",\"content\":\"$1a\",\"author\":\"Abstract Algorithms\",\"tags\":[\"data-structures\",\"algorithms\",\"hash-tables\",\"performance\"],\"readingTime\":\"5 min read\",\"coverImage\":\"/posts/understanding-hash-tables-ultimate-guide/assets/overview.png\",\"fixedUrl\":\"$undefined\",\"series\":\"$undefined\"}]}]}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"Microservices: Outbox Pattern\\\",\\\"description\\\":\\\"Learn about Microservices: Outbox Pattern.\\\",\\\"datePublished\\\":\\\"2022-11-21 23:02:39 +0530\\\",\\\"dateModified\\\":\\\"2022-11-21 23:02:39 +0530\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Abstract Algorithms\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"Abstract Algorithms\\\",\\\"url\\\":\\\"https://abstractalgorithms.github.io\\\"},\\\"url\\\":\\\"https://abstractalgorithms.github.io/posts/microservices-outbox-pattern\\\",\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://abstractalgorithms.github.io/posts/microservices-outbox-pattern\\\"},\\\"image\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://abstractalgorithms.github.io/posts/microservices-outbox-pattern/assets/overview.png\\\"}}\"}}]]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Microservices: Outbox Pattern | Abstract Algorithms\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Learn about Microservices: Outbox Pattern.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"Abstract Algorithms\"}],[\"$\",\"meta\",\"5\",{\"name\":\"keywords\",\"content\":\"algorithms,data structures,system design,software engineering,programming,computer science,performance optimization,big o notation,hash tables,database indexing\"}],[\"$\",\"meta\",\"6\",{\"name\":\"creator\",\"content\":\"Abstract Algorithms\"}],[\"$\",\"meta\",\"7\",{\"name\":\"publisher\",\"content\":\"Abstract Algorithms\"}],[\"$\",\"meta\",\"8\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"9\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:title\",\"content\":\"Microservices: Outbox Pattern\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:description\",\"content\":\"Learn about Microservices: Outbox Pattern.\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"13\",{\"property\":\"article:published_time\",\"content\":\"2022-11-21 23:02:39 +0530\"}],[\"$\",\"meta\",\"14\",{\"property\":\"article:author\",\"content\":\"Abstract Algorithms\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:title\",\"content\":\"Abstract Algorithms\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:description\",\"content\":\"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices\"}],[\"$\",\"link\",\"18\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",\"19\",{\"rel\":\"icon\",\"href\":\"/icon.svg\",\"type\":\"image/svg+xml\",\"sizes\":\"32x32\"}],[\"$\",\"link\",\"20\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.svg\",\"type\":\"image/svg+xml\",\"sizes\":\"180x180\"}],[\"$\",\"meta\",\"21\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>