3:I[4707,[],""]
5:I[6423,[],""]
6:I[981,["8592","static/chunks/common-1942b2e5063f4af5.js","3185","static/chunks/app/layout-f803094fc502a10d.js"],"AuthProvider"]
7:I[8931,["8592","static/chunks/common-1942b2e5063f4af5.js","3185","static/chunks/app/layout-f803094fc502a10d.js"],"default"]
8:I[917,["7601","static/chunks/app/error-1745ca505ccb7f84.js"],"default"]
9:I[5618,["9160","static/chunks/app/not-found-5aff7e7753541a4f.js"],"default"]
4:["slug","database-indexes-guide","d"]
0:["Ph-JG0SHzMEpsRc4oi7jl",[[["",{"children":["posts",{"children":[["slug","database-indexes-guide","d"],{"children":["__PAGE__?{\"slug\":\"database-indexes-guide\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["posts",{"children":[["slug","database-indexes-guide","d"],{"children":["__PAGE__",{},[["$L1","$L2",null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/275ed64cc4367444.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/f2c5f2458408eb15.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"Abstract Algorithms\",\"description\":\"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices\",\"url\":\"https://abstractalgorithms.github.io\",\"potentialAction\":{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https://abstractalgorithms.github.io/posts/{search_term_string}\"},\"query-input\":\"required name=search_term_string\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Abstract Algorithms\",\"url\":\"https://abstractalgorithms.github.io\"}}"}}],["$","link",null,{"rel":"manifest","href":"/manifest.json"}],["$","meta",null,{"name":"theme-color","content":"#00D885"}],["$","meta",null,{"name":"google-site-verification","content":"D5v1M3nD8oO9DNaZKujCwBLNNqf35CTJo114uv8yMNU"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-VZR168MHE2"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n            window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-VZR168MHE2');\n          "}}]]}],["$","body",null,{"className":"__className_e8ce0c","children":["$","$L6",null,{"children":["$","$L7",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$8","errorStyles":[],"errorScripts":[],"template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","$L9",null,{}],"notFoundStyles":[]}]}]}]}]]}]],null],null],["$La",null]]]]
b:I[4457,["8592","static/chunks/common-1942b2e5063f4af5.js","333","static/chunks/app/posts/%5Bslug%5D/page-3890dd086ea4f2ea.js"],"default"]
15:I[9798,["8592","static/chunks/common-1942b2e5063f4af5.js","333","static/chunks/app/posts/%5Bslug%5D/page-3890dd086ea4f2ea.js"],"default"]
c:T24c0,<h2>What is a Database Index?</h2>
<p>A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional space and maintenance overhead. Think of it like an index in a book: it helps you find information quickly without scanning every page.</p>
<pre><code class="language-sql">-- Without index: Full table scan O(n)
SELECT * FROM users WHERE email = 'john@example.com';

-- With index on email: Tree traversal O(log n)
CREATE INDEX idx_users_email ON users(email);
SELECT * FROM users WHERE email = 'john@example.com';
</code></pre>
<h2>How Indexes Work Internally</h2>
<h3>The Problem: Linear Search</h3>
<p>Without indexes, databases perform <strong>full table scans</strong>:</p>
<ul>
<li>Read every row sequentially</li>
<li>Check if the row matches the condition</li>
<li>Time complexity: O(n) where n is the number of rows</li>
</ul>
<h3>The Solution: Tree Structures</h3>
<p>Indexes create <strong>sorted tree structures</strong>:</p>
<ul>
<li>Maintain sorted order of key values</li>
<li>Use binary search principles</li>
<li>Time complexity: O(log n) for lookups</li>
</ul>
<h2>Core Index Types</h2>
<h3>1. B-Tree Indexes (Most Common)</h3>
<p><strong>Structure:</strong></p>
<ul>
<li>Balanced tree with multiple keys per node</li>
<li>Leaf nodes contain actual data pointers</li>
<li>All leaf nodes are at the same level</li>
</ul>
<p><strong>Best For:</strong></p>
<ul>
<li>Range queries (<code>WHERE age BETWEEN 25 AND 35</code>)</li>
<li>Sorting operations (<code>ORDER BY</code>)</li>
<li>Equality searches (<code>WHERE id = 123</code>)</li>
</ul>
<p><strong>Database Support:</strong></p>
<ul>
<li>MySQL (InnoDB): Primary index type</li>
<li>PostgreSQL: Default for most data types</li>
<li>SQL Server: Clustered and non-clustered indexes</li>
<li>Oracle: Standard B-Tree indexes</li>
</ul>
<pre><code class="language-sql">-- B-Tree index example
CREATE INDEX idx_users_age ON users(age);

-- Efficient queries:
SELECT * FROM users WHERE age = 30;          -- Equality
SELECT * FROM users WHERE age > 25;          -- Range
SELECT * FROM users WHERE age BETWEEN 20 AND 40; -- Range
SELECT * FROM users ORDER BY age;            -- Sorting
</code></pre>
<h3>2. Hash Indexes</h3>
<p><strong>Structure:</strong></p>
<ul>
<li>Uses hash function to map keys to buckets</li>
<li>Direct access to data location</li>
<li>No ordering maintained</li>
</ul>
<p><strong>Best For:</strong></p>
<ul>
<li>Exact equality searches only</li>
<li>High-frequency lookups</li>
<li>Memory-based operations</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>No range queries</li>
<li>No sorting support</li>
<li>Hash collisions can degrade performance</li>
</ul>
<pre><code class="language-sql">-- Hash index (MySQL Memory engine)
CREATE TABLE user_sessions (
    session_id VARCHAR(64) PRIMARY KEY,
    user_id INT,
    data TEXT
) ENGINE=MEMORY;

-- Perfect for:
SELECT * FROM user_sessions WHERE session_id = 'abc123def456';
-- NOT suitable for:
SELECT * FROM user_sessions WHERE session_id LIKE 'abc%';
</code></pre>
<h3>3. Bitmap Indexes</h3>
<p><strong>Structure:</strong></p>
<ul>
<li>Uses bitmaps (bit vectors) for each distinct value</li>
<li>Each bit represents whether a row contains the value</li>
<li>Highly compressed for low-cardinality data</li>
</ul>
<p><strong>Best For:</strong></p>
<ul>
<li>Data warehousing</li>
<li>Columns with few distinct values (gender, status, category)</li>
<li>Complex analytical queries with multiple conditions</li>
</ul>
<p><strong>Database Support:</strong></p>
<ul>
<li>Oracle: Full bitmap index support</li>
<li>PostgreSQL: Partial support via extensions</li>
<li>Not available in MySQL or SQL Server</li>
</ul>
<pre><code class="language-sql">-- Bitmap index example (Oracle)
CREATE BITMAP INDEX idx_employee_gender ON employees(gender);
CREATE BITMAP INDEX idx_employee_status ON employees(status);

-- Efficient for analytical queries:
SELECT COUNT(*) 
FROM employees 
WHERE gender = 'F' 
  AND status = 'ACTIVE' 
  AND department = 'ENGINEERING';
</code></pre>
<h3>4. Specialized Index Types</h3>
<h4>Full-Text Indexes</h4>
<p>For searching within text content:</p>
<pre><code class="language-sql">-- MySQL Full-Text Index
CREATE FULLTEXT INDEX idx_articles_content ON articles(title, content);
SELECT * FROM articles WHERE MATCH(title, content) AGAINST('database optimization');

-- PostgreSQL GIN Index for text search
CREATE INDEX idx_articles_content ON articles USING gin(to_tsvector('english', content));
SELECT * FROM articles WHERE to_tsvector('english', content) @@ to_tsquery('database &#x26; optimization');
</code></pre>
<h4>Spatial Indexes</h4>
<p>For geographic and geometric data:</p>
<pre><code class="language-sql">-- PostGIS Spatial Index
CREATE INDEX idx_locations_geom ON locations USING gist(geom);
SELECT * FROM locations WHERE ST_DWithin(geom, ST_Point(-122.4194, 37.7749), 1000);
</code></pre>
<h2>Index Storage and Structure</h2>
<h3>Clustered vs Non-Clustered Indexes</h3>
<h4>Clustered Index</h4>
<ul>
<li><strong>Physical ordering</strong>: Data rows are stored in the same order as the index</li>
<li><strong>One per table</strong>: Only one clustered index possible</li>
<li><strong>Direct data access</strong>: Index leaf nodes contain actual data rows</li>
</ul>
<pre><code class="language-sql">-- SQL Server clustered index
CREATE CLUSTERED INDEX idx_orders_date ON orders(order_date);
-- Data rows are physically ordered by order_date
</code></pre>
<h4>Non-Clustered Index</h4>
<ul>
<li><strong>Logical ordering</strong>: Index is separate from data storage</li>
<li><strong>Multiple allowed</strong>: Can have many non-clustered indexes</li>
<li><strong>Pointer to data</strong>: Index points to the actual data location</li>
</ul>
<pre><code class="language-sql">-- Non-clustered index
CREATE NONCLUSTERED INDEX idx_customers_email ON customers(email);
-- Index structure points to data rows
</code></pre>
<h3>Index Pages and Storage</h3>
<pre><code>B-Tree Structure:
                [Root Page]
               /           \
         [Internal Page]  [Internal Page]
         /      |     \   /      |      \
    [Leaf]  [Leaf]  [Leaf] [Leaf] [Leaf] [Leaf]
      |       |       |     |       |      |
   [Data]  [Data]  [Data] [Data]  [Data] [Data]
</code></pre>
<h2>Performance Characteristics</h2>
<h3>Index Benefits</h3>
<ul>
<li><strong>Faster SELECT queries</strong>: O(log n) vs O(n)</li>
<li><strong>Efficient sorting</strong>: ORDER BY uses index order</li>
<li><strong>Quick joins</strong>: JOIN operations use indexes</li>
<li><strong>Unique constraints</strong>: Prevent duplicate values</li>
</ul>
<h3>Index Costs</h3>
<ul>
<li><strong>Storage overhead</strong>: Additional disk space (20-30% typical)</li>
<li><strong>Write performance</strong>: INSERT/UPDATE/DELETE slower</li>
<li><strong>Maintenance overhead</strong>: Index must be updated with data changes</li>
<li><strong>Memory usage</strong>: Indexes consume buffer pool memory</li>
</ul>
<h2>When to Use Each Index Type</h2>
<h3>Use B-Tree When:</h3>
<ul>
<li>Range queries are common</li>
<li>Sorting is frequently needed</li>
<li>General-purpose OLTP applications</li>
<li>High selectivity columns</li>
</ul>
<h3>Use Hash When:</h3>
<ul>
<li>Only equality searches needed</li>
<li>High-frequency exact lookups</li>
<li>Memory-based tables</li>
<li>Session or cache tables</li>
</ul>
<h3>Use Bitmap When:</h3>
<ul>
<li>Data warehousing scenarios</li>
<li>Low-cardinality columns</li>
<li>Complex analytical queries</li>
<li>Read-heavy workloads</li>
</ul>
<h2>Next in This Series</h2>
<p>In the upcoming parts, we'll dive deeper into:</p>
<ul>
<li><strong>Part 2</strong>: SQL Database Indexing Strategies (MySQL, PostgreSQL, SQL Server)</li>
<li><strong>Part 3</strong>: NoSQL Database Indexing (MongoDB, Cassandra, Redis)</li>
<li><strong>Part 4</strong>: Composite Indexes and Query Optimization</li>
<li><strong>Part 5</strong>: Index Performance Monitoring and Maintenance</li>
<li><strong>Part 6</strong>: Advanced Indexing Techniques and Partitioning</li>
<li><strong>Part 7</strong>: Client-Side Optimization and Caching Strategies</li>
<li><strong>Part 8</strong>: Real-World Case Studies and Best Practices</li>
<li><strong>Query Patterns:</strong> Design indexes based on how data is accessed (e.g., filter, sort, join columns).</li>
<li><strong>Index Fragmentation:</strong> Over time, indexes can become fragmented and less efficient; periodic maintenance may be needed.</li>
</ul>
<h2>What Causes Bad Query Performance?</h2>
<ul>
<li><strong>Missing Indexes:</strong> Full table scans for every query.</li>
<li><strong>Unselective Indexes:</strong> Indexes on columns with many repeated values (low cardinality) are less useful.</li>
<li><strong>Too Many Indexes:</strong> Increases write cost and can confuse the query planner.</li>
<li><strong>Outdated Statistics:</strong> The database optimizer relies on statistics to choose indexes; stale stats can lead to poor plans.</li>
<li><strong>Improper Query Design:</strong> Functions or operations on indexed columns can prevent index usage.</li>
</ul>
<hr>
<p><em>Indexes are a powerful tool for optimizing database performance. By understanding how they work and when to use them, you can significantly improve your application's data retrieval speed.</em></p>
d:T24c0,<h2>What is a Database Index?</h2>
<p>A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional space and maintenance overhead. Think of it like an index in a book: it helps you find information quickly without scanning every page.</p>
<pre><code class="language-sql">-- Without index: Full table scan O(n)
SELECT * FROM users WHERE email = 'john@example.com';

-- With index on email: Tree traversal O(log n)
CREATE INDEX idx_users_email ON users(email);
SELECT * FROM users WHERE email = 'john@example.com';
</code></pre>
<h2>How Indexes Work Internally</h2>
<h3>The Problem: Linear Search</h3>
<p>Without indexes, databases perform <strong>full table scans</strong>:</p>
<ul>
<li>Read every row sequentially</li>
<li>Check if the row matches the condition</li>
<li>Time complexity: O(n) where n is the number of rows</li>
</ul>
<h3>The Solution: Tree Structures</h3>
<p>Indexes create <strong>sorted tree structures</strong>:</p>
<ul>
<li>Maintain sorted order of key values</li>
<li>Use binary search principles</li>
<li>Time complexity: O(log n) for lookups</li>
</ul>
<h2>Core Index Types</h2>
<h3>1. B-Tree Indexes (Most Common)</h3>
<p><strong>Structure:</strong></p>
<ul>
<li>Balanced tree with multiple keys per node</li>
<li>Leaf nodes contain actual data pointers</li>
<li>All leaf nodes are at the same level</li>
</ul>
<p><strong>Best For:</strong></p>
<ul>
<li>Range queries (<code>WHERE age BETWEEN 25 AND 35</code>)</li>
<li>Sorting operations (<code>ORDER BY</code>)</li>
<li>Equality searches (<code>WHERE id = 123</code>)</li>
</ul>
<p><strong>Database Support:</strong></p>
<ul>
<li>MySQL (InnoDB): Primary index type</li>
<li>PostgreSQL: Default for most data types</li>
<li>SQL Server: Clustered and non-clustered indexes</li>
<li>Oracle: Standard B-Tree indexes</li>
</ul>
<pre><code class="language-sql">-- B-Tree index example
CREATE INDEX idx_users_age ON users(age);

-- Efficient queries:
SELECT * FROM users WHERE age = 30;          -- Equality
SELECT * FROM users WHERE age > 25;          -- Range
SELECT * FROM users WHERE age BETWEEN 20 AND 40; -- Range
SELECT * FROM users ORDER BY age;            -- Sorting
</code></pre>
<h3>2. Hash Indexes</h3>
<p><strong>Structure:</strong></p>
<ul>
<li>Uses hash function to map keys to buckets</li>
<li>Direct access to data location</li>
<li>No ordering maintained</li>
</ul>
<p><strong>Best For:</strong></p>
<ul>
<li>Exact equality searches only</li>
<li>High-frequency lookups</li>
<li>Memory-based operations</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>No range queries</li>
<li>No sorting support</li>
<li>Hash collisions can degrade performance</li>
</ul>
<pre><code class="language-sql">-- Hash index (MySQL Memory engine)
CREATE TABLE user_sessions (
    session_id VARCHAR(64) PRIMARY KEY,
    user_id INT,
    data TEXT
) ENGINE=MEMORY;

-- Perfect for:
SELECT * FROM user_sessions WHERE session_id = 'abc123def456';
-- NOT suitable for:
SELECT * FROM user_sessions WHERE session_id LIKE 'abc%';
</code></pre>
<h3>3. Bitmap Indexes</h3>
<p><strong>Structure:</strong></p>
<ul>
<li>Uses bitmaps (bit vectors) for each distinct value</li>
<li>Each bit represents whether a row contains the value</li>
<li>Highly compressed for low-cardinality data</li>
</ul>
<p><strong>Best For:</strong></p>
<ul>
<li>Data warehousing</li>
<li>Columns with few distinct values (gender, status, category)</li>
<li>Complex analytical queries with multiple conditions</li>
</ul>
<p><strong>Database Support:</strong></p>
<ul>
<li>Oracle: Full bitmap index support</li>
<li>PostgreSQL: Partial support via extensions</li>
<li>Not available in MySQL or SQL Server</li>
</ul>
<pre><code class="language-sql">-- Bitmap index example (Oracle)
CREATE BITMAP INDEX idx_employee_gender ON employees(gender);
CREATE BITMAP INDEX idx_employee_status ON employees(status);

-- Efficient for analytical queries:
SELECT COUNT(*) 
FROM employees 
WHERE gender = 'F' 
  AND status = 'ACTIVE' 
  AND department = 'ENGINEERING';
</code></pre>
<h3>4. Specialized Index Types</h3>
<h4>Full-Text Indexes</h4>
<p>For searching within text content:</p>
<pre><code class="language-sql">-- MySQL Full-Text Index
CREATE FULLTEXT INDEX idx_articles_content ON articles(title, content);
SELECT * FROM articles WHERE MATCH(title, content) AGAINST('database optimization');

-- PostgreSQL GIN Index for text search
CREATE INDEX idx_articles_content ON articles USING gin(to_tsvector('english', content));
SELECT * FROM articles WHERE to_tsvector('english', content) @@ to_tsquery('database &#x26; optimization');
</code></pre>
<h4>Spatial Indexes</h4>
<p>For geographic and geometric data:</p>
<pre><code class="language-sql">-- PostGIS Spatial Index
CREATE INDEX idx_locations_geom ON locations USING gist(geom);
SELECT * FROM locations WHERE ST_DWithin(geom, ST_Point(-122.4194, 37.7749), 1000);
</code></pre>
<h2>Index Storage and Structure</h2>
<h3>Clustered vs Non-Clustered Indexes</h3>
<h4>Clustered Index</h4>
<ul>
<li><strong>Physical ordering</strong>: Data rows are stored in the same order as the index</li>
<li><strong>One per table</strong>: Only one clustered index possible</li>
<li><strong>Direct data access</strong>: Index leaf nodes contain actual data rows</li>
</ul>
<pre><code class="language-sql">-- SQL Server clustered index
CREATE CLUSTERED INDEX idx_orders_date ON orders(order_date);
-- Data rows are physically ordered by order_date
</code></pre>
<h4>Non-Clustered Index</h4>
<ul>
<li><strong>Logical ordering</strong>: Index is separate from data storage</li>
<li><strong>Multiple allowed</strong>: Can have many non-clustered indexes</li>
<li><strong>Pointer to data</strong>: Index points to the actual data location</li>
</ul>
<pre><code class="language-sql">-- Non-clustered index
CREATE NONCLUSTERED INDEX idx_customers_email ON customers(email);
-- Index structure points to data rows
</code></pre>
<h3>Index Pages and Storage</h3>
<pre><code>B-Tree Structure:
                [Root Page]
               /           \
         [Internal Page]  [Internal Page]
         /      |     \   /      |      \
    [Leaf]  [Leaf]  [Leaf] [Leaf] [Leaf] [Leaf]
      |       |       |     |       |      |
   [Data]  [Data]  [Data] [Data]  [Data] [Data]
</code></pre>
<h2>Performance Characteristics</h2>
<h3>Index Benefits</h3>
<ul>
<li><strong>Faster SELECT queries</strong>: O(log n) vs O(n)</li>
<li><strong>Efficient sorting</strong>: ORDER BY uses index order</li>
<li><strong>Quick joins</strong>: JOIN operations use indexes</li>
<li><strong>Unique constraints</strong>: Prevent duplicate values</li>
</ul>
<h3>Index Costs</h3>
<ul>
<li><strong>Storage overhead</strong>: Additional disk space (20-30% typical)</li>
<li><strong>Write performance</strong>: INSERT/UPDATE/DELETE slower</li>
<li><strong>Maintenance overhead</strong>: Index must be updated with data changes</li>
<li><strong>Memory usage</strong>: Indexes consume buffer pool memory</li>
</ul>
<h2>When to Use Each Index Type</h2>
<h3>Use B-Tree When:</h3>
<ul>
<li>Range queries are common</li>
<li>Sorting is frequently needed</li>
<li>General-purpose OLTP applications</li>
<li>High selectivity columns</li>
</ul>
<h3>Use Hash When:</h3>
<ul>
<li>Only equality searches needed</li>
<li>High-frequency exact lookups</li>
<li>Memory-based tables</li>
<li>Session or cache tables</li>
</ul>
<h3>Use Bitmap When:</h3>
<ul>
<li>Data warehousing scenarios</li>
<li>Low-cardinality columns</li>
<li>Complex analytical queries</li>
<li>Read-heavy workloads</li>
</ul>
<h2>Next in This Series</h2>
<p>In the upcoming parts, we'll dive deeper into:</p>
<ul>
<li><strong>Part 2</strong>: SQL Database Indexing Strategies (MySQL, PostgreSQL, SQL Server)</li>
<li><strong>Part 3</strong>: NoSQL Database Indexing (MongoDB, Cassandra, Redis)</li>
<li><strong>Part 4</strong>: Composite Indexes and Query Optimization</li>
<li><strong>Part 5</strong>: Index Performance Monitoring and Maintenance</li>
<li><strong>Part 6</strong>: Advanced Indexing Techniques and Partitioning</li>
<li><strong>Part 7</strong>: Client-Side Optimization and Caching Strategies</li>
<li><strong>Part 8</strong>: Real-World Case Studies and Best Practices</li>
<li><strong>Query Patterns:</strong> Design indexes based on how data is accessed (e.g., filter, sort, join columns).</li>
<li><strong>Index Fragmentation:</strong> Over time, indexes can become fragmented and less efficient; periodic maintenance may be needed.</li>
</ul>
<h2>What Causes Bad Query Performance?</h2>
<ul>
<li><strong>Missing Indexes:</strong> Full table scans for every query.</li>
<li><strong>Unselective Indexes:</strong> Indexes on columns with many repeated values (low cardinality) are less useful.</li>
<li><strong>Too Many Indexes:</strong> Increases write cost and can confuse the query planner.</li>
<li><strong>Outdated Statistics:</strong> The database optimizer relies on statistics to choose indexes; stale stats can lead to poor plans.</li>
<li><strong>Improper Query Design:</strong> Functions or operations on indexed columns can prevent index usage.</li>
</ul>
<hr>
<p><em>Indexes are a powerful tool for optimizing database performance. By understanding how they work and when to use them, you can significantly improve your application's data retrieval speed.</em></p>
e:T2d9a,<h2>MySQL Indexing Deep Dive</h2>
<h3>InnoDB Storage Engine</h3>
<p>MySQL's InnoDB engine uses clustered indexes by default:</p>
<pre><code class="language-sql">-- Primary key automatically becomes clustered index
CREATE TABLE users (
    id INT AUTO_INCREMENT PRIMARY KEY,  -- Clustered index
    email VARCHAR(255) UNIQUE,          -- Secondary index
    name VARCHAR(100),
    age INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_email (email),            -- Explicit secondary index
    INDEX idx_age_name (age, name)      -- Composite index
);
</code></pre>
<h3>MySQL Index Types and Syntax</h3>
<h4>Single Column Indexes</h4>
<pre><code class="language-sql">-- Create index during table creation
CREATE TABLE products (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    price DECIMAL(10,2),
    category_id INT,
    INDEX idx_price (price),
    INDEX idx_category (category_id)
);

-- Add index to existing table
ALTER TABLE products ADD INDEX idx_name (name);
CREATE INDEX idx_name_price ON products(name, price);
</code></pre>
<h4>Composite Indexes (Multiple Columns)</h4>
<pre><code class="language-sql">-- Order matters! This index can efficiently handle:
-- 1. WHERE category_id = ?
-- 2. WHERE category_id = ? AND price > ?
-- 3. WHERE category_id = ? AND price > ? AND name LIKE ?
CREATE INDEX idx_category_price_name ON products(category_id, price, name);

-- This won't efficiently use the above index:
SELECT * FROM products WHERE price > 100;  -- Missing category_id prefix
</code></pre>
<h4>Partial Indexes (Prefix Indexes)</h4>
<pre><code class="language-sql">-- Index only first 10 characters of name (saves space)
CREATE INDEX idx_name_prefix ON products(name(10));

-- Good for columns with long text values
CREATE INDEX idx_description_prefix ON articles(description(50));
</code></pre>
<h3>MySQL Index Optimization</h3>
<h4>Using EXPLAIN to Analyze Queries</h4>
<pre><code class="language-sql">-- Analyze query execution plan
EXPLAIN SELECT * FROM users WHERE age > 25 AND name LIKE 'John%';

-- Extended explain with more details
EXPLAIN FORMAT=JSON SELECT * FROM users WHERE email = 'john@example.com';
</code></pre>
<h4>Index Hints</h4>
<pre><code class="language-sql">-- Force MySQL to use a specific index
SELECT * FROM users USE INDEX (idx_age_name) WHERE age > 25;

-- Suggest an index (MySQL may ignore)
SELECT * FROM users USE INDEX (idx_age) WHERE age > 25 AND name LIKE 'J%';

-- Force MySQL to ignore an index
SELECT * FROM users IGNORE INDEX (idx_age) WHERE age > 25;
</code></pre>
<h2>PostgreSQL Advanced Indexing</h2>
<h3>PostgreSQL Index Types</h3>
<h4>GiST (Generalized Search Tree)</h4>
<pre><code class="language-sql">-- Excellent for full-text search and geometric data
CREATE INDEX idx_articles_content ON articles USING gist(to_tsvector('english', content));

-- Range types and arrays
CREATE INDEX idx_price_ranges ON products USING gist(price_range);
</code></pre>
<h4>GIN (Generalized Inverted Index)</h4>
<pre><code class="language-sql">-- Perfect for JSONB, arrays, and full-text search
CREATE INDEX idx_user_tags ON users USING gin(tags);  -- For array columns
CREATE INDEX idx_user_metadata ON users USING gin(metadata);  -- For JSONB

-- Full-text search
CREATE INDEX idx_articles_search ON articles USING gin(to_tsvector('english', title || ' ' || content));
</code></pre>
<h4>BRIN (Block Range Index)</h4>
<pre><code class="language-sql">-- Efficient for large tables with naturally ordered data
CREATE INDEX idx_orders_date ON orders USING brin(order_date);

-- Great for time-series data with minimal storage overhead
CREATE INDEX idx_logs_timestamp ON application_logs USING brin(created_at);
</code></pre>
<h3>PostgreSQL Partial Indexes</h3>
<pre><code class="language-sql">-- Index only active users (saves space and improves performance)
CREATE INDEX idx_active_users_email ON users(email) WHERE status = 'active';

-- Index only recent orders
CREATE INDEX idx_recent_orders ON orders(customer_id) 
WHERE order_date >= '2024-01-01';

-- Index only non-null values
CREATE INDEX idx_users_phone ON users(phone) WHERE phone IS NOT NULL;
</code></pre>
<h3>PostgreSQL Expression Indexes</h3>
<pre><code class="language-sql">-- Index on computed values
CREATE INDEX idx_users_lower_email ON users(lower(email));
CREATE INDEX idx_products_discounted_price ON products((price * 0.9)) WHERE on_sale = true;

-- Functional index for complex queries
CREATE INDEX idx_user_full_name ON users((first_name || ' ' || last_name));
</code></pre>
<h2>SQL Server Indexing Strategies</h2>
<h3>Clustered vs Non-Clustered Indexes</h3>
<h4>Clustered Index Management</h4>
<pre><code class="language-sql">-- Create clustered index (only one per table)
CREATE CLUSTERED INDEX idx_orders_date ON orders(order_date);

-- Drop and recreate clustered index
DROP INDEX idx_orders_date ON orders;
CREATE CLUSTERED INDEX idx_orders_customer_date ON orders(customer_id, order_date);
</code></pre>
<h4>Non-Clustered Indexes with Included Columns</h4>
<pre><code class="language-sql">-- Include additional columns at leaf level (covering index)
CREATE NONCLUSTERED INDEX idx_users_email_covering 
ON users(email) 
INCLUDE (first_name, last_name, phone);

-- This query uses index-only scan (no key lookup needed)
SELECT first_name, last_name, phone FROM users WHERE email = 'john@example.com';
</code></pre>
<h3>SQL Server Index Features</h3>
<h4>Filtered Indexes</h4>
<pre><code class="language-sql">-- Index only specific subset of data
CREATE NONCLUSTERED INDEX idx_active_users 
ON users(last_login_date) 
WHERE status = 'active' AND last_login_date IS NOT NULL;
</code></pre>
<h4>Columnstore Indexes</h4>
<pre><code class="language-sql">-- For analytical workloads (OLAP)
CREATE NONCLUSTERED COLUMNSTORE INDEX idx_sales_columnstore 
ON sales(product_id, customer_id, sale_date, amount, quantity);

-- Clustered columnstore for data warehouse tables
CREATE CLUSTERED COLUMNSTORE INDEX idx_fact_sales ON fact_sales;
</code></pre>
<h2>Oracle Database Indexing</h2>
<h3>Oracle Index Types</h3>
<h4>Function-Based Indexes</h4>
<pre><code class="language-sql">-- Index on expressions
CREATE INDEX idx_users_upper_email ON users(UPPER(email));
CREATE INDEX idx_orders_year ON orders(EXTRACT(YEAR FROM order_date));

-- Complex function-based index
CREATE INDEX idx_products_profit_margin ON products((price - cost) / price * 100);
</code></pre>
<h4>Reverse Key Indexes</h4>
<pre><code class="language-sql">-- Distribute sequential inserts across index blocks
CREATE INDEX idx_orders_id_reverse ON orders(order_id) REVERSE;
</code></pre>
<h4>Bitmap Join Indexes</h4>
<pre><code class="language-sql">-- Pre-join dimension tables for star schema queries
CREATE BITMAP INDEX idx_sales_customer_region 
ON sales(customers.region)
FROM sales, customers
WHERE sales.customer_id = customers.customer_id;
</code></pre>
<h2>Cross-Database Index Best Practices</h2>
<h3>Index Naming Conventions</h3>
<pre><code class="language-sql">-- Consistent naming across databases
-- Pattern: idx_[table]_[columns]_[type]
CREATE INDEX idx_users_email_unique ON users(email);          -- Unique
CREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);  -- Composite
CREATE INDEX idx_products_name_partial ON products(name(20)); -- Partial/Prefix
CREATE INDEX idx_logs_created_filtered ON logs(created_at) WHERE level = 'ERROR';  -- Filtered
</code></pre>
<h3>Monitoring Index Usage</h3>
<h4>MySQL</h4>
<pre><code class="language-sql">-- Check index usage statistics
SELECT 
    TABLE_SCHEMA,
    TABLE_NAME,
    INDEX_NAME,
    CARDINALITY,
    SUB_PART
FROM INFORMATION_SCHEMA.STATISTICS 
WHERE TABLE_SCHEMA = 'your_database';

-- Performance Schema for index usage
SELECT 
    object_schema,
    object_name,
    index_name,
    count_read,
    count_write,
    sum_timer_read,
    sum_timer_write
FROM performance_schema.table_io_waits_summary_by_index_usage;
</code></pre>
<h4>PostgreSQL</h4>
<pre><code class="language-sql">-- Index usage statistics
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes;

-- Unused indexes
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan
FROM pg_stat_user_indexes 
WHERE idx_scan = 0;
</code></pre>
<h4>SQL Server</h4>
<pre><code class="language-sql">-- Index usage statistics
SELECT 
    OBJECT_NAME(i.object_id) AS table_name,
    i.name AS index_name,
    dm_ius.user_seeks,
    dm_ius.user_scans,
    dm_ius.user_lookups,
    dm_ius.user_updates
FROM sys.indexes i
LEFT JOIN sys.dm_db_index_usage_stats dm_ius 
    ON i.object_id = dm_ius.object_id AND i.index_id = dm_ius.index_id
WHERE i.object_id = OBJECT_ID('your_table');
</code></pre>
<h2>Common SQL Indexing Patterns</h2>
<h3>Covering Indexes</h3>
<pre><code class="language-sql">-- Include all needed columns to avoid table lookups
-- MySQL
CREATE INDEX idx_users_email_covering ON users(email, first_name, last_name, phone);

-- SQL Server with INCLUDE
CREATE INDEX idx_users_email_covering ON users(email) INCLUDE (first_name, last_name, phone);

-- PostgreSQL (covering through index-only scans)
CREATE INDEX idx_users_email_names ON users(email, first_name, last_name);
</code></pre>
<h3>Composite Index Column Order</h3>
<pre><code class="language-sql">-- Rule: Most selective column first, then by query patterns
-- Good: High selectivity on email, then commonly filtered by status
CREATE INDEX idx_users_email_status ON users(email, status);

-- Consider query patterns:
-- Query 1: WHERE email = ? AND status = ?     -- Uses index efficiently
-- Query 2: WHERE status = ?                   -- Less efficient
-- Query 3: WHERE email = ?                    -- Uses index efficiently

-- Solution: Create multiple indexes for different query patterns
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_status ON users(status);
CREATE INDEX idx_users_email_status ON users(email, status);  -- For combined queries
</code></pre>
<h2>Index Maintenance and Optimization</h2>
<h3>Rebuilding Indexes</h3>
<pre><code class="language-sql">-- MySQL
OPTIMIZE TABLE users;
ALTER TABLE users ENGINE=InnoDB;  -- Rebuilds table and indexes

-- PostgreSQL
REINDEX INDEX idx_users_email;
REINDEX TABLE users;

-- SQL Server
ALTER INDEX idx_users_email ON users REBUILD;
ALTER INDEX ALL ON users REBUILD;

-- Oracle
ALTER INDEX idx_users_email REBUILD;
</code></pre>
<h3>Index Statistics</h3>
<pre><code class="language-sql">-- MySQL
ANALYZE TABLE users;

-- PostgreSQL
ANALYZE users;
ANALYZE users(email);  -- Specific column

-- SQL Server
UPDATE STATISTICS users;
UPDATE STATISTICS users idx_users_email;

-- Oracle
EXEC DBMS_STATS.GATHER_TABLE_STATS('schema', 'users');
</code></pre>
<h2>Performance Tuning Tips</h2>
<ol>
<li><strong>Monitor Query Patterns</strong>: Create indexes based on actual query patterns, not assumptions</li>
<li><strong>Avoid Over-Indexing</strong>: Each index has maintenance overhead</li>
<li><strong>Use Composite Indexes Wisely</strong>: Column order matters for query efficiency</li>
<li><strong>Regular Maintenance</strong>: Keep statistics updated and rebuild fragmented indexes</li>
<li><strong>Test in Production-Like Environment</strong>: Index performance varies with data size and distribution</li>
</ol>
<h2>Next Steps</h2>
<p>In Part 3, we'll explore NoSQL database indexing strategies, covering MongoDB, Cassandra, Redis, and other NoSQL systems with their unique indexing approaches.</p>
f:T387f,<h2>MongoDB Indexing Strategies</h2>
<h3>MongoDB Index Types</h3>
<h4>Single Field Indexes</h4>
<pre><code class="language-javascript">// Create index on a single field
db.users.createIndex({ "email": 1 })          // Ascending
db.users.createIndex({ "age": -1 })           // Descending
db.users.createIndex({ "status": 1 })

// Query using single field index
db.users.find({ "email": "john@example.com" })
db.users.find({ "age": { $gte: 25 } }).sort({ "age": -1 })
</code></pre>
<h4>Compound Indexes</h4>
<pre><code class="language-javascript">// Create compound index (order matters!)
db.orders.createIndex({ "customer_id": 1, "order_date": -1, "status": 1 })

// Efficient queries using compound index:
db.orders.find({ "customer_id": 123 })                                    // Uses index
db.orders.find({ "customer_id": 123, "order_date": { $gte: new Date() } }) // Uses index
db.orders.find({ "customer_id": 123, "order_date": -1, "status": "active" }) // Uses full index

// Inefficient queries:
db.orders.find({ "order_date": { $gte: new Date() } })  // Can't use index efficiently
db.orders.find({ "status": "active" })                  // Can't use index efficiently
</code></pre>
<h4>Text Indexes for Full-Text Search</h4>
<pre><code class="language-javascript">// Create text index
db.articles.createIndex({ 
    "title": "text", 
    "content": "text" 
}, { 
    weights: { "title": 10, "content": 1 },
    name: "article_text_index"
})

// Text search queries
db.articles.find({ $text: { $search: "database optimization" } })
db.articles.find({ 
    $text: { 
        $search: "\"database indexes\"",  // Exact phrase
        $caseSensitive: false 
    } 
}).sort({ score: { $meta: "textScore" } })
</code></pre>
<h4>Geospatial Indexes</h4>
<pre><code class="language-javascript">// 2dsphere index for GeoJSON data
db.locations.createIndex({ "coordinates": "2dsphere" })

// Geospatial queries
db.locations.find({
    coordinates: {
        $near: {
            $geometry: { type: "Point", coordinates: [-122.4194, 37.7749] },
            $maxDistance: 1000  // meters
        }
    }
})

// Geospatial aggregation
db.locations.aggregate([
    {
        $geoNear: {
            near: { type: "Point", coordinates: [-122.4194, 37.7749] },
            distanceField: "distance",
            maxDistance: 5000,
            spherical: true
        }
    }
])
</code></pre>
<h4>Partial Indexes</h4>
<pre><code class="language-javascript">// Index only documents matching a condition
db.users.createIndex(
    { "email": 1 }, 
    { partialFilterExpression: { "status": "active" } }
)

// Index only non-null values
db.products.createIndex(
    { "discount_price": 1 },
    { partialFilterExpression: { "discount_price": { $exists: true } } }
)
</code></pre>
<h4>Sparse Indexes</h4>
<pre><code class="language-javascript">// Index only documents that contain the indexed field
db.users.createIndex({ "phone": 1 }, { sparse: true })

// Useful for optional fields to save space
db.profiles.createIndex({ "linkedin_url": 1 }, { sparse: true })
</code></pre>
<h3>MongoDB Index Performance</h3>
<h4>Analyzing Query Performance</h4>
<pre><code class="language-javascript">// Explain query execution
db.users.find({ "email": "john@example.com" }).explain("executionStats")

// Index usage statistics
db.users.aggregate([{ $indexStats: {} }])

// Get index information
db.users.getIndexes()
</code></pre>
<h4>Index Hints</h4>
<pre><code class="language-javascript">// Force use of specific index
db.users.find({ "age": { $gte: 25 } }).hint({ "age": 1 })

// Use natural order (no index)
db.users.find().hint({ $natural: 1 })
</code></pre>
<h2>Cassandra Indexing</h2>
<h3>Primary Key and Clustering</h3>
<h4>Partition Key and Clustering Columns</h4>
<pre><code class="language-sql">-- Table with compound primary key
CREATE TABLE user_sessions (
    user_id UUID,           -- Partition key
    session_date DATE,      -- Clustering column
    session_id TIMEUUID,    -- Clustering column
    ip_address TEXT,
    user_agent TEXT,
    PRIMARY KEY (user_id, session_date, session_id)
) WITH CLUSTERING ORDER BY (session_date DESC, session_id DESC);

-- Efficient queries (follow primary key structure):
SELECT * FROM user_sessions WHERE user_id = ?;
SELECT * FROM user_sessions WHERE user_id = ? AND session_date = ?;
SELECT * FROM user_sessions WHERE user_id = ? AND session_date >= ? AND session_date &#x3C;= ?;
</code></pre>
<h4>Secondary Indexes</h4>
<pre><code class="language-sql">-- Create secondary index
CREATE INDEX idx_user_sessions_ip ON user_sessions(ip_address);

-- Query using secondary index
SELECT * FROM user_sessions WHERE ip_address = '192.168.1.100';

-- Note: Secondary indexes in Cassandra have limitations:
-- - Can be expensive for large datasets
-- - Limited to equality comparisons
-- - Should be used with other WHERE clauses when possible
</code></pre>
<h4>Materialized Views</h4>
<pre><code class="language-sql">-- Create materialized view for different query patterns
CREATE MATERIALIZED VIEW user_sessions_by_ip AS
    SELECT user_id, session_date, session_id, ip_address, user_agent
    FROM user_sessions
    WHERE ip_address IS NOT NULL AND user_id IS NOT NULL 
          AND session_date IS NOT NULL AND session_id IS NOT NULL
    PRIMARY KEY (ip_address, user_id, session_date, session_id);

-- Query the materialized view
SELECT * FROM user_sessions_by_ip WHERE ip_address = '192.168.1.100';
</code></pre>
<h3>Cassandra Indexing Best Practices</h3>
<h4>Avoid Anti-Patterns</h4>
<pre><code class="language-sql">-- BAD: Querying without partition key
SELECT * FROM user_sessions WHERE session_date = '2024-03-20';  -- Requires ALLOW FILTERING

-- BAD: Secondary index on high-cardinality column
CREATE INDEX idx_sessions_id ON user_sessions(session_id);  -- Will be slow

-- GOOD: Include partition key in queries
SELECT * FROM user_sessions 
WHERE user_id = ? AND session_date = '2024-03-20';

-- GOOD: Secondary index on low-cardinality column
CREATE INDEX idx_sessions_status ON user_sessions(status);  -- If status has few values
</code></pre>
<h2>Redis Indexing and Search</h2>
<h3>Redis Search (RediSearch Module)</h3>
<h4>Creating Indexes</h4>
<pre><code class="language-redis"># Create index for hash documents
FT.CREATE user_idx 
    ON hash 
    PREFIX 1 "user:" 
    SCHEMA 
        name TEXT SORTABLE 
        email TEXT SORTABLE 
        age NUMERIC SORTABLE 
        city TAG SORTABLE
        bio TEXT

# Create index for JSON documents
FT.CREATE product_idx 
    ON JSON 
    PREFIX 1 "product:" 
    SCHEMA 
        $.name AS name TEXT SORTABLE 
        $.price AS price NUMERIC SORTABLE 
        $.category AS category TAG SORTABLE 
        $.description AS description TEXT
</code></pre>
<h4>Searching with RediSearch</h4>
<pre><code class="language-redis"># Text search
FT.SEARCH user_idx "john"
FT.SEARCH user_idx "john doe"
FT.SEARCH user_idx "@name:john"

# Numeric range queries
FT.SEARCH user_idx "@age:[25 35]"

# Tag queries
FT.SEARCH user_idx "@city:{San Francisco}"

# Complex queries
FT.SEARCH user_idx "@name:john @age:[25 35] @city:{San Francisco}"

# Aggregation
FT.AGGREGATE user_idx "*" 
    GROUPBY 1 @city 
    REDUCE COUNT 0 AS count 
    SORTBY 2 @count DESC
</code></pre>
<h3>Redis Native Data Structure Indexing</h3>
<h4>Sets for Indexing</h4>
<pre><code class="language-redis"># Index users by city using sets
SADD "city:san_francisco" "user:1" "user:5" "user:10"
SADD "city:new_york" "user:2" "user:7"

# Find users in a specific city
SMEMBERS "city:san_francisco"

# Find users in multiple cities (union)
SUNION "city:san_francisco" "city:new_york"

# Find users in common cities (intersection)
SINTER "city:san_francisco" "active_users"
</code></pre>
<h4>Sorted Sets for Range Queries</h4>
<pre><code class="language-redis"># Index users by age using sorted sets
ZADD "users_by_age" 25 "user:1" 30 "user:2" 35 "user:3"

# Range queries
ZRANGEBYSCORE "users_by_age" 25 35        # Users aged 25-35
ZREVRANGEBYSCORE "users_by_age" 35 25     # Users aged 25-35 (descending)
ZCOUNT "users_by_age" 25 35               # Count users aged 25-35
</code></pre>
<h2>DynamoDB Indexing</h2>
<h3>Primary Key Structure</h3>
<pre><code class="language-javascript">// Hash key only
const userTable = {
    TableName: 'Users',
    KeySchema: [
        { AttributeName: 'userId', KeyType: 'HASH' }
    ],
    AttributeDefinitions: [
        { AttributeName: 'userId', AttributeType: 'S' }
    ]
};

// Hash key + Sort key
const orderTable = {
    TableName: 'Orders',
    KeySchema: [
        { AttributeName: 'customerId', KeyType: 'HASH' },    // Partition key
        { AttributeName: 'orderDate', KeyType: 'RANGE' }     // Sort key
    ],
    AttributeDefinitions: [
        { AttributeName: 'customerId', AttributeType: 'S' },
        { AttributeName: 'orderDate', AttributeType: 'S' }
    ]
};
</code></pre>
<h3>Global Secondary Indexes (GSI)</h3>
<pre><code class="language-javascript">// Create GSI for different query patterns
const gsiDefinition = {
    IndexName: 'email-index',
    KeySchema: [
        { AttributeName: 'email', KeyType: 'HASH' }
    ],
    AttributeDefinitions: [
        { AttributeName: 'email', AttributeType: 'S' }
    ],
    Projection: { ProjectionType: 'ALL' },  // Include all attributes
    ProvisionedThroughput: {
        ReadCapacityUnits: 5,
        WriteCapacityUnits: 5
    }
};

// Query using GSI
const params = {
    TableName: 'Users',
    IndexName: 'email-index',
    KeyConditionExpression: 'email = :email',
    ExpressionAttributeValues: {
        ':email': 'john@example.com'
    }
};
</code></pre>
<h3>Local Secondary Indexes (LSI)</h3>
<pre><code class="language-javascript">// LSI uses same partition key but different sort key
const lsiDefinition = {
    IndexName: 'customer-status-index',
    KeySchema: [
        { AttributeName: 'customerId', KeyType: 'HASH' },    // Same partition key
        { AttributeName: 'status', KeyType: 'RANGE' }        // Different sort key
    ],
    Projection: {
        ProjectionType: 'INCLUDE',
        NonKeyAttributes: ['orderTotal', 'items']
    }
};
</code></pre>
<h2>Elasticsearch Indexing</h2>
<h3>Index Mapping and Analysis</h3>
<pre><code class="language-json">// Create index with custom mapping
PUT /products
{
  "mappings": {
    "properties": {
      "name": {
        "type": "text",
        "analyzer": "standard",
        "fields": {
          "keyword": {
            "type": "keyword"
          }
        }
      },
      "price": {
        "type": "float"
      },
      "category": {
        "type": "keyword"
      },
      "description": {
        "type": "text",
        "analyzer": "english"
      },
      "created_at": {
        "type": "date"
      },
      "location": {
        "type": "geo_point"
      }
    }
  }
}
</code></pre>
<h3>Elasticsearch Query Optimization</h3>
<pre><code class="language-json">// Multi-field search with boosting
GET /products/_search
{
  "query": {
    "multi_match": {
      "query": "wireless headphones",
      "fields": ["name^3", "description"],
      "type": "best_fields"
    }
  }
}

// Filtered search with aggregations
GET /products/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "description": "wireless" } }
      ],
      "filter": [
        { "range": { "price": { "gte": 50, "lte": 200 } } },
        { "term": { "category": "electronics" } }
      ]
    }
  },
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price",
        "ranges": [
          { "to": 50 },
          { "from": 50, "to": 100 },
          { "from": 100, "to": 200 },
          { "from": 200 }
        ]
      }
    }
  }
}
</code></pre>
<h2>NoSQL Indexing Best Practices</h2>
<h3>Design for Query Patterns</h3>
<ol>
<li><strong>Understand Access Patterns</strong>: Design indexes based on how data will be queried</li>
<li><strong>Denormalization</strong>: Accept data duplication to optimize read performance</li>
<li><strong>Composite Keys</strong>: Use compound keys to support multiple query patterns</li>
</ol>
<h3>MongoDB Specific</h3>
<ul>
<li><strong>ESR Rule</strong>: Equality, Sort, Range - order compound index fields by this priority</li>
<li><strong>Index Intersection</strong>: MongoDB can use multiple single-field indexes together</li>
<li><strong>Index Prefix</strong>: Compound indexes can support queries on index prefixes</li>
</ul>
<h3>Cassandra Specific</h3>
<ul>
<li><strong>Partition Key Design</strong>: Ensure even data distribution across nodes</li>
<li><strong>Clustering Columns</strong>: Use for sorting and range queries within partitions</li>
<li><strong>Secondary Index Limitations</strong>: Use sparingly and with other WHERE clauses</li>
</ul>
<h3>Document Database Patterns</h3>
<pre><code class="language-javascript">// MongoDB: Embedded vs Referenced data
// Embedded for one-to-few relationships
{
    "_id": ObjectId("..."),
    "user_id": 123,
    "order_date": ISODate("..."),
    "items": [
        { "product_id": 456, "quantity": 2, "price": 29.99 },
        { "product_id": 789, "quantity": 1, "price": 19.99 }
    ]
}

// Referenced for one-to-many relationships
// Orders collection
{ "_id": ObjectId("..."), "user_id": 123, "total": 79.97 }

// Order_items collection
{ "_id": ObjectId("..."), "order_id": ObjectId("..."), "product_id": 456 }
</code></pre>
<h2>Performance Monitoring</h2>
<h3>MongoDB Monitoring</h3>
<pre><code class="language-javascript">// Index usage statistics
db.users.aggregate([{ $indexStats: {} }])

// Slow query profiling
db.setProfilingLevel(2, { slowms: 100 })
db.system.profile.find().sort({ ts: -1 }).limit(5)
</code></pre>
<h3>Cassandra Monitoring</h3>
<pre><code class="language-sql">-- Check table statistics
SELECT * FROM system.size_estimates WHERE keyspace_name = 'your_keyspace';

-- Monitor read/write latencies
nodetool cfstats your_keyspace.your_table
</code></pre>
<h2>Next Steps</h2>
<p>In Part 4, we'll explore composite indexes and advanced query optimization techniques, including index intersection, covering indexes, and query plan analysis across different database systems.</p>
10:T40a5,<h2>Understanding Composite Indexes</h2>
<p>Composite indexes (also called compound or multi-column indexes) include multiple columns in a single index structure. The order of columns in composite indexes is crucial for query performance.</p>
<h3>The Index Column Order Principle</h3>
<pre><code class="language-sql">-- Example table
CREATE TABLE sales (
    id INT PRIMARY KEY,
    customer_id INT,
    product_id INT,
    sale_date DATE,
    amount DECIMAL(10,2),
    region VARCHAR(50),
    salesperson_id INT
);

-- Composite index with specific column order
CREATE INDEX idx_sales_composite ON sales(customer_id, sale_date, amount);
</code></pre>
<h3>How Composite Indexes Work</h3>
<pre><code>Index Structure: (customer_id, sale_date, amount)
┌─────────────┬─────────────┬────────┬──────────┐
│ customer_id │ sale_date   │ amount │ Row Ptr  │
├─────────────┼─────────────┼────────┼──────────┤
│     100     │ 2024-01-15  │  250.0 │   →      │
│     100     │ 2024-01-20  │  175.0 │   →      │
│     100     │ 2024-02-10  │  300.0 │   →      │
│     101     │ 2024-01-12  │  450.0 │   →      │
│     101     │ 2024-01-25  │  200.0 │   →      │
└─────────────┴─────────────┴────────┴──────────┘
</code></pre>
<h3>Query Efficiency with Composite Indexes</h3>
<pre><code class="language-sql">-- HIGHLY EFFICIENT: Uses full index
SELECT * FROM sales 
WHERE customer_id = 100 
  AND sale_date BETWEEN '2024-01-01' AND '2024-01-31'
  AND amount > 200;

-- EFFICIENT: Uses index prefix (customer_id, sale_date)
SELECT * FROM sales 
WHERE customer_id = 100 
  AND sale_date BETWEEN '2024-01-01' AND '2024-01-31';

-- EFFICIENT: Uses index prefix (customer_id)
SELECT * FROM sales WHERE customer_id = 100;

-- INEFFICIENT: Cannot use index effectively
SELECT * FROM sales WHERE sale_date = '2024-01-15';  -- Missing customer_id prefix

-- INEFFICIENT: Cannot use index effectively  
SELECT * FROM sales WHERE amount > 200;  -- Missing customer_id and sale_date prefix
</code></pre>
<h2>Optimal Column Ordering Strategies</h2>
<h3>The ESR Rule (Equality, Sort, Range)</h3>
<pre><code class="language-sql">-- Query pattern analysis
SELECT * FROM orders 
WHERE customer_id = ?          -- Equality
  AND status = ?               -- Equality  
ORDER BY order_date DESC       -- Sort
  AND total_amount > ?;        -- Range

-- Optimal index order: Equality → Sort → Range
CREATE INDEX idx_orders_esr ON orders(customer_id, status, order_date, total_amount);
</code></pre>
<h3>Selectivity-Based Ordering</h3>
<pre><code class="language-sql">-- High selectivity (many unique values) → Low selectivity (few unique values)
CREATE INDEX idx_users_selective ON users(
    email,        -- High selectivity (unique emails)
    age,          -- Medium selectivity  
    status        -- Low selectivity ('active', 'inactive', 'pending')
);

-- Check column selectivity
SELECT 
    COUNT(DISTINCT email) / COUNT(*) as email_selectivity,
    COUNT(DISTINCT age) / COUNT(*) as age_selectivity,
    COUNT(DISTINCT status) / COUNT(*) as status_selectivity
FROM users;
</code></pre>
<h3>Frequency-Based Ordering</h3>
<pre><code class="language-sql">-- Most frequently queried columns first
-- Analysis shows: 80% of queries filter by region, 60% by date, 30% by salesperson

CREATE INDEX idx_sales_frequency ON sales(
    region,           -- Used in 80% of queries (most frequent)
    sale_date,        -- Used in 60% of queries
    salesperson_id    -- Used in 30% of queries
);
</code></pre>
<h2>Advanced Composite Index Techniques</h2>
<h3>Index Intersection vs Single Composite Index</h3>
<pre><code class="language-sql">-- Option 1: Multiple single-column indexes
CREATE INDEX idx_customer ON sales(customer_id);
CREATE INDEX idx_date ON sales(sale_date);
CREATE INDEX idx_amount ON sales(amount);

-- Option 2: Single composite index
CREATE INDEX idx_composite ON sales(customer_id, sale_date, amount);

-- Query performance comparison
SELECT * FROM sales 
WHERE customer_id = 100 
  AND sale_date >= '2024-01-01' 
  AND amount > 200;

-- Option 1: Database may use index intersection (combining multiple indexes)
-- Option 2: Single index lookup (generally more efficient)
</code></pre>
<h3>Covering Indexes (Include Columns)</h3>
<pre><code class="language-sql">-- SQL Server: INCLUDE additional columns at leaf level
CREATE NONCLUSTERED INDEX idx_sales_covering 
ON sales(customer_id, sale_date) 
INCLUDE (amount, product_id, salesperson_id);

-- This query can be satisfied entirely from the index
SELECT customer_id, sale_date, amount, product_id 
FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- PostgreSQL: Add extra columns to create covering index
CREATE INDEX idx_sales_covering ON sales(customer_id, sale_date, amount, product_id, salesperson_id);

-- MySQL: Include all needed columns in the index
CREATE INDEX idx_sales_covering ON sales(customer_id, sale_date, amount, product_id, salesperson_id);
</code></pre>
<h3>Partial Composite Indexes</h3>
<pre><code class="language-sql">-- PostgreSQL: Index only relevant data
CREATE INDEX idx_active_customer_sales 
ON sales(customer_id, sale_date) 
WHERE status = 'completed' AND amount > 0;

-- SQL Server: Filtered index
CREATE INDEX idx_active_customer_sales 
ON sales(customer_id, sale_date) 
WHERE status = 'completed' AND amount > 0;

-- Benefits: Smaller index size, faster maintenance, targeted queries
</code></pre>
<h2>Query Optimization Techniques</h2>
<h3>Analyzing Query Execution Plans</h3>
<h4>MySQL Query Analysis</h4>
<pre><code class="language-sql">-- Basic explain
EXPLAIN SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- Extended explain with cost information
EXPLAIN FORMAT=JSON 
SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01'
ORDER BY sale_date DESC;

-- Analyze actual execution
EXPLAIN ANALYZE 
SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';
</code></pre>
<h4>PostgreSQL Query Analysis</h4>
<pre><code class="language-sql">-- Basic execution plan
EXPLAIN SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- Detailed execution plan with costs
EXPLAIN (ANALYZE, COSTS, BUFFERS) 
SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- JSON format for programmatic analysis
EXPLAIN (ANALYZE, COSTS, BUFFERS, FORMAT JSON) 
SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';
</code></pre>
<h4>SQL Server Query Analysis</h4>
<pre><code class="language-sql">-- Show execution plan
SET SHOWPLAN_ALL ON;
SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- Include actual execution statistics
SET STATISTICS IO ON;
SET STATISTICS TIME ON;
SELECT * FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- Use SQL Server Management Studio for graphical plans
</code></pre>
<h3>Index Hints and Forcing</h3>
<pre><code class="language-sql">-- MySQL: Force specific index usage
SELECT * FROM sales USE INDEX (idx_sales_composite)
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- PostgreSQL: No direct index hints, but can disable other access methods
SET enable_seqscan = off;
SELECT * FROM sales WHERE customer_id = 100;
SET enable_seqscan = on;

-- SQL Server: Index hints
SELECT * FROM sales WITH (INDEX(idx_sales_composite))
WHERE customer_id = 100 AND sale_date >= '2024-01-01';

-- Oracle: Index hints
SELECT /*+ INDEX(sales idx_sales_composite) */ * 
FROM sales 
WHERE customer_id = 100 AND sale_date >= '2024-01-01';
</code></pre>
<h2>Complex Query Optimization Patterns</h2>
<h3>Join Optimization with Composite Indexes</h3>
<pre><code class="language-sql">-- Tables
CREATE TABLE customers (
    id INT PRIMARY KEY,
    email VARCHAR(255),
    region VARCHAR(50),
    status VARCHAR(20)
);

CREATE TABLE orders (
    id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE,
    total_amount DECIMAL(10,2),
    status VARCHAR(20)
);

-- Indexes for join optimization
CREATE INDEX idx_customers_region_status ON customers(region, status);
CREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);
CREATE INDEX idx_orders_date_amount ON orders(order_date, total_amount);

-- Optimized join query
SELECT c.email, o.order_date, o.total_amount
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE c.region = 'West Coast' 
  AND c.status = 'active'
  AND o.order_date >= '2024-01-01'
  AND o.total_amount > 100;
</code></pre>
<h3>Subquery Optimization</h3>
<pre><code class="language-sql">-- Original inefficient query
SELECT * FROM customers c
WHERE EXISTS (
    SELECT 1 FROM orders o 
    WHERE o.customer_id = c.id 
      AND o.order_date >= '2024-01-01'
);

-- Create index to optimize the subquery
CREATE INDEX idx_orders_customer_date_exists ON orders(customer_id, order_date);

-- Alternative: Convert to JOIN for better performance
SELECT DISTINCT c.*
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE o.order_date >= '2024-01-01';
</code></pre>
<h3>Window Function Optimization</h3>
<pre><code class="language-sql">-- Window function query
SELECT 
    customer_id,
    order_date,
    total_amount,
    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as rn
FROM orders
WHERE order_date >= '2024-01-01';

-- Optimal index for window function
CREATE INDEX idx_orders_window ON orders(customer_id, order_date DESC);
-- The index supports both the WHERE clause and the window function's PARTITION BY and ORDER BY
</code></pre>
<h2>Index Maintenance for Composite Indexes</h2>
<h3>Monitoring Index Usage</h3>
<pre><code class="language-sql">-- PostgreSQL: Check index usage statistics
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched
FROM pg_stat_user_indexes 
WHERE tablename = 'sales'
ORDER BY idx_scan DESC;

-- MySQL: Check index usage with Performance Schema
SELECT 
    object_schema,
    object_name,
    index_name,
    count_read,
    count_write,
    sum_timer_read,
    sum_timer_write
FROM performance_schema.table_io_waits_summary_by_index_usage
WHERE object_schema = 'your_database' 
  AND object_name = 'sales';

-- SQL Server: Index usage statistics
SELECT 
    OBJECT_NAME(s.object_id) AS table_name,
    i.name AS index_name,
    s.user_seeks,
    s.user_scans,
    s.user_lookups,
    s.user_updates
FROM sys.dm_db_index_usage_stats s
JOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id
WHERE OBJECT_NAME(s.object_id) = 'sales';
</code></pre>
<h3>Identifying Redundant Indexes</h3>
<pre><code class="language-sql">-- Find potentially redundant indexes
-- Index A: (customer_id, sale_date)
-- Index B: (customer_id, sale_date, amount) 
-- Index B can handle all queries that Index A can handle

-- PostgreSQL query to find redundant indexes
SELECT 
    t.schemaname,
    t.tablename,
    c.reltuples::bigint AS rows,
    pg_size_pretty(pg_total_relation_size(c.oid)) AS size,
    ARRAY_AGG(DISTINCT indexname ORDER BY indexname) AS indexes
FROM pg_stat_user_tables t
JOIN pg_class c ON c.relname = t.tablename
JOIN pg_stat_user_indexes i ON i.relid = c.oid
GROUP BY t.schemaname, t.tablename, c.reltuples, c.oid
HAVING COUNT(*) > 1;
</code></pre>
<h3>Index Fragmentation and Rebuilding</h3>
<pre><code class="language-sql">-- SQL Server: Check index fragmentation
SELECT 
    OBJECT_NAME(i.object_id) AS table_name,
    i.name AS index_name,
    s.avg_fragmentation_in_percent,
    s.page_count
FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'DETAILED') s
JOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id
WHERE s.avg_fragmentation_in_percent > 10
  AND s.page_count > 1000;

-- Rebuild highly fragmented indexes
ALTER INDEX idx_sales_composite ON sales REBUILD;

-- PostgreSQL: Reindex when needed
REINDEX INDEX idx_sales_composite;

-- MySQL: Optimize table to rebuild indexes
OPTIMIZE TABLE sales;
</code></pre>
<h2>Performance Testing and Benchmarking</h2>
<h3>Creating Test Data for Index Testing</h3>
<pre><code class="language-sql">-- Generate test data
INSERT INTO sales (customer_id, product_id, sale_date, amount, region, salesperson_id)
SELECT 
    (RANDOM() * 10000)::INT + 1,  -- customer_id (1-10000)
    (RANDOM() * 1000)::INT + 1,   -- product_id (1-1000)
    DATE '2023-01-01' + (RANDOM() * 365)::INT,  -- sale_date (2023)
    (RANDOM() * 1000 + 10)::DECIMAL(10,2),      -- amount (10-1010)
    CASE (RANDOM() * 4)::INT 
        WHEN 0 THEN 'North'
        WHEN 1 THEN 'South' 
        WHEN 2 THEN 'East'
        ELSE 'West'
    END,                          -- region
    (RANDOM() * 100)::INT + 1     -- salesperson_id (1-100)
FROM generate_series(1, 1000000); -- 1M rows
</code></pre>
<h3>A/B Testing Index Performance</h3>
<pre><code class="language-sql">-- Test 1: Without composite index
DROP INDEX IF EXISTS idx_sales_composite;
\timing on
SELECT * FROM sales 
WHERE customer_id = 100 
  AND sale_date >= '2024-01-01' 
  AND amount > 200;
\timing off

-- Test 2: With composite index
CREATE INDEX idx_sales_composite ON sales(customer_id, sale_date, amount);
\timing on
SELECT * FROM sales 
WHERE customer_id = 100 
  AND sale_date >= '2024-01-01' 
  AND amount > 200;
\timing off
</code></pre>
<h3>Load Testing with Composite Indexes</h3>
<pre><code class="language-python"># Python script for concurrent query testing
import psycopg2
import threading
import time
import random

def run_queries(connection_string, num_queries):
    conn = psycopg2.connect(connection_string)
    cursor = conn.cursor()
    
    start_time = time.time()
    for i in range(num_queries):
        customer_id = random.randint(1, 10000)
        cursor.execute("""
            SELECT * FROM sales 
            WHERE customer_id = %s 
              AND sale_date >= '2024-01-01' 
              AND amount > 200
        """, (customer_id,))
        results = cursor.fetchall()
    
    end_time = time.time()
    print(f"Thread completed {num_queries} queries in {end_time - start_time:.2f} seconds")
    
    cursor.close()
    conn.close()

# Run concurrent load test
threads = []
for i in range(10):  # 10 concurrent threads
    thread = threading.Thread(target=run_queries, args=(connection_string, 100))
    threads.append(thread)
    thread.start()

for thread in threads:
    thread.join()
</code></pre>
<h2>Best Practices Summary</h2>
<h3>Design Principles</h3>
<ol>
<li><strong>Understand Query Patterns</strong>: Analyze actual application queries before creating indexes</li>
<li><strong>Column Order Matters</strong>: Follow ESR rule and consider selectivity</li>
<li><strong>Covering Indexes</strong>: Include frequently accessed columns to avoid table lookups</li>
<li><strong>Partial Indexes</strong>: Filter out irrelevant data to reduce index size</li>
</ol>
<h3>Maintenance Guidelines</h3>
<ol>
<li><strong>Monitor Usage</strong>: Regularly check index usage statistics</li>
<li><strong>Remove Unused Indexes</strong>: Drop indexes that aren't being used</li>
<li><strong>Rebuild When Needed</strong>: Address fragmentation in high-write environments</li>
<li><strong>Update Statistics</strong>: Keep optimizer statistics current</li>
</ol>
<h3>Performance Testing</h3>
<ol>
<li><strong>Test with Real Data</strong>: Use production-like data volumes and distributions</li>
<li><strong>Measure Before and After</strong>: Always benchmark performance improvements</li>
<li><strong>Load Testing</strong>: Test under concurrent workloads</li>
<li><strong>Monitor Resources</strong>: Watch CPU, memory, and I/O impact</li>
</ol>
<h2>Next Steps</h2>
<p>In Part 5, we'll dive into index performance monitoring and maintenance strategies, including automated index tuning, fragmentation management, and advanced monitoring techniques across different database platforms.</p>
11:T4df7,<h2>Index Performance Monitoring Fundamentals</h2>
<h3>Key Performance Metrics</h3>
<h4>Query Performance Indicators</h4>
<ul>
<li><strong>Query Execution Time</strong>: End-to-end query duration</li>
<li><strong>Index Seek vs Index Scan</strong>: Seek is targeted, scan reads entire index</li>
<li><strong>Key Lookups</strong>: Additional operations to fetch non-indexed columns</li>
<li><strong>Sort Operations</strong>: Whether sorting uses indexes or requires explicit sorting</li>
<li><strong>Buffer Cache Hit Ratio</strong>: Percentage of index pages found in memory</li>
</ul>
<h4>Index Health Metrics</h4>
<ul>
<li><strong>Index Fragmentation</strong>: Physical disorder of index pages</li>
<li><strong>Index Usage Statistics</strong>: How frequently indexes are accessed</li>
<li><strong>Index Size Growth</strong>: Storage consumption over time</li>
<li><strong>Write Performance Impact</strong>: Effect on INSERT/UPDATE/DELETE operations</li>
</ul>
<h3>Database-Specific Monitoring Tools</h3>
<h4>MySQL Performance Monitoring</h4>
<pre><code class="language-sql">-- Enable Performance Schema
SET GLOBAL performance_schema = ON;

-- Monitor index usage
SELECT 
    object_schema,
    object_name,
    index_name,
    count_read,
    count_write,
    sum_timer_read/1000000000 AS read_time_seconds,
    sum_timer_write/1000000000 AS write_time_seconds
FROM performance_schema.table_io_waits_summary_by_index_usage
WHERE object_schema = 'your_database'
ORDER BY count_read DESC;

-- Check slow queries using indexes
SELECT 
    digest_text,
    count_star,
    avg_timer_wait/1000000000 AS avg_time_seconds,
    sum_rows_examined,
    sum_rows_sent
FROM performance_schema.events_statements_summary_by_digest
WHERE digest_text LIKE '%your_table%'
ORDER BY avg_timer_wait DESC;

-- Index effectiveness analysis
SELECT 
    TABLE_SCHEMA,
    TABLE_NAME,
    INDEX_NAME,
    CARDINALITY,
    CARDINALITY / (SELECT table_rows FROM information_schema.tables 
                   WHERE table_schema = s.TABLE_SCHEMA 
                   AND table_name = s.TABLE_NAME) AS selectivity
FROM information_schema.statistics s
WHERE TABLE_SCHEMA = 'your_database'
ORDER BY selectivity DESC;
</code></pre>
<h4>PostgreSQL Monitoring</h4>
<pre><code class="language-sql">-- Index usage statistics
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch,
    idx_scan::float / GREATEST(seq_scan + idx_scan, 1) AS index_usage_ratio
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;

-- Unused indexes (potential candidates for removal)
SELECT 
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS size
FROM pg_stat_user_indexes
WHERE idx_scan = 0 
  AND schemaname = 'public'
ORDER BY pg_relation_size(indexrelid) DESC;

-- Index bloat analysis
SELECT 
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS size,
    CASE 
        WHEN indisunique THEN 'UNIQUE'
        ELSE 'NON-UNIQUE'
    END AS index_type,
    n_tup_ins + n_tup_upd + n_tup_del AS total_writes,
    idx_scan AS total_scans
FROM pg_stat_user_indexes 
JOIN pg_stat_user_tables USING (schemaname, tablename)
JOIN pg_index ON indexrelid = pg_index.indexrelid
WHERE schemaname = 'public'
ORDER BY pg_relation_size(indexrelid) DESC;

-- Buffer cache hit ratio for indexes
SELECT 
    schemaname,
    tablename,
    indexname,
    heap_blks_read,
    heap_blks_hit,
    CASE 
        WHEN heap_blks_hit + heap_blks_read = 0 THEN NULL
        ELSE heap_blks_hit::float / (heap_blks_hit + heap_blks_read)
    END AS hit_ratio
FROM pg_statio_user_indexes
WHERE schemaname = 'public'
ORDER BY hit_ratio ASC NULLS LAST;
</code></pre>
<h4>SQL Server Monitoring</h4>
<pre><code class="language-sql">-- Index usage statistics
SELECT 
    OBJECT_NAME(i.object_id) AS table_name,
    i.name AS index_name,
    i.type_desc AS index_type,
    dm_ius.user_seeks,
    dm_ius.user_scans,
    dm_ius.user_lookups,
    dm_ius.user_updates,
    dm_ius.user_seeks + dm_ius.user_scans + dm_ius.user_lookups AS total_reads,
    CASE 
        WHEN dm_ius.user_updates > 0 
        THEN (dm_ius.user_seeks + dm_ius.user_scans + dm_ius.user_lookups) / dm_ius.user_updates 
        ELSE NULL 
    END AS read_write_ratio
FROM sys.indexes i
LEFT JOIN sys.dm_db_index_usage_stats dm_ius 
    ON i.object_id = dm_ius.object_id AND i.index_id = dm_ius.index_id
WHERE OBJECTPROPERTY(i.object_id, 'IsUserTable') = 1
ORDER BY total_reads DESC;

-- Index fragmentation analysis
SELECT 
    OBJECT_NAME(i.object_id) AS table_name,
    i.name AS index_name,
    s.avg_fragmentation_in_percent,
    s.fragment_count,
    s.page_count,
    CASE 
        WHEN s.avg_fragmentation_in_percent &#x3C; 5 THEN 'No action needed'
        WHEN s.avg_fragmentation_in_percent &#x3C; 30 THEN 'Reorganize'
        ELSE 'Rebuild'
    END AS recommended_action
FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'DETAILED') s
JOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id
WHERE s.page_count > 100  -- Only consider indexes with significant pages
ORDER BY s.avg_fragmentation_in_percent DESC;

-- Missing index suggestions
SELECT 
    mid.statement AS table_name,
    mids.equality_columns,
    mids.inequality_columns,
    mids.included_columns,
    migs.user_seeks,
    migs.user_scans,
    migs.avg_total_user_cost,
    migs.avg_user_impact,
    'CREATE INDEX idx_' + REPLACE(REPLACE(REPLACE(mid.statement, '[', ''), ']', ''), '.', '_') + 
    '_suggested ON ' + mid.statement + 
    ' (' + ISNULL(mids.equality_columns, '') + 
    CASE WHEN mids.inequality_columns IS NOT NULL 
         THEN CASE WHEN mids.equality_columns IS NOT NULL THEN ',' ELSE '' END + mids.inequality_columns 
         ELSE '' END + ')' +
    CASE WHEN mids.included_columns IS NOT NULL 
         THEN ' INCLUDE (' + mids.included_columns + ')' 
         ELSE '' END AS create_statement
FROM sys.dm_db_missing_index_groups mig
JOIN sys.dm_db_missing_index_group_stats migs ON mig.index_group_handle = migs.group_handle
JOIN sys.dm_db_missing_index_details mid ON mig.index_handle = mid.index_handle
WHERE migs.avg_user_impact > 50  -- High impact suggestions only
ORDER BY migs.avg_user_impact DESC;
</code></pre>
<h2>Automated Index Maintenance</h2>
<h3>SQL Server Automated Maintenance</h3>
<pre><code class="language-sql">-- Create maintenance plan for index optimization
-- This script reorganizes or rebuilds indexes based on fragmentation level

DECLARE @DatabaseName NVARCHAR(50) = 'YourDatabase'
DECLARE @FragmentationThreshold FLOAT = 5.0
DECLARE @RebuildThreshold FLOAT = 30.0

-- Cursor to iterate through fragmented indexes
DECLARE index_cursor CURSOR FOR
SELECT 
    OBJECT_NAME(i.object_id) AS table_name,
    i.name AS index_name,
    s.avg_fragmentation_in_percent,
    CASE 
        WHEN s.avg_fragmentation_in_percent >= @RebuildThreshold THEN 'REBUILD'
        WHEN s.avg_fragmentation_in_percent >= @FragmentationThreshold THEN 'REORGANIZE'
        ELSE 'NO_ACTION'
    END AS action_type
FROM sys.dm_db_index_physical_stats(DB_ID(@DatabaseName), NULL, NULL, NULL, 'DETAILED') s
JOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id
WHERE s.avg_fragmentation_in_percent >= @FragmentationThreshold
  AND s.page_count > 100;

DECLARE @TableName NVARCHAR(128), @IndexName NVARCHAR(128), @Fragmentation FLOAT, @Action NVARCHAR(20)
DECLARE @SQL NVARCHAR(500)

OPEN index_cursor
FETCH NEXT FROM index_cursor INTO @TableName, @IndexName, @Fragmentation, @Action

WHILE @@FETCH_STATUS = 0
BEGIN
    IF @Action = 'REBUILD'
    BEGIN
        SET @SQL = 'ALTER INDEX ' + @IndexName + ' ON ' + @TableName + ' REBUILD WITH (ONLINE = ON)'
        PRINT 'Rebuilding: ' + @IndexName + ' (Fragmentation: ' + CAST(@Fragmentation AS VARCHAR(10)) + '%)'
    END
    ELSE IF @Action = 'REORGANIZE'
    BEGIN
        SET @SQL = 'ALTER INDEX ' + @IndexName + ' ON ' + @TableName + ' REORGANIZE'
        PRINT 'Reorganizing: ' + @IndexName + ' (Fragmentation: ' + CAST(@Fragmentation AS VARCHAR(10)) + '%)'
    END
    
    IF @SQL IS NOT NULL
    BEGIN
        EXEC sp_executesql @SQL
        SET @SQL = NULL
    END
    
    FETCH NEXT FROM index_cursor INTO @TableName, @IndexName, @Fragmentation, @Action
END

CLOSE index_cursor
DEALLOCATE index_cursor
</code></pre>
<h3>PostgreSQL Automated Maintenance</h3>
<pre><code class="language-bash">#!/bin/bash
# PostgreSQL index maintenance script

DATABASE="your_database"
REINDEX_THRESHOLD=0.2  # 20% bloat threshold

# Function to check index bloat and reindex if necessary
check_and_reindex() {
    psql -d $DATABASE -c "
    DO \$\$
    DECLARE
        idx_record RECORD;
        bloat_query TEXT := '
            SELECT 
                schemaname,
                tablename,
                indexname,
                CASE 
                    WHEN pg_relation_size(indexrelid) = 0 THEN 0
                    ELSE (pg_relation_size(indexrelid)::float / 
                          GREATEST(pg_relation_size(c.oid), 1))
                END AS bloat_ratio
            FROM pg_stat_user_indexes ui
            JOIN pg_class c ON c.relname = ui.tablename
            WHERE schemaname = ''public''
            AND pg_relation_size(indexrelid) > 1000000';  -- Only large indexes
    BEGIN
        FOR idx_record IN EXECUTE bloat_query LOOP
            IF idx_record.bloat_ratio > $REINDEX_THRESHOLD THEN
                RAISE NOTICE 'Reindexing %.% (bloat ratio: %)', 
                    idx_record.indexname, idx_record.tablename, idx_record.bloat_ratio;
                EXECUTE 'REINDEX INDEX CONCURRENTLY ' || idx_record.indexname;
            END IF;
        END LOOP;
    END
    \$\$;
    "
}

# Update table statistics
psql -d $DATABASE -c "
SELECT 'ANALYZE ' || schemaname || '.' || tablename || ';'
FROM pg_stat_user_tables 
WHERE schemaname = 'public'
" | grep ANALYZE | psql -d $DATABASE

# Check and reindex bloated indexes
check_and_reindex

echo "Index maintenance completed"
</code></pre>
<h3>MySQL Automated Maintenance</h3>
<pre><code class="language-sql">-- MySQL maintenance procedure
DELIMITER //
CREATE PROCEDURE OptimizeIndexes()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE table_name VARCHAR(255);
    DECLARE table_cursor CURSOR FOR 
        SELECT TABLE_NAME 
        FROM information_schema.TABLES 
        WHERE TABLE_SCHEMA = DATABASE() 
        AND TABLE_TYPE = 'BASE TABLE';
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;

    OPEN table_cursor;
    
    table_loop: LOOP
        FETCH table_cursor INTO table_name;
        IF done THEN
            LEAVE table_loop;
        END IF;
        
        -- Optimize table (rebuilds indexes)
        SET @sql = CONCAT('OPTIMIZE TABLE ', table_name);
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
        -- Analyze table (updates statistics)
        SET @sql = CONCAT('ANALYZE TABLE ', table_name);
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
    END LOOP;
    
    CLOSE table_cursor;
END //
DELIMITER ;

-- Schedule the procedure to run weekly
-- CREATE EVENT weekly_index_maintenance
-- ON SCHEDULE EVERY 1 WEEK
-- STARTS '2024-01-01 02:00:00'
-- DO CALL OptimizeIndexes();
</code></pre>
<h2>Real-Time Performance Monitoring</h2>
<h3>Setting Up Monitoring Dashboards</h3>
<h4>PostgreSQL with pg_stat_statements</h4>
<pre><code class="language-sql">-- Enable pg_stat_statements extension
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Configure postgresql.conf
-- shared_preload_libraries = 'pg_stat_statements'
-- pg_stat_statements.track = all

-- Query to identify slow queries using indexes
SELECT 
    query,
    calls,
    total_time / calls AS avg_time,
    rows / calls AS avg_rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
WHERE query LIKE '%your_table%'
ORDER BY total_time DESC
LIMIT 10;

-- Reset statistics
SELECT pg_stat_statements_reset();
</code></pre>
<h4>MySQL Performance Schema Monitoring</h4>
<pre><code class="language-sql">-- Monitor index usage patterns
SELECT 
    object_schema,
    object_name,
    index_name,
    count_read,
    count_write,
    sum_timer_read / count_read / 1000000000 AS avg_read_time_seconds,
    sum_timer_write / count_write / 1000000000 AS avg_write_time_seconds
FROM performance_schema.table_io_waits_summary_by_index_usage
WHERE count_read > 0 OR count_write > 0
ORDER BY count_read DESC;

-- Monitor statement performance
SELECT 
    DIGEST_TEXT,
    COUNT_STAR,
    AVG_TIMER_WAIT / 1000000000 AS avg_time_seconds,
    SUM_ROWS_EXAMINED / COUNT_STAR AS avg_rows_examined,
    SUM_ROWS_SENT / COUNT_STAR AS avg_rows_sent
FROM performance_schema.events_statements_summary_by_digest
WHERE DIGEST_TEXT IS NOT NULL
ORDER BY AVG_TIMER_WAIT DESC
LIMIT 10;
</code></pre>
<h3>Application-Level Monitoring</h3>
<h4>Python Application Monitoring</h4>
<pre><code class="language-python">import time
import logging
from functools import wraps

# Query performance decorator
def monitor_query_performance(query_name):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            # Log slow queries
            if execution_time > 1.0:  # Queries taking more than 1 second
                logging.warning(f"Slow query detected: {query_name} took {execution_time:.2f} seconds")
            
            # Store metrics for analysis
            store_performance_metric(query_name, execution_time, len(result) if result else 0)
            
            return result
        return wrapper
    return decorator

# Usage example
@monitor_query_performance("get_user_orders")
def get_user_orders(user_id, start_date, end_date):
    query = """
    SELECT * FROM orders 
    WHERE customer_id = %s 
      AND order_date BETWEEN %s AND %s
    """
    # Execute query and return results
    return execute_query(query, (user_id, start_date, end_date))

# Performance metrics storage
def store_performance_metric(query_name, execution_time, result_count):
    # Store in time-series database, logging system, or monitoring service
    metrics = {
        'query_name': query_name,
        'execution_time': execution_time,
        'result_count': result_count,
        'timestamp': time.time()
    }
    # Send to monitoring system (e.g., Prometheus, DataDog, etc.)
</code></pre>
<h4>Node.js Application Monitoring</h4>
<pre><code class="language-javascript">const queryMonitor = (queryName) => {
    return (target, propertyName, descriptor) => {
        const method = descriptor.value;
        
        descriptor.value = async function(...args) {
            const startTime = Date.now();
            
            try {
                const result = await method.apply(this, args);
                const endTime = Date.now();
                const duration = endTime - startTime;
                
                // Log performance metrics
                console.log(`Query ${queryName}: ${duration}ms`);
                
                // Send to monitoring service
                if (duration > 1000) {
                    console.warn(`Slow query detected: ${queryName} took ${duration}ms`);
                }
                
                return result;
            } catch (error) {
                console.error(`Query ${queryName} failed:`, error);
                throw error;
            }
        };
    };
};

// Usage
class OrderService {
    @queryMonitor('getUserOrders')
    async getUserOrders(userId, startDate, endDate) {
        const query = `
            SELECT * FROM orders 
            WHERE customer_id = ? 
              AND order_date BETWEEN ? AND ?
        `;
        return await this.db.query(query, [userId, startDate, endDate]);
    }
}
</code></pre>
<h2>Index Performance Alerts</h2>
<h3>Setting Up Automated Alerts</h3>
<h4>PostgreSQL Alert Script</h4>
<pre><code class="language-bash">#!/bin/bash
# PostgreSQL performance alert script

DATABASE="your_database"
SLOW_QUERY_THRESHOLD=5.0  # seconds
UNUSED_INDEX_SIZE_THRESHOLD=100  # MB

# Check for slow queries
SLOW_QUERIES=$(psql -d $DATABASE -t -c "
SELECT COUNT(*) 
FROM pg_stat_statements 
WHERE mean_time > $SLOW_QUERY_THRESHOLD * 1000  -- Convert to milliseconds
")

if [ "$SLOW_QUERIES" -gt 0 ]; then
    echo "Alert: $SLOW_QUERIES slow queries detected"
    # Send alert (email, Slack, etc.)
fi

# Check for large unused indexes
UNUSED_INDEXES=$(psql -d $DATABASE -t -c "
SELECT COUNT(*) 
FROM pg_stat_user_indexes 
WHERE idx_scan = 0 
  AND pg_relation_size(indexrelid) > $UNUSED_INDEX_SIZE_THRESHOLD * 1024 * 1024
")

if [ "$UNUSED_INDEXES" -gt 0 ]; then
    echo "Alert: $UNUSED_INDEXES large unused indexes detected"
    # Send alert
fi
</code></pre>
<h4>SQL Server Alert Setup</h4>
<pre><code class="language-sql">-- Create alert for high fragmentation
EXEC msdb.dbo.sp_add_alert
    @name = N'High Index Fragmentation',
    @message_id = 50001,
    @severity = 16,
    @enabled = 1;

-- Create custom alert job
EXEC msdb.dbo.sp_add_job
    @job_name = N'Index Health Check';

EXEC msdb.dbo.sp_add_jobstep
    @job_name = N'Index Health Check',
    @step_name = N'Check Fragmentation',
    @command = N'
    IF EXISTS (
        SELECT 1 
        FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, ''DETAILED'')
        WHERE avg_fragmentation_in_percent > 30 AND page_count > 1000
    )
    BEGIN
        RAISERROR(''High index fragmentation detected'', 16, 1)
    END';

EXEC msdb.dbo.sp_add_schedule
    @schedule_name = N'Daily Check',
    @freq_type = 4,  -- Daily
    @freq_interval = 1;

EXEC msdb.dbo.sp_attach_schedule
    @job_name = N'Index Health Check',
    @schedule_name = N'Daily Check';
</code></pre>
<h2>Best Practices for Production Monitoring</h2>
<h3>Monitoring Strategy</h3>
<ol>
<li><strong>Baseline Performance</strong>: Establish performance baselines before implementing changes</li>
<li><strong>Continuous Monitoring</strong>: Set up automated monitoring for key metrics</li>
<li><strong>Proactive Alerts</strong>: Configure alerts for performance degradation</li>
<li><strong>Regular Reviews</strong>: Schedule periodic index usage reviews</li>
</ol>
<h3>Key Metrics to Track</h3>
<ol>
<li><strong>Query Performance</strong>: Execution time, rows examined vs. returned</li>
<li><strong>Index Usage</strong>: Seek/scan ratios, usage frequency</li>
<li><strong>Resource Utilization</strong>: CPU, memory, I/O impact</li>
<li><strong>Fragmentation Levels</strong>: Physical disorder of index pages</li>
</ol>
<h3>Maintenance Windows</h3>
<ol>
<li><strong>Schedule Regular Maintenance</strong>: Plan index rebuilds during low-activity periods</li>
<li><strong>Test Changes</strong>: Always test index changes in staging environments</li>
<li><strong>Monitor After Changes</strong>: Watch performance closely after index modifications</li>
<li><strong>Rollback Plans</strong>: Have procedures to quickly revert problematic changes</li>
</ol>
<h2>Next Steps</h2>
<p>In Part 6, we'll explore advanced indexing techniques including partitioned indexes, columnar indexes, and specialized indexing strategies for big data and analytics workloads.</p>
12:T46a4,<h2>Advanced Indexing Techniques</h2>
<h3>Partitioned Indexes</h3>
<p>Partitioned indexes split large indexes across multiple physical structures, improving performance and manageability for very large tables.</p>
<h4>PostgreSQL Table Partitioning with Indexes</h4>
<pre><code class="language-sql">-- Create partitioned table by date range
CREATE TABLE sales_partitioned (
    id BIGSERIAL,
    customer_id INT,
    sale_date DATE,
    amount DECIMAL(10,2),
    product_id INT,
    region VARCHAR(50)
) PARTITION BY RANGE (sale_date);

-- Create partitions for different date ranges
CREATE TABLE sales_2023 PARTITION OF sales_partitioned
    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');

CREATE TABLE sales_2024 PARTITION OF sales_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

-- Create indexes on each partition
CREATE INDEX idx_sales_2023_customer ON sales_2023(customer_id, sale_date);
CREATE INDEX idx_sales_2024_customer ON sales_2024(customer_id, sale_date);

-- Create global index across all partitions
CREATE INDEX idx_sales_partitioned_customer ON sales_partitioned(customer_id);

-- Queries automatically use partition pruning
EXPLAIN (ANALYZE, BUFFERS) 
SELECT * FROM sales_partitioned 
WHERE sale_date BETWEEN '2024-01-01' AND '2024-01-31'
  AND customer_id = 1000;
</code></pre>
<h4>SQL Server Partitioned Indexes</h4>
<pre><code class="language-sql">-- Create partition function and scheme
CREATE PARTITION FUNCTION sales_date_function (DATE)
AS RANGE RIGHT FOR VALUES (
    '2023-01-01', '2023-04-01', '2023-07-01', '2023-10-01',
    '2024-01-01', '2024-04-01', '2024-07-01', '2024-10-01'
);

CREATE PARTITION SCHEME sales_date_scheme
AS PARTITION sales_date_function
TO (
    [Partition1], [Partition2], [Partition3], [Partition4],
    [Partition5], [Partition6], [Partition7], [Partition8]
);

-- Create partitioned table
CREATE TABLE sales_partitioned (
    id BIGINT IDENTITY(1,1),
    customer_id INT,
    sale_date DATE,
    amount DECIMAL(10,2),
    product_id INT,
    region VARCHAR(50),
    CONSTRAINT PK_sales_partitioned PRIMARY KEY (id, sale_date)
) ON sales_date_scheme(sale_date);

-- Create partitioned index
CREATE INDEX idx_sales_customer_partitioned
ON sales_partitioned(customer_id, sale_date)
ON sales_date_scheme(sale_date);
</code></pre>
<h3>Columnar Indexes</h3>
<p>Columnar indexes store data column-wise rather than row-wise, providing exceptional performance for analytical queries.</p>
<h4>SQL Server Columnstore Indexes</h4>
<pre><code class="language-sql">-- Create clustered columnstore index for OLAP workload
CREATE TABLE fact_sales (
    sale_id BIGINT,
    customer_id INT,
    product_id INT,
    sale_date DATE,
    quantity INT,
    unit_price DECIMAL(10,2),
    total_amount DECIMAL(12,2),
    store_id INT,
    region_id INT
);

-- Clustered columnstore index (entire table stored as columnstore)
CREATE CLUSTERED COLUMNSTORE INDEX cci_fact_sales ON fact_sales;

-- Non-clustered columnstore index (for mixed workloads)
CREATE TABLE sales_mixed (
    id INT IDENTITY(1,1) PRIMARY KEY,
    customer_id INT,
    product_id INT,
    sale_date DATE,
    amount DECIMAL(10,2),
    created_at DATETIME2 DEFAULT GETDATE()
);

-- Non-clustered columnstore for analytics
CREATE NONCLUSTERED COLUMNSTORE INDEX ncci_sales_analytics
ON sales_mixed(customer_id, product_id, sale_date, amount);

-- Analytical query performance
SELECT 
    YEAR(sale_date) as sale_year,
    region_id,
    SUM(total_amount) as total_sales,
    AVG(total_amount) as avg_sale,
    COUNT(*) as transaction_count
FROM fact_sales
WHERE sale_date >= '2023-01-01'
GROUP BY YEAR(sale_date), region_id
ORDER BY total_sales DESC;
</code></pre>
<h4>PostgreSQL Columnar Storage (with Citus)</h4>
<pre><code class="language-sql">-- Using columnar extension for analytics
CREATE EXTENSION columnar;

-- Create columnar table for analytics
CREATE TABLE analytics_sales (
    customer_id INT,
    product_category VARCHAR(50),
    sale_date DATE,
    amount DECIMAL(10,2),
    quantity INT,
    region VARCHAR(50)
) USING columnar;

-- Analytical queries perform much better
SELECT 
    product_category,
    region,
    DATE_TRUNC('month', sale_date) as month,
    SUM(amount) as total_sales,
    SUM(quantity) as total_quantity
FROM analytics_sales
WHERE sale_date >= '2023-01-01'
GROUP BY product_category, region, DATE_TRUNC('month', sale_date)
ORDER BY total_sales DESC;
</code></pre>
<h3>Expression-Based and Functional Indexes</h3>
<p>Create indexes on computed values and function results for complex query patterns.</p>
<h4>PostgreSQL Functional Indexes</h4>
<pre><code class="language-sql">-- Index on function result
CREATE INDEX idx_users_lower_email ON users(lower(email));
CREATE INDEX idx_products_profit_margin ON products((price - cost) / price * 100);

-- Index on extracted date parts
CREATE INDEX idx_orders_year_month ON orders(EXTRACT(YEAR FROM order_date), EXTRACT(MONTH FROM order_date));

-- Complex expression index
CREATE INDEX idx_customer_full_name ON customers((first_name || ' ' || last_name));

-- JSONB functional indexes
CREATE INDEX idx_user_preferences_theme 
ON users((preferences->>'theme')) 
WHERE preferences->>'theme' IS NOT NULL;

-- Trigram indexes for fuzzy text search
CREATE EXTENSION pg_trgm;
CREATE INDEX idx_products_name_trgm ON products USING gin(name gin_trgm_ops);

-- Usage examples
SELECT * FROM users WHERE lower(email) = 'john@example.com';
SELECT * FROM products WHERE (price - cost) / price * 100 > 50;
SELECT * FROM products WHERE name % 'wireless headphne';  -- Fuzzy match
</code></pre>
<h4>Oracle Function-Based Indexes</h4>
<pre><code class="language-sql">-- Function-based indexes
CREATE INDEX idx_employees_upper_last_name ON employees(UPPER(last_name));
CREATE INDEX idx_orders_year ON orders(EXTRACT(YEAR FROM order_date));

-- Case-insensitive searches
CREATE INDEX idx_products_case_insensitive ON products(UPPER(product_name));

-- Complex calculations
CREATE INDEX idx_inventory_turnover ON inventory((units_sold / average_inventory) * 365);

-- Virtual columns with indexes (Oracle 11g+)
ALTER TABLE products ADD (profit_margin GENERATED ALWAYS AS ((price - cost) / price * 100));
CREATE INDEX idx_products_profit_margin ON products(profit_margin);
</code></pre>
<h3>Specialized Index Types</h3>
<h4>Graph Database Indexing (Neo4j)</h4>
<pre><code class="language-cypher">// Create node indexes
CREATE INDEX customer_email_idx FOR (c:Customer) ON (c.email);
CREATE INDEX product_sku_idx FOR (p:Product) ON (p.sku);
CREATE INDEX order_date_idx FOR (o:Order) ON (o.date);

// Composite indexes
CREATE INDEX customer_region_status FOR (c:Customer) ON (c.region, c.status);

// Full-text indexes
CREATE FULLTEXT INDEX product_search FOR (p:Product) ON EACH [p.name, p.description];

// Range indexes for relationships
CREATE RANGE INDEX purchase_amount FOR ()-[r:PURCHASED]-() ON (r.amount);

// Query using indexes
MATCH (c:Customer {email: 'john@example.com'})-[r:PURCHASED]->(p:Product)
WHERE r.amount > 100
RETURN c.name, p.name, r.amount;

// Full-text search
CALL db.index.fulltext.queryNodes('product_search', 'wireless bluetooth') 
YIELD node, score
RETURN node.name, node.description, score;
</code></pre>
<h4>Time-Series Database Indexing (InfluxDB)</h4>
<pre><code class="language-sql">-- InfluxDB automatically creates indexes on tags
-- Tags are indexed, fields are not

-- Example schema design
-- Measurement: cpu_usage
-- Tags: host, region, environment (automatically indexed)
-- Fields: usage_percent, load_average (not indexed)

-- Query using tag indexes (fast)
SELECT mean(usage_percent) 
FROM cpu_usage 
WHERE host = 'server-01' 
  AND region = 'us-west' 
  AND time >= now() - 1h 
GROUP BY time(5m);

-- Query on field (slower, requires scan)
SELECT * FROM cpu_usage WHERE usage_percent > 90;

-- Best practices for time-series indexing:
-- 1. Use tags for dimensions you filter/group by
-- 2. Keep tag cardinality reasonable (&#x3C; 1M unique combinations)
-- 3. Use fields for measured values
-- 4. Design retention policies for old data
</code></pre>
<h3>Vector Indexes for AI/ML Workloads</h3>
<h4>PostgreSQL with pgvector</h4>
<pre><code class="language-sql">-- Install pgvector extension
CREATE EXTENSION vector;

-- Create table with vector column
CREATE TABLE document_embeddings (
    id BIGSERIAL PRIMARY KEY,
    document_id INT,
    title TEXT,
    content_vector vector(384),  -- 384-dimensional embeddings
    created_at TIMESTAMP DEFAULT NOW()
);

-- Create HNSW index for fast similarity search
CREATE INDEX idx_embeddings_hnsw 
ON document_embeddings 
USING hnsw (content_vector vector_cosine_ops);

-- Alternative: IVFFlat index
CREATE INDEX idx_embeddings_ivf 
ON document_embeddings 
USING ivfflat (content_vector vector_cosine_ops) 
WITH (lists = 100);

-- Similarity search queries
SELECT 
    document_id,
    title,
    content_vector &#x3C;=> '[0.1, 0.2, 0.3, ...]'::vector AS distance
FROM document_embeddings
ORDER BY content_vector &#x3C;=> '[0.1, 0.2, 0.3, ...]'::vector
LIMIT 10;

-- K-nearest neighbors with filters
SELECT 
    document_id,
    title,
    content_vector &#x3C;-> '[0.1, 0.2, 0.3, ...]'::vector AS distance
FROM document_embeddings
WHERE created_at >= '2024-01-01'
ORDER BY content_vector &#x3C;-> '[0.1, 0.2, 0.3, ...]'::vector
LIMIT 5;
</code></pre>
<h4>Elasticsearch Vector Search</h4>
<pre><code class="language-json">// Create index mapping with dense vector field
PUT /documents
{
  "mappings": {
    "properties": {
      "title": { "type": "text" },
      "content": { "type": "text" },
      "embedding": {
        "type": "dense_vector",
        "dims": 384,
        "index": true,
        "similarity": "cosine"
      },
      "created_at": { "type": "date" }
    }
  }
}

// Index document with vector
POST /documents/_doc/1
{
  "title": "Machine Learning Basics",
  "content": "Introduction to machine learning concepts...",
  "embedding": [0.1, 0.2, 0.3, ...],
  "created_at": "2024-01-15"
}

// Vector similarity search
GET /documents/_search
{
  "knn": {
    "field": "embedding",
    "query_vector": [0.1, 0.2, 0.3, ...],
    "k": 10,
    "num_candidates": 100
  },
  "filter": {
    "range": {
      "created_at": {
        "gte": "2024-01-01"
      }
    }
  }
}
</code></pre>
<h2>Big Data Indexing Strategies</h2>
<h3>Apache Spark with Delta Lake</h3>
<pre><code class="language-scala">// Create Delta table with optimized layout
import io.delta.tables._

// Create partitioned Delta table
spark.sql("""
CREATE TABLE sales_delta (
    customer_id LONG,
    product_id LONG,
    sale_date DATE,
    amount DECIMAL(10,2),
    region STRING
) 
USING DELTA
PARTITIONED BY (region, date_format(sale_date, 'yyyy-MM'))
""")

// Z-ORDER optimization for multi-dimensional clustering
spark.sql("OPTIMIZE sales_delta ZORDER BY (customer_id, product_id)")

// Data skipping with statistics
spark.sql("ANALYZE TABLE sales_delta COMPUTE STATISTICS FOR ALL COLUMNS")

// Bloom filters for high-cardinality columns
spark.sql("""
ALTER TABLE sales_delta 
SET TBLPROPERTIES (
    'delta.bloomFilter.customer_id' = 'true',
    'delta.bloomFilter.product_id' = 'true'
)
""")
</code></pre>
<h3>Apache Iceberg Indexing</h3>
<pre><code class="language-sql">-- Create Iceberg table with hidden partitioning
CREATE TABLE sales_iceberg (
    customer_id BIGINT,
    product_id BIGINT,
    sale_date DATE,
    amount DECIMAL(10,2),
    region STRING
) 
USING iceberg
PARTITIONED BY (bucket(16, customer_id), days(sale_date));

-- Iceberg automatically maintains partition statistics
-- Query planning uses these statistics for pruning

-- Sort order for better clustering
ALTER TABLE sales_iceberg WRITE ORDERED BY (customer_id, sale_date);
</code></pre>
<h3>ClickHouse Specialized Indexes</h3>
<pre><code class="language-sql">-- Primary key acts as sparse index
CREATE TABLE events (
    user_id UInt64,
    event_time DateTime,
    event_type String,
    page_url String,
    session_id String
) 
ENGINE = MergeTree()
ORDER BY (user_id, event_time);

-- Skip indexes for non-primary key columns
ALTER TABLE events ADD INDEX idx_event_type event_type TYPE set(100) GRANULARITY 4;
ALTER TABLE events ADD INDEX idx_page_url page_url TYPE bloom_filter(0.01) GRANULARITY 1;

-- Projection for pre-aggregated data
ALTER TABLE events ADD PROJECTION daily_stats (
    SELECT 
        user_id,
        toDate(event_time) as date,
        event_type,
        count()
    GROUP BY user_id, date, event_type
);

-- Materialize the projection
ALTER TABLE events MATERIALIZE PROJECTION daily_stats;
</code></pre>
<h2>Index Design for Specific Workloads</h2>
<h3>OLTP (Online Transaction Processing) Optimization</h3>
<pre><code class="language-sql">-- Optimize for frequent point lookups and small range scans
-- High concurrency, low latency requirements

-- Order processing system indexes
CREATE TABLE orders_oltp (
    order_id BIGINT PRIMARY KEY,
    customer_id INT,
    order_date TIMESTAMP,
    status VARCHAR(20),
    total_amount DECIMAL(10,2),
    shipping_address_id INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- OLTP-optimized indexes
CREATE INDEX idx_orders_customer_status ON orders_oltp(customer_id, status);  -- Customer order lookup
CREATE INDEX idx_orders_date_status ON orders_oltp(order_date, status);       -- Date range queries
CREATE INDEX idx_orders_status_updated ON orders_oltp(status, updated_at);    -- Status monitoring
CREATE UNIQUE INDEX idx_orders_customer_date ON orders_oltp(customer_id, order_date, order_id);  -- Avoid duplicates

-- Covering index for order summary
CREATE INDEX idx_orders_customer_covering ON orders_oltp(customer_id) 
INCLUDE (order_date, status, total_amount);  -- SQL Server syntax
</code></pre>
<h3>OLAP (Online Analytical Processing) Optimization</h3>
<pre><code class="language-sql">-- Optimize for complex aggregations and analytical queries
-- Lower concurrency, higher latency acceptable

-- Sales analytics table
CREATE TABLE sales_olap (
    sale_id BIGINT,
    customer_id INT,
    product_id INT,
    category_id INT,
    sale_date DATE,
    quantity INT,
    unit_price DECIMAL(10,2),
    total_amount DECIMAL(12,2),
    store_id INT,
    region_id INT,
    salesperson_id INT
);

-- OLAP-optimized indexes (wider, covering more columns)
CREATE INDEX idx_sales_time_hierarchy ON sales_olap(sale_date, category_id, region_id, store_id);
CREATE INDEX idx_sales_product_analysis ON sales_olap(product_id, category_id, sale_date);
CREATE INDEX idx_sales_customer_behavior ON sales_olap(customer_id, sale_date, product_id);

-- Columnstore index for analytics (SQL Server)
CREATE NONCLUSTERED COLUMNSTORE INDEX ncci_sales_analytics 
ON sales_olap(sale_date, customer_id, product_id, category_id, quantity, total_amount, region_id);

-- Aggregate tables with appropriate indexes
CREATE TABLE sales_daily_summary (
    sale_date DATE,
    category_id INT,
    region_id INT,
    total_sales DECIMAL(15,2),
    total_quantity INT,
    transaction_count INT,
    PRIMARY KEY (sale_date, category_id, region_id)
);
</code></pre>
<h3>Hybrid Workload (HTAP) Optimization</h3>
<pre><code class="language-sql">-- Balance between OLTP and OLAP requirements
-- Use read replicas or specialized engines

-- Main OLTP table with minimal indexes
CREATE TABLE transactions_htap (
    transaction_id BIGINT PRIMARY KEY,
    account_id INT,
    transaction_date TIMESTAMP,
    amount DECIMAL(12,2),
    transaction_type VARCHAR(20),
    description TEXT,
    status VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- OLTP indexes (lean and focused)
CREATE INDEX idx_transactions_account_date ON transactions_htap(account_id, transaction_date);
CREATE INDEX idx_transactions_status ON transactions_htap(status) WHERE status != 'completed';

-- Analytical read replica with additional indexes
-- (This could be a separate analytical database)
CREATE INDEX idx_transactions_analytics_time ON transactions_htap(transaction_date, transaction_type, amount);
CREATE INDEX idx_transactions_analytics_type ON transactions_htap(transaction_type, transaction_date, account_id);

-- Use database-specific features for HTAP
-- SQL Server: In-Memory OLTP with columnstore
-- MySQL: HeatWave analytics engine
-- PostgreSQL: Parallel query execution
-- Oracle: In-Memory column store
</code></pre>
<h2>Performance Optimization Patterns</h2>
<h3>Index Design Principles for Scale</h3>
<ol>
<li><strong>Minimize Index Count</strong>: Each index has maintenance overhead</li>
<li><strong>Maximize Index Utilization</strong>: Design for multiple query patterns</li>
<li><strong>Consider Data Distribution</strong>: Account for skewed data</li>
<li><strong>Plan for Growth</strong>: Design for future data volumes</li>
</ol>
<h3>Advanced Optimization Techniques</h3>
<pre><code class="language-sql">-- Filtered indexes for skewed data
CREATE INDEX idx_orders_recent ON orders(customer_id, order_date) 
WHERE order_date >= '2024-01-01';

-- Partial unique indexes
CREATE UNIQUE INDEX idx_users_active_email ON users(email) 
WHERE status = 'active';

-- Conditional indexes for sparse data
CREATE INDEX idx_products_discount ON products(discount_percentage) 
WHERE discount_percentage > 0;

-- Descending indexes for recent-first queries
CREATE INDEX idx_logs_timestamp_desc ON application_logs(timestamp DESC);
</code></pre>
<h2>Next Steps</h2>
<p>In Part 7, we'll explore client-side optimization strategies including connection pooling, query caching, application-level indexing, and CDN optimization techniques to complement database indexing strategies.</p>
13:T744c,<h2>Client-Side Database Optimization Strategies</h2>
<p>While database indexes optimize server-side performance, client-side optimizations are equally crucial for overall application performance. This comprehensive guide covers connection management, caching strategies, query optimization, and application-level techniques.</p>
<h3>Connection Pooling and Management</h3>
<h4>Connection Pool Configuration</h4>
<pre><code class="language-javascript">// Node.js with PostgreSQL (pg-pool)
const { Pool } = require('pg');

const pool = new Pool({
    user: 'username',
    host: 'localhost',
    database: 'myapp',
    password: 'password',
    port: 5432,
    
    // Connection pool settings
    min: 5,                    // Minimum connections
    max: 20,                   // Maximum connections
    idleTimeoutMillis: 30000,  // Close idle connections after 30s
    connectionTimeoutMillis: 2000,  // Timeout when getting connection
    
    // Performance optimizations
    keepAlive: true,
    keepAliveInitialDelayMillis: 0,
    
    // Query timeout
    query_timeout: 10000,      // 10 second query timeout
    statement_timeout: 10000   // 10 second statement timeout
});

// Connection with retry logic
async function getConnectionWithRetry(maxRetries = 3) {
    for (let i = 0; i &#x3C; maxRetries; i++) {
        try {
            return await pool.connect();
        } catch (error) {
            if (i === maxRetries - 1) throw error;
            await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));
        }
    }
}

// Graceful shutdown
process.on('SIGINT', async () => {
    console.log('Closing connection pool...');
    await pool.end();
    process.exit(0);
});
</code></pre>
<pre><code class="language-python"># Python with SQLAlchemy connection pooling
from sqlalchemy import create_engine, pool
from sqlalchemy.pool import QueuePool
import logging

# Configure connection pool
engine = create_engine(
    'postgresql://user:password@localhost/myapp',
    
    # Pool configuration
    poolclass=QueuePool,
    pool_size=10,              # Number of connections to maintain
    max_overflow=20,           # Additional connections when pool is full
    pool_recycle=3600,         # Recycle connections after 1 hour
    pool_pre_ping=True,        # Validate connections before use
    
    # Connection timeout
    connect_args={
        'connect_timeout': 10,
        'application_name': 'myapp'
    },
    
    # Logging
    echo=False  # Set to True for query logging
)

# Connection context manager
from contextlib import contextmanager

@contextmanager
def get_db_connection():
    connection = engine.connect()
    try:
        yield connection
    except Exception as e:
        connection.rollback()
        raise
    finally:
        connection.close()

# Usage
def get_user_orders(user_id):
    with get_db_connection() as conn:
        result = conn.execute(
            "SELECT * FROM orders WHERE customer_id = %s",
            (user_id,)
        )
        return result.fetchall()
</code></pre>
<h4>Connection Pool Monitoring</h4>
<pre><code class="language-javascript">// Node.js connection pool monitoring
function monitorConnectionPool(pool) {
    setInterval(() => {
        console.log('Pool Stats:', {
            totalCount: pool.totalCount,
            idleCount: pool.idleCount,
            waitingCount: pool.waitingCount
        });
        
        // Alert if pool is under pressure
        if (pool.waitingCount > 0) {
            console.warn('Connection pool under pressure!');
        }
        
        // Alert if too many idle connections
        if (pool.idleCount > pool.options.max * 0.8) {
            console.warn('Too many idle connections');
        }
    }, 30000); // Check every 30 seconds
}

monitorConnectionPool(pool);
</code></pre>
<h3>Query Result Caching</h3>
<h4>Redis-Based Query Caching</h4>
<pre><code class="language-javascript">// Node.js with Redis caching
const redis = require('redis');
const client = redis.createClient({
    host: 'localhost',
    port: 6379,
    retry_strategy: (options) => {
        if (options.error &#x26;&#x26; options.error.code === 'ECONNREFUSED') {
            return new Error('Redis server connection refused');
        }
        if (options.total_retry_time > 1000 * 60 * 60) {
            return new Error('Retry time exhausted');
        }
        return Math.min(options.attempt * 100, 3000);
    }
});

class QueryCache {
    constructor(redisClient, defaultTTL = 300) {
        this.redis = redisClient;
        this.defaultTTL = defaultTTL;
    }
    
    // Generate cache key from query and parameters
    generateCacheKey(query, params = []) {
        const crypto = require('crypto');
        const keyData = query + JSON.stringify(params);
        return `query_cache:${crypto.createHash('md5').update(keyData).digest('hex')}`;
    }
    
    // Get cached result
    async getCachedResult(query, params = []) {
        const cacheKey = this.generateCacheKey(query, params);
        try {
            const cached = await this.redis.get(cacheKey);
            return cached ? JSON.parse(cached) : null;
        } catch (error) {
            console.error('Cache retrieval error:', error);
            return null;
        }
    }
    
    // Cache query result
    async setCachedResult(query, params = [], result, ttl = null) {
        const cacheKey = this.generateCacheKey(query, params);
        const expiration = ttl || this.defaultTTL;
        
        try {
            await this.redis.setex(cacheKey, expiration, JSON.stringify(result));
        } catch (error) {
            console.error('Cache storage error:', error);
        }
    }
    
    // Invalidate cache by pattern
    async invalidatePattern(pattern) {
        try {
            const keys = await this.redis.keys(`query_cache:${pattern}`);
            if (keys.length > 0) {
                await this.redis.del(keys);
            }
        } catch (error) {
            console.error('Cache invalidation error:', error);
        }
    }
}

// Usage example
const queryCache = new QueryCache(client);

async function getUserOrders(userId) {
    const query = "SELECT * FROM orders WHERE customer_id = $1 ORDER BY order_date DESC";
    const params = [userId];
    
    // Try cache first
    let result = await queryCache.getCachedResult(query, params);
    
    if (!result) {
        // Cache miss - execute query
        const dbResult = await pool.query(query, params);
        result = dbResult.rows;
        
        // Cache for 5 minutes
        await queryCache.setCachedResult(query, params, result, 300);
    }
    
    return result;
}

// Cache invalidation on data changes
async function createOrder(orderData) {
    const result = await pool.query(
        "INSERT INTO orders (...) VALUES (...) RETURNING *",
        orderData
    );
    
    // Invalidate related caches
    await queryCache.invalidatePattern(`*customer_id*${orderData.customer_id}*`);
    
    return result.rows[0];
}
</code></pre>
<h4>Application-Level Caching</h4>
<pre><code class="language-python"># Python with in-memory caching using functools.lru_cache
from functools import lru_cache, wraps
import time
import hashlib
import json

class TTLCache:
    def __init__(self, maxsize=128, ttl=300):
        self.cache = {}
        self.timestamps = {}
        self.maxsize = maxsize
        self.ttl = ttl
    
    def get(self, key):
        if key in self.cache:
            if time.time() - self.timestamps[key] &#x3C; self.ttl:
                return self.cache[key]
            else:
                # Expired
                del self.cache[key]
                del self.timestamps[key]
        return None
    
    def set(self, key, value):
        # Implement LRU eviction if needed
        if len(self.cache) >= self.maxsize:
            oldest_key = min(self.timestamps.keys(), key=self.timestamps.get)
            del self.cache[oldest_key]
            del self.timestamps[oldest_key]
        
        self.cache[key] = value
        self.timestamps[key] = time.time()

# Cache decorator
def cached_query(ttl=300, maxsize=128):
    cache = TTLCache(maxsize, ttl)
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key from function args
            key_data = json.dumps((args, sorted(kwargs.items())), default=str)
            cache_key = hashlib.md5(key_data.encode()).hexdigest()
            
            # Try cache first
            result = cache.get(cache_key)
            if result is not None:
                return result
            
            # Cache miss - execute function
            result = func(*args, **kwargs)
            cache.set(cache_key, result)
            
            return result
        return wrapper
    return decorator

# Usage
@cached_query(ttl=600, maxsize=100)  # Cache for 10 minutes
def get_product_details(product_id):
    with get_db_connection() as conn:
        result = conn.execute(
            "SELECT * FROM products WHERE id = %s",
            (product_id,)
        )
        return result.fetchone()

@cached_query(ttl=300)  # Cache for 5 minutes
def get_category_products(category_id, limit=10):
    with get_db_connection() as conn:
        result = conn.execute(
            """
            SELECT p.*, c.name as category_name 
            FROM products p 
            JOIN categories c ON p.category_id = c.id 
            WHERE p.category_id = %s 
            ORDER BY p.created_at DESC 
            LIMIT %s
            """,
            (category_id, limit)
        )
        return result.fetchall()
</code></pre>
<h3>Lazy Loading and Pagination</h3>
<h4>Cursor-Based Pagination</h4>
<pre><code class="language-javascript">// Efficient cursor-based pagination
class CursorPaginator {
    constructor(pool) {
        this.pool = pool;
    }
    
    async getPage(table, orderBy, limit = 20, cursor = null, filters = {}) {
        let query = `SELECT * FROM ${table}`;
        let params = [];
        let paramIndex = 1;
        
        // Add filters
        const filterClauses = [];
        for (const [column, value] of Object.entries(filters)) {
            filterClauses.push(`${column} = $${paramIndex++}`);
            params.push(value);
        }
        
        // Add cursor condition
        if (cursor) {
            filterClauses.push(`${orderBy} > $${paramIndex++}`);
            params.push(cursor);
        }
        
        if (filterClauses.length > 0) {
            query += ` WHERE ${filterClauses.join(' AND ')}`;
        }
        
        query += ` ORDER BY ${orderBy} LIMIT $${paramIndex}`;
        params.push(limit + 1); // Fetch one extra to determine if there's a next page
        
        const result = await this.pool.query(query, params);
        const hasNextPage = result.rows.length > limit;
        const items = hasNextPage ? result.rows.slice(0, -1) : result.rows;
        
        return {
            items,
            hasNextPage,
            nextCursor: hasNextPage ? items[items.length - 1][orderBy] : null
        };
    }
}

// Usage
const paginator = new CursorPaginator(pool);

async function getUserOrdersPage(userId, cursor = null) {
    return await paginator.getPage(
        'orders',
        'created_at',
        20,
        cursor,
        { customer_id: userId }
    );
}
</code></pre>
<h4>Lazy Loading with Batch Fetching</h4>
<pre><code class="language-javascript">// DataLoader for batching and caching
const DataLoader = require('dataloader');

// Batch function to load multiple users at once
async function batchLoadUsers(userIds) {
    const query = 'SELECT * FROM users WHERE id = ANY($1)';
    const result = await pool.query(query, [userIds]);
    
    // Return results in the same order as input
    const userMap = new Map(result.rows.map(user => [user.id, user]));
    return userIds.map(id => userMap.get(id) || null);
}

// Create DataLoader instance
const userLoader = new DataLoader(batchLoadUsers, {
    cache: true,
    maxBatchSize: 100,
    batchScheduleFn: callback => setTimeout(callback, 10) // Batch within 10ms
});

// Batch function for user orders
async function batchLoadUserOrders(userIds) {
    const query = `
        SELECT customer_id, json_agg(
            json_build_object(
                'id', id,
                'order_date', order_date,
                'total', total_amount
            ) ORDER BY order_date DESC
        ) as orders
        FROM orders 
        WHERE customer_id = ANY($1) 
        GROUP BY customer_id
    `;
    
    const result = await pool.query(query, [userIds]);
    const orderMap = new Map(result.rows.map(row => [row.customer_id, row.orders]));
    
    return userIds.map(id => orderMap.get(id) || []);
}

const userOrdersLoader = new DataLoader(batchLoadUserOrders);

// Usage in resolvers or route handlers
async function handleUserDetails(req, res) {
    const userId = req.params.userId;
    
    // These will be batched if called within the same event loop tick
    const user = await userLoader.load(userId);
    const orders = await userOrdersLoader.load(userId);
    
    res.json({ user, orders });
}
</code></pre>
<h3>Query Optimization Techniques</h3>
<h4>Prepared Statements</h4>
<pre><code class="language-javascript">// Node.js prepared statements
class PreparedStatements {
    constructor(pool) {
        this.pool = pool;
        this.statements = new Map();
    }
    
    async prepare(name, query) {
        if (!this.statements.has(name)) {
            const client = await this.pool.connect();
            try {
                await client.query(`PREPARE ${name} AS ${query}`);
                this.statements.set(name, query);
            } finally {
                client.release();
            }
        }
    }
    
    async execute(name, params = []) {
        const client = await this.pool.connect();
        try {
            return await client.query(`EXECUTE ${name}(${params.map((_, i) => `$${i + 1}`).join(',')})`, params);
        } finally {
            client.release();
        }
    }
}

// Usage
const preparedStatements = new PreparedStatements(pool);

// Prepare frequently used queries
await preparedStatements.prepare('get_user_orders', 
    'SELECT * FROM orders WHERE customer_id = $1 ORDER BY order_date DESC'
);

await preparedStatements.prepare('get_product_by_sku',
    'SELECT * FROM products WHERE sku = $1'
);

// Execute prepared statements
const orders = await preparedStatements.execute('get_user_orders', [userId]);
const product = await preparedStatements.execute('get_product_by_sku', [sku]);
</code></pre>
<h4>Query Builder Optimization</h4>
<pre><code class="language-javascript">// Knex.js query builder with optimizations
const knex = require('knex')({
    client: 'postgresql',
    connection: {
        host: 'localhost',
        user: 'username',
        password: 'password',
        database: 'myapp'
    },
    pool: {
        min: 2,
        max: 10
    },
    // Enable query debugging
    debug: process.env.NODE_ENV === 'development'
});

class OrderService {
    // Optimized query with selective fields
    async getUserOrders(userId, options = {}) {
        const {
            limit = 20,
            offset = 0,
            status = null,
            startDate = null,
            endDate = null,
            includeItems = false
        } = options;
        
        let query = knex('orders')
            .select([
                'orders.id',
                'orders.order_date',
                'orders.status',
                'orders.total_amount'
            ])
            .where('orders.customer_id', userId)
            .orderBy('orders.order_date', 'desc')
            .limit(limit)
            .offset(offset);
        
        // Add optional filters
        if (status) {
            query = query.where('orders.status', status);
        }
        
        if (startDate) {
            query = query.where('orders.order_date', '>=', startDate);
        }
        
        if (endDate) {
            query = query.where('orders.order_date', '&#x3C;=', endDate);
        }
        
        // Conditional joins
        if (includeItems) {
            query = query
                .select([
                    'orders.*',
                    knex.raw(`
                        json_agg(
                            json_build_object(
                                'product_id', oi.product_id,
                                'quantity', oi.quantity,
                                'price', oi.unit_price
                            )
                        ) as items
                    `)
                ])
                .leftJoin('order_items as oi', 'orders.id', 'oi.order_id')
                .groupBy('orders.id');
        }
        
        return await query;
    }
    
    // Bulk operations
    async createMultipleOrders(orderData) {
        return await knex.transaction(async (trx) => {
            const orders = await trx('orders')
                .insert(orderData)
                .returning('*');
            
            // Batch insert order items if provided
            const orderItems = [];
            orders.forEach((order, index) => {
                if (orderData[index].items) {
                    orderData[index].items.forEach(item => {
                        orderItems.push({
                            order_id: order.id,
                            ...item
                        });
                    });
                }
            });
            
            if (orderItems.length > 0) {
                await trx('order_items').insert(orderItems);
            }
            
            return orders;
        });
    }
}
</code></pre>
<h3>CDN and Static Asset Optimization</h3>
<h4>Database-Driven CDN Invalidation</h4>
<pre><code class="language-javascript">// CDN cache invalidation service
class CDNService {
    constructor(cdnProvider, cacheService) {
        this.cdn = cdnProvider;
        this.cache = cacheService;
    }
    
    // Generate cache tags for database entities
    generateCacheTags(entityType, entityId, additionalTags = []) {
        return [
            `${entityType}:${entityId}`,
            entityType,
            ...additionalTags
        ];
    }
    
    // Invalidate CDN cache when data changes
    async invalidateOnDataChange(entityType, entityId, affectedPaths = []) {
        const tags = this.generateCacheTags(entityType, entityId);
        
        try {
            // Purge CDN cache by tags
            await this.cdn.purgeByTags(tags);
            
            // Purge specific paths if provided
            if (affectedPaths.length > 0) {
                await this.cdn.purgeByPaths(affectedPaths);
            }
            
            // Clear application cache
            await this.cache.invalidatePattern(`*${entityType}*${entityId}*`);
            
        } catch (error) {
            console.error('CDN invalidation failed:', error);
        }
    }
    
    // Smart cache headers based on data freshness
    getCacheHeaders(entityType, lastModified) {
        const now = Date.now();
        const age = now - new Date(lastModified).getTime();
        
        // Shorter cache for recently modified data
        let maxAge = 3600; // 1 hour default
        
        if (age &#x3C; 300000) { // Modified in last 5 minutes
            maxAge = 60;
        } else if (age &#x3C; 3600000) { // Modified in last hour
            maxAge = 300;
        } else if (age > 86400000) { // Modified more than 1 day ago
            maxAge = 86400; // Cache for 24 hours
        }
        
        return {
            'Cache-Control': `public, max-age=${maxAge}, s-maxage=${maxAge * 2}`,
            'ETag': `"${entityType}-${lastModified}"`,
            'Last-Modified': new Date(lastModified).toUTCString()
        };
    }
}

// Usage in API endpoints
app.get('/api/products/:id', async (req, res) => {
    const productId = req.params.id;
    
    try {
        // Get product with cache tags
        const product = await productService.getById(productId);
        
        if (!product) {
            return res.status(404).json({ error: 'Product not found' });
        }
        
        // Set cache headers
        const cacheHeaders = cdnService.getCacheHeaders('product', product.updated_at);
        res.set(cacheHeaders);
        
        // Add cache tags for CDN
        res.set('Cache-Tag', cdnService.generateCacheTags('product', productId).join(','));
        
        res.json(product);
        
    } catch (error) {
        res.status(500).json({ error: 'Internal server error' });
    }
});

// Invalidate cache on product updates
app.put('/api/products/:id', async (req, res) => {
    const productId = req.params.id;
    
    try {
        const updatedProduct = await productService.update(productId, req.body);
        
        // Invalidate related caches
        await cdnService.invalidateOnDataChange('product', productId, [
            `/api/products/${productId}`,
            `/products/${productId}`,
            '/api/products' // Product list might be affected
        ]);
        
        res.json(updatedProduct);
        
    } catch (error) {
        res.status(500).json({ error: 'Update failed' });
    }
});
</code></pre>
<h3>Read Replicas and Load Balancing</h3>
<h4>Database Read/Write Splitting</h4>
<pre><code class="language-javascript">// Database connection manager with read/write splitting
class DatabaseManager {
    constructor(config) {
        // Primary database for writes
        this.writePool = new Pool({
            ...config.primary,
            max: config.primary.maxConnections || 10
        });
        
        // Read replicas for reads
        this.readPools = config.replicas.map(replicaConfig => 
            new Pool({
                ...replicaConfig,
                max: replicaConfig.maxConnections || 15
            })
        );
        
        this.readPoolIndex = 0;
    }
    
    // Get connection for write operations
    async getWriteConnection() {
        return await this.writePool.connect();
    }
    
    // Get connection for read operations (round-robin)
    async getReadConnection() {
        const pool = this.readPools[this.readPoolIndex];
        this.readPoolIndex = (this.readPoolIndex + 1) % this.readPools.length;
        return await pool.connect();
    }
    
    // Execute read query
    async queryRead(text, params) {
        const client = await this.getReadConnection();
        try {
            return await client.query(text, params);
        } finally {
            client.release();
        }
    }
    
    // Execute write query
    async queryWrite(text, params) {
        const client = await this.getWriteConnection();
        try {
            return await client.query(text, params);
        } finally {
            client.release();
        }
    }
    
    // Transaction support (always uses primary)
    async transaction(callback) {
        const client = await this.getWriteConnection();
        
        try {
            await client.query('BEGIN');
            const result = await callback(client);
            await client.query('COMMIT');
            return result;
        } catch (error) {
            await client.query('ROLLBACK');
            throw error;
        } finally {
            client.release();
        }
    }
}

// Configuration
const dbManager = new DatabaseManager({
    primary: {
        host: 'primary-db.example.com',
        user: 'app_user',
        password: 'password',
        database: 'myapp',
        maxConnections: 10
    },
    replicas: [
        {
            host: 'replica1-db.example.com',
            user: 'app_user',
            password: 'password',
            database: 'myapp',
            maxConnections: 15
        },
        {
            host: 'replica2-db.example.com',
            user: 'app_user',
            password: 'password',
            database: 'myapp',
            maxConnections: 15
        }
    ]
});

// Service layer using read/write splitting
class UserService {
    // Read operations use replicas
    async getUser(userId) {
        const result = await dbManager.queryRead(
            'SELECT * FROM users WHERE id = $1',
            [userId]
        );
        return result.rows[0];
    }
    
    async searchUsers(criteria) {
        const result = await dbManager.queryRead(
            'SELECT * FROM users WHERE name ILIKE $1 LIMIT 50',
            [`%${criteria}%`]
        );
        return result.rows;
    }
    
    // Write operations use primary
    async createUser(userData) {
        const result = await dbManager.queryWrite(
            'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING *',
            [userData.name, userData.email]
        );
        return result.rows[0];
    }
    
    async updateUser(userId, updates) {
        return await dbManager.transaction(async (client) => {
            // All operations in transaction use primary
            const result = await client.query(
                'UPDATE users SET name = $1, email = $2, updated_at = NOW() WHERE id = $3 RETURNING *',
                [updates.name, updates.email, userId]
            );
            
            // Log the update
            await client.query(
                'INSERT INTO user_audit (user_id, action, changed_at) VALUES ($1, $2, NOW())',
                [userId, 'update']
            );
            
            return result.rows[0];
        });
    }
}
</code></pre>
<h2>Performance Monitoring and Optimization</h2>
<h3>Client-Side Performance Metrics</h3>
<pre><code class="language-javascript">// Performance monitoring middleware
class PerformanceMonitor {
    constructor() {
        this.metrics = new Map();
    }
    
    // Middleware to track query performance
    trackQuery(queryName) {
        return (req, res, next) => {
            const startTime = process.hrtime.bigint();
            
            // Override res.json to capture response time
            const originalJson = res.json;
            res.json = function(data) {
                const endTime = process.hrtime.bigint();
                const duration = Number(endTime - startTime) / 1000000; // Convert to milliseconds
                
                // Store metrics
                monitor.recordMetric(queryName, duration, req, res);
                
                return originalJson.call(this, data);
            };
            
            next();
        };
    }
    
    recordMetric(queryName, duration, req, res) {
        const metric = {
            queryName,
            duration,
            timestamp: Date.now(),
            statusCode: res.statusCode,
            userAgent: req.get('User-Agent'),
            ip: req.ip
        };
        
        // Store in time-series format
        if (!this.metrics.has(queryName)) {
            this.metrics.set(queryName, []);
        }
        
        this.metrics.get(queryName).push(metric);
        
        // Keep only last 1000 measurements per query
        const measurements = this.metrics.get(queryName);
        if (measurements.length > 1000) {
            measurements.shift();
        }
        
        // Alert on slow queries
        if (duration > 1000) {
            console.warn(`Slow query detected: ${queryName} took ${duration}ms`);
        }
    }
    
    getStats(queryName) {
        const measurements = this.metrics.get(queryName) || [];
        if (measurements.length === 0) return null;
        
        const durations = measurements.map(m => m.duration);
        const sorted = durations.sort((a, b) => a - b);
        
        return {
            count: measurements.length,
            avg: durations.reduce((sum, d) => sum + d, 0) / durations.length,
            min: sorted[0],
            max: sorted[sorted.length - 1],
            p50: sorted[Math.floor(sorted.length * 0.5)],
            p95: sorted[Math.floor(sorted.length * 0.95)],
            p99: sorted[Math.floor(sorted.length * 0.99)]
        };
    }
}

const monitor = new PerformanceMonitor();

// Usage
app.get('/api/users/:id', 
    monitor.trackQuery('get_user'),
    async (req, res) => {
        const user = await userService.getUser(req.params.id);
        res.json(user);
    }
);

// Metrics endpoint
app.get('/metrics', (req, res) => {
    const stats = {};
    for (const [queryName] of monitor.metrics) {
        stats[queryName] = monitor.getStats(queryName);
    }
    res.json(stats);
});
</code></pre>
<h2>Next Steps</h2>
<p>In Part 8 (final part), we'll explore real-world case studies and best practices, including production optimization examples, migration strategies, troubleshooting guides, and comprehensive checklists for database index optimization across different industries and use cases.</p>
14:T77c6,<h2>Real-World Case Studies</h2>
<h3>Case Study 1: E-commerce Platform Optimization</h3>
<h4>The Challenge</h4>
<p>An e-commerce platform with 10 million products and 1 million daily active users was experiencing:</p>
<ul>
<li>Product search queries taking 3-5 seconds</li>
<li>Checkout process timeouts during peak hours</li>
<li>Admin dashboard reports timing out</li>
<li>Database CPU at 90% during traffic spikes</li>
</ul>
<h4>The Solution</h4>
<p><strong>Phase 1: Critical Query Optimization</strong></p>
<pre><code class="language-sql">-- Original slow product search query
SELECT p.*, c.name as category_name, AVG(r.rating) as avg_rating
FROM products p
LEFT JOIN categories c ON p.category_id = c.id
LEFT JOIN reviews r ON p.id = r.product_id
WHERE p.name ILIKE '%wireless%'
   OR p.description ILIKE '%wireless%'
GROUP BY p.id, c.name
ORDER BY avg_rating DESC, p.created_at DESC
LIMIT 20;

-- Problem: Full table scans, expensive ILIKE operations, complex aggregations
-- Execution time: 4.2 seconds
</code></pre>
<p><strong>Optimized Approach:</strong></p>
<pre><code class="language-sql">-- Step 1: Create full-text search index
CREATE INDEX idx_products_search 
ON products 
USING gin(to_tsvector('english', name || ' ' || description));

-- Step 2: Denormalize ratings for faster access
CREATE TABLE product_ratings_cache (
    product_id INT PRIMARY KEY,
    avg_rating DECIMAL(3,2),
    review_count INT,
    last_updated TIMESTAMP DEFAULT NOW()
);

-- Trigger to maintain ratings cache
CREATE OR REPLACE FUNCTION update_product_rating_cache()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO product_ratings_cache (product_id, avg_rating, review_count)
    SELECT 
        NEW.product_id,
        AVG(rating),
        COUNT(*)
    FROM reviews 
    WHERE product_id = NEW.product_id
    GROUP BY product_id
    ON CONFLICT (product_id) 
    DO UPDATE SET 
        avg_rating = EXCLUDED.avg_rating,
        review_count = EXCLUDED.review_count,
        last_updated = NOW();
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Step 3: Optimized search query
SELECT 
    p.id,
    p.name,
    p.price,
    p.image_url,
    c.name as category_name,
    prc.avg_rating,
    prc.review_count
FROM products p
JOIN categories c ON p.category_id = c.id
LEFT JOIN product_ratings_cache prc ON p.id = prc.product_id
WHERE to_tsvector('english', p.name || ' ' || p.description) @@ to_tsquery('english', 'wireless')
ORDER BY 
    CASE WHEN prc.avg_rating IS NOT NULL THEN prc.avg_rating ELSE 0 END DESC,
    p.created_at DESC
LIMIT 20;

-- Result: Query time reduced from 4.2s to 0.08s (98% improvement)
</code></pre>
<p><strong>Phase 2: Checkout Optimization</strong></p>
<pre><code class="language-sql">-- Original checkout process issues:
-- 1. Inventory checks were slow
-- 2. Multiple round trips to database
-- 3. Lock contention during updates

-- Solution: Batch operations with proper indexing
CREATE INDEX idx_inventory_product_location ON inventory(product_id, warehouse_location);
CREATE INDEX idx_orders_processing ON orders(status, created_at) WHERE status = 'processing';

-- Optimized checkout procedure
CREATE OR REPLACE FUNCTION process_checkout(
    p_customer_id INT,
    p_items JSONB,
    p_shipping_address JSONB
) RETURNS JSON AS $$
DECLARE
    v_order_id INT;
    v_item JSONB;
    v_total DECIMAL(10,2) := 0;
    v_insufficient_stock TEXT[];
BEGIN
    -- Step 1: Validate inventory in batch
    SELECT array_agg(
        CASE 
            WHEN i.available_quantity &#x3C; (item->>'quantity')::INT 
            THEN item->>'product_id'
        END
    ) INTO v_insufficient_stock
    FROM jsonb_array_elements(p_items) AS item
    JOIN inventory i ON i.product_id = (item->>'product_id')::INT
    WHERE i.available_quantity &#x3C; (item->>'quantity')::INT;
    
    IF array_length(v_insufficient_stock, 1) > 0 THEN
        RETURN json_build_object(
            'success', false,
            'error', 'insufficient_stock',
            'products', v_insufficient_stock
        );
    END IF;
    
    -- Step 2: Create order and reserve inventory atomically
    INSERT INTO orders (customer_id, status, shipping_address, created_at)
    VALUES (p_customer_id, 'confirmed', p_shipping_address, NOW())
    RETURNING id INTO v_order_id;
    
    -- Step 3: Batch insert order items and update inventory
    INSERT INTO order_items (order_id, product_id, quantity, unit_price)
    SELECT 
        v_order_id,
        (item->>'product_id')::INT,
        (item->>'quantity')::INT,
        p.price
    FROM jsonb_array_elements(p_items) AS item
    JOIN products p ON p.id = (item->>'product_id')::INT;
    
    -- Step 4: Update inventory in batch
    UPDATE inventory 
    SET available_quantity = available_quantity - subquery.quantity
    FROM (
        SELECT 
            (item->>'product_id')::INT as product_id,
            (item->>'quantity')::INT as quantity
        FROM jsonb_array_elements(p_items) AS item
    ) AS subquery
    WHERE inventory.product_id = subquery.product_id;
    
    -- Step 5: Calculate total
    SELECT SUM(oi.quantity * oi.unit_price) INTO v_total
    FROM order_items oi
    WHERE oi.order_id = v_order_id;
    
    UPDATE orders SET total_amount = v_total WHERE id = v_order_id;
    
    RETURN json_build_object(
        'success', true,
        'order_id', v_order_id,
        'total', v_total
    );
END;
$$ LANGUAGE plpgsql;

-- Result: Checkout time reduced from 2.3s to 0.3s (87% improvement)
</code></pre>
<p><strong>Results:</strong></p>
<ul>
<li>Search performance: 98% improvement (4.2s → 0.08s)</li>
<li>Checkout performance: 87% improvement (2.3s → 0.3s)</li>
<li>Database CPU utilization: 90% → 45%</li>
<li>Peak hour success rate: 85% → 99.5%</li>
</ul>
<h3>Case Study 2: Social Media Analytics Platform</h3>
<h4>The Challenge</h4>
<p>A social media analytics platform processing 100M events/day faced:</p>
<ul>
<li>Real-time dashboard queries taking 15+ seconds</li>
<li>ETL processes blocking user queries</li>
<li>Reporting queries causing memory issues</li>
<li>Unable to scale beyond current load</li>
</ul>
<h4>The Solution</h4>
<p><strong>Phase 1: Time-Series Data Optimization</strong></p>
<pre><code class="language-sql">-- Original events table (100M+ rows)
CREATE TABLE events (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT,
    event_type VARCHAR(50),
    platform VARCHAR(20),
    timestamp TIMESTAMP,
    metadata JSONB,
    processed_at TIMESTAMP DEFAULT NOW()
);

-- Problem: Single massive table, no partitioning, slow aggregations

-- Solution: Partitioned time-series design
CREATE TABLE events_partitioned (
    id BIGINT,
    user_id BIGINT,
    event_type VARCHAR(50),
    platform VARCHAR(20),
    timestamp TIMESTAMP,
    metadata JSONB,
    processed_at TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (timestamp);

-- Create monthly partitions
CREATE TABLE events_2024_01 PARTITION OF events_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
CREATE TABLE events_2024_02 PARTITION OF events_partitioned
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');
-- ... continue for each month

-- Indexes per partition
CREATE INDEX idx_events_2024_01_user_type ON events_2024_01(user_id, event_type, timestamp);
CREATE INDEX idx_events_2024_01_platform_time ON events_2024_01(platform, timestamp);

-- Automated partition management
CREATE OR REPLACE FUNCTION create_monthly_partition(target_date DATE)
RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
    start_date DATE;
    end_date DATE;
BEGIN
    start_date := date_trunc('month', target_date);
    end_date := start_date + INTERVAL '1 month';
    partition_name := 'events_' || to_char(start_date, 'YYYY_MM');
    
    EXECUTE format('
        CREATE TABLE %I PARTITION OF events_partitioned
        FOR VALUES FROM (%L) TO (%L)',
        partition_name, start_date, end_date
    );
    
    -- Create indexes
    EXECUTE format('
        CREATE INDEX %I ON %I(user_id, event_type, timestamp)',
        'idx_' || partition_name || '_user_type', partition_name
    );
    
    EXECUTE format('
        CREATE INDEX %I ON %I(platform, timestamp)',
        'idx_' || partition_name || '_platform_time', partition_name
    );
END;
$$ LANGUAGE plpgsql;
</code></pre>
<p><strong>Phase 2: Materialized Views for Analytics</strong></p>
<pre><code class="language-sql">-- Create materialized views for common aggregations
CREATE MATERIALIZED VIEW hourly_event_stats AS
SELECT 
    date_trunc('hour', timestamp) as hour,
    platform,
    event_type,
    COUNT(*) as event_count,
    COUNT(DISTINCT user_id) as unique_users
FROM events_partitioned
WHERE timestamp >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY date_trunc('hour', timestamp), platform, event_type;

CREATE INDEX idx_hourly_stats_time_platform ON hourly_event_stats(hour, platform);

-- Automated refresh
CREATE OR REPLACE FUNCTION refresh_hourly_stats()
RETURNS VOID AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY hourly_event_stats;
END;
$$ LANGUAGE plpgsql;

-- Schedule refresh every hour
SELECT cron.schedule('refresh-hourly-stats', '0 * * * *', 'SELECT refresh_hourly_stats();');

-- Daily aggregations
CREATE MATERIALIZED VIEW daily_platform_stats AS
SELECT 
    date_trunc('day', timestamp) as day,
    platform,
    COUNT(*) as total_events,
    COUNT(DISTINCT user_id) as daily_active_users,
    COUNT(DISTINCT user_id) FILTER (WHERE event_type = 'login') as login_users
FROM events_partitioned
WHERE timestamp >= CURRENT_DATE - INTERVAL '365 days'
GROUP BY date_trunc('day', timestamp), platform;

-- Fast dashboard queries
SELECT 
    platform,
    SUM(total_events) as events_last_7_days,
    AVG(daily_active_users) as avg_daily_users
FROM daily_platform_stats
WHERE day >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY platform;

-- Result: Dashboard query time 15s → 0.2s (99% improvement)
</code></pre>
<p><strong>Phase 3: Columnar Storage for Analytics</strong></p>
<pre><code class="language-sql">-- Create columnar table for heavy analytics
-- (Using Citus columnar extension)
CREATE TABLE events_analytics (
    user_id BIGINT,
    event_type VARCHAR(50),
    platform VARCHAR(20),
    event_date DATE,
    event_hour INT,
    metadata_category VARCHAR(100),
    session_duration INT
) USING columnar;

-- ETL process to populate columnar table
INSERT INTO events_analytics
SELECT 
    user_id,
    event_type,
    platform,
    DATE(timestamp) as event_date,
    EXTRACT(HOUR FROM timestamp) as event_hour,
    metadata->>'category' as metadata_category,
    CASE 
        WHEN event_type = 'session_end' 
        THEN (metadata->>'duration')::INT 
        ELSE NULL 
    END as session_duration
FROM events_partitioned
WHERE DATE(timestamp) = CURRENT_DATE - INTERVAL '1 day';

-- Complex analytics queries now run much faster
SELECT 
    platform,
    event_date,
    COUNT(*) as events,
    COUNT(DISTINCT user_id) as unique_users,
    AVG(session_duration) FILTER (WHERE session_duration IS NOT NULL) as avg_session
FROM events_analytics
WHERE event_date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY platform, event_date
ORDER BY platform, event_date;

-- Result: Complex analytics queries 45s → 3s (93% improvement)
</code></pre>
<p><strong>Results:</strong></p>
<ul>
<li>Dashboard performance: 99% improvement (15s → 0.2s)</li>
<li>Complex analytics: 93% improvement (45s → 3s)</li>
<li>ETL impact on user queries: Eliminated</li>
<li>System scalability: 3x increase in throughput</li>
</ul>
<h3>Case Study 3: Financial Services Transaction Processing</h3>
<h4>The Challenge</h4>
<p>A fintech company processing 50M transactions/day experienced:</p>
<ul>
<li>Fraud detection queries timing out</li>
<li>Account balance calculations taking minutes</li>
<li>Compliance reports causing system outages</li>
<li>Unable to provide real-time balance updates</li>
</ul>
<h4>The Solution</h4>
<p><strong>Phase 1: Transaction Processing Optimization</strong></p>
<pre><code class="language-sql">-- Original design issues:
-- 1. All transactions in single table
-- 2. Balance calculated by summing all transactions
-- 3. No proper indexing for fraud detection patterns

-- Solution: Event sourcing with balance snapshots
CREATE TABLE transactions (
    id BIGSERIAL PRIMARY KEY,
    account_id BIGINT,
    transaction_type VARCHAR(20),
    amount DECIMAL(15,2),
    currency VARCHAR(3),
    timestamp TIMESTAMP DEFAULT NOW(),
    reference_id VARCHAR(100),
    merchant_id BIGINT,
    category VARCHAR(50),
    metadata JSONB
);

-- Partition by timestamp for efficient querying
CREATE TABLE transactions_partitioned (
    LIKE transactions INCLUDING ALL
) PARTITION BY RANGE (timestamp);

-- Create account balance cache
CREATE TABLE account_balances (
    account_id BIGINT PRIMARY KEY,
    current_balance DECIMAL(15,2),
    available_balance DECIMAL(15,2),
    last_transaction_id BIGINT,
    last_updated TIMESTAMP DEFAULT NOW()
);

-- Indexes for fraud detection
CREATE INDEX idx_transactions_account_time ON transactions_partitioned(account_id, timestamp);
CREATE INDEX idx_transactions_merchant_amount ON transactions_partitioned(merchant_id, amount, timestamp);
CREATE INDEX idx_transactions_amount_time ON transactions_partitioned(amount, timestamp) WHERE amount > 1000;
CREATE INDEX idx_transactions_velocity ON transactions_partitioned(account_id, timestamp, amount);

-- Real-time balance update trigger
CREATE OR REPLACE FUNCTION update_account_balance()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO account_balances (account_id, current_balance, available_balance, last_transaction_id)
    VALUES (
        NEW.account_id,
        NEW.amount,
        NEW.amount,
        NEW.id
    )
    ON CONFLICT (account_id)
    DO UPDATE SET
        current_balance = account_balances.current_balance + NEW.amount,
        available_balance = account_balances.available_balance + NEW.amount,
        last_transaction_id = NEW.id,
        last_updated = NOW();
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER tr_update_balance
    AFTER INSERT ON transactions_partitioned
    FOR EACH ROW
    EXECUTE FUNCTION update_account_balance();
</code></pre>
<p><strong>Phase 2: Fraud Detection Optimization</strong></p>
<pre><code class="language-sql">-- Fraud detection patterns
CREATE MATERIALIZED VIEW fraud_detection_patterns AS
SELECT 
    account_id,
    date_trunc('hour', timestamp) as hour,
    COUNT(*) as transaction_count,
    SUM(amount) as total_amount,
    COUNT(DISTINCT merchant_id) as unique_merchants,
    MAX(amount) as max_transaction,
    stddev(amount) as amount_stddev
FROM transactions_partitioned
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY account_id, date_trunc('hour', timestamp);

-- Real-time fraud scoring function
CREATE OR REPLACE FUNCTION calculate_fraud_score(
    p_account_id BIGINT,
    p_amount DECIMAL,
    p_merchant_id BIGINT
) RETURNS DECIMAL AS $$
DECLARE
    v_score DECIMAL := 0;
    v_hourly_count INT;
    v_hourly_amount DECIMAL;
    v_avg_transaction DECIMAL;
    v_merchant_history INT;
BEGIN
    -- Check transaction velocity
    SELECT COUNT(*), COALESCE(SUM(amount), 0)
    INTO v_hourly_count, v_hourly_amount
    FROM transactions_partitioned
    WHERE account_id = p_account_id
      AND timestamp >= NOW() - INTERVAL '1 hour';
    
    -- Score based on velocity
    IF v_hourly_count > 10 THEN v_score := v_score + 20; END IF;
    IF v_hourly_amount > 10000 THEN v_score := v_score + 30; END IF;
    
    -- Check merchant history
    SELECT COUNT(*)
    INTO v_merchant_history
    FROM transactions_partitioned
    WHERE account_id = p_account_id
      AND merchant_id = p_merchant_id
      AND timestamp >= NOW() - INTERVAL '30 days';
    
    -- New merchant penalty
    IF v_merchant_history = 0 AND p_amount > 500 THEN
        v_score := v_score + 25;
    END IF;
    
    -- Amount pattern analysis
    SELECT AVG(amount)
    INTO v_avg_transaction
    FROM transactions_partitioned
    WHERE account_id = p_account_id
      AND timestamp >= NOW() - INTERVAL '30 days';
    
    -- Unusual amount penalty
    IF p_amount > v_avg_transaction * 5 THEN
        v_score := v_score + 40;
    END IF;
    
    RETURN v_score;
END;
$$ LANGUAGE plpgsql;

-- Fast fraud check during transaction processing
SELECT 
    *,
    calculate_fraud_score(account_id, amount, merchant_id) as fraud_score
FROM transactions_partitioned
WHERE id = NEW.id;

-- Result: Fraud detection time 30s → 0.1s (99.7% improvement)
</code></pre>
<p><strong>Phase 3: Compliance Reporting Optimization</strong></p>
<pre><code class="language-sql">-- Pre-aggregated compliance data
CREATE TABLE daily_transaction_summary (
    account_id BIGINT,
    transaction_date DATE,
    transaction_count INT,
    total_inflow DECIMAL(15,2),
    total_outflow DECIMAL(15,2),
    max_single_transaction DECIMAL(15,2),
    suspicious_activity_count INT,
    PRIMARY KEY (account_id, transaction_date)
);

-- Automated daily aggregation
CREATE OR REPLACE FUNCTION generate_daily_summary(target_date DATE)
RETURNS VOID AS $$
BEGIN
    INSERT INTO daily_transaction_summary
    SELECT 
        account_id,
        DATE(timestamp) as transaction_date,
        COUNT(*) as transaction_count,
        SUM(CASE WHEN amount > 0 THEN amount ELSE 0 END) as total_inflow,
        SUM(CASE WHEN amount &#x3C; 0 THEN ABS(amount) ELSE 0 END) as total_outflow,
        MAX(ABS(amount)) as max_single_transaction,
        COUNT(*) FILTER (WHERE ABS(amount) > 10000) as suspicious_activity_count
    FROM transactions_partitioned
    WHERE DATE(timestamp) = target_date
    GROUP BY account_id, DATE(timestamp)
    ON CONFLICT (account_id, transaction_date)
    DO UPDATE SET
        transaction_count = EXCLUDED.transaction_count,
        total_inflow = EXCLUDED.total_inflow,
        total_outflow = EXCLUDED.total_outflow,
        max_single_transaction = EXCLUDED.max_single_transaction,
        suspicious_activity_count = EXCLUDED.suspicious_activity_count;
END;
$$ LANGUAGE plpgsql;

-- Fast compliance reporting
SELECT 
    account_id,
    SUM(total_inflow) as monthly_inflow,
    SUM(total_outflow) as monthly_outflow,
    MAX(max_single_transaction) as largest_transaction,
    SUM(suspicious_activity_count) as total_suspicious
FROM daily_transaction_summary
WHERE transaction_date >= date_trunc('month', CURRENT_DATE)
  AND transaction_date &#x3C; date_trunc('month', CURRENT_DATE) + INTERVAL '1 month'
GROUP BY account_id
HAVING SUM(total_inflow) > 100000  -- Accounts with high activity
ORDER BY monthly_inflow DESC;

-- Result: Compliance report generation 2 hours → 5 minutes (96% improvement)
</code></pre>
<p><strong>Results:</strong></p>
<ul>
<li>Balance calculation: 2 minutes → 0.01s (99.99% improvement)</li>
<li>Fraud detection: 30s → 0.1s (99.7% improvement)</li>
<li>Compliance reports: 2 hours → 5 minutes (96% improvement)</li>
<li>Real-time balance updates: Achieved</li>
<li>System availability: 99.9% → 99.99%</li>
</ul>
<h2>Migration Strategies</h2>
<h3>Zero-Downtime Index Creation</h3>
<pre><code class="language-sql">-- PostgreSQL: Concurrent index creation
CREATE INDEX CONCURRENTLY idx_users_email_new ON users(email);

-- Rename old index and activate new one
BEGIN;
ALTER INDEX idx_users_email RENAME TO idx_users_email_old;
ALTER INDEX idx_users_email_new RENAME TO idx_users_email;
COMMIT;

-- Drop old index
DROP INDEX idx_users_email_old;
</code></pre>
<pre><code class="language-sql">-- SQL Server: Online index operations
CREATE INDEX idx_users_email_new ON users(email)
WITH (ONLINE = ON, SORT_IN_TEMPDB = ON);

-- Switch indexes atomically
BEGIN TRANSACTION;
EXEC sp_rename 'users.idx_users_email', 'idx_users_email_old', 'INDEX';
EXEC sp_rename 'users.idx_users_email_new', 'idx_users_email', 'INDEX';
COMMIT;

DROP INDEX idx_users_email_old ON users;
</code></pre>
<h3>Schema Migration Best Practices</h3>
<pre><code class="language-python"># Database migration script with rollback
class DatabaseMigration:
    def __init__(self, connection):
        self.conn = connection
        
    def migrate_with_rollback(self):
        savepoint_name = f"migration_{int(time.time())}"
        
        try:
            # Create savepoint
            self.conn.execute(f"SAVEPOINT {savepoint_name}")
            
            # Step 1: Create new indexes
            self.create_new_indexes()
            
            # Step 2: Verify performance
            if not self.verify_performance():
                raise Exception("Performance verification failed")
            
            # Step 3: Drop old indexes
            self.drop_old_indexes()
            
            # Step 4: Update statistics
            self.update_statistics()
            
            print("Migration completed successfully")
            
        except Exception as e:
            print(f"Migration failed: {e}")
            self.conn.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name}")
            print("Migration rolled back")
            raise
    
    def create_new_indexes(self):
        indexes = [
            "CREATE INDEX CONCURRENTLY idx_orders_customer_date_new ON orders(customer_id, order_date)",
            "CREATE INDEX CONCURRENTLY idx_products_category_price_new ON products(category_id, price)",
        ]
        
        for index_sql in indexes:
            print(f"Creating index: {index_sql}")
            self.conn.execute(index_sql)
    
    def verify_performance(self):
        # Run test queries and verify performance
        test_queries = [
            ("SELECT * FROM orders WHERE customer_id = 1000 ORDER BY order_date", 0.1),
            ("SELECT * FROM products WHERE category_id = 5 AND price > 100", 0.05),
        ]
        
        for query, max_time in test_queries:
            start_time = time.time()
            self.conn.execute(query)
            execution_time = time.time() - start_time
            
            if execution_time > max_time:
                print(f"Query too slow: {execution_time}s > {max_time}s")
                return False
        
        return True
</code></pre>
<h2>Troubleshooting Guide</h2>
<h3>Common Performance Issues</h3>
<h4>Issue 1: Query Suddenly Became Slow</h4>
<pre><code class="language-sql">-- Diagnostic steps:

-- 1. Check for missing statistics
SELECT 
    schemaname,
    tablename,
    last_analyze,
    n_tup_ins + n_tup_upd + n_tup_del as total_changes
FROM pg_stat_user_tables
WHERE last_analyze &#x3C; NOW() - INTERVAL '1 week'
ORDER BY total_changes DESC;

-- 2. Check for index bloat
SELECT 
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) as size,
    idx_scan,
    idx_tup_read
FROM pg_stat_user_indexes
WHERE idx_scan = 0 
  AND pg_relation_size(indexrelid) > 1000000  -- 1MB+
ORDER BY pg_relation_size(indexrelid) DESC;

-- 3. Check for lock contention
SELECT 
    mode,
    locktype,
    database,
    relation,
    page,
    tuple,
    classid,
    granted,
    pid
FROM pg_locks
WHERE NOT granted;

-- Solutions:
-- 1. Update statistics: ANALYZE table_name;
-- 2. Rebuild bloated indexes: REINDEX INDEX index_name;
-- 3. Identify blocking queries and optimize them
</code></pre>
<h4>Issue 2: High CPU Usage</h4>
<pre><code class="language-sql">-- Find expensive queries
SELECT 
    query,
    calls,
    total_time / calls as avg_time,
    rows / calls as avg_rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
ORDER BY total_time DESC
LIMIT 10;

-- Check for sequential scans on large tables
SELECT 
    schemaname,
    tablename,
    seq_scan,
    seq_tup_read,
    seq_tup_read / GREATEST(seq_scan, 1) as avg_seq_read,
    n_tup_ins + n_tup_upd + n_tup_del as total_writes
FROM pg_stat_user_tables
WHERE seq_scan > 100
  AND seq_tup_read / GREATEST(seq_scan, 1) > 10000
ORDER BY seq_tup_read DESC;
</code></pre>
<h3>Index Optimization Checklist</h3>
<h4>Pre-Implementation Checklist</h4>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Analyze current query patterns using query logs</li>
<li class="task-list-item"><input type="checkbox" disabled> Identify slow queries with EXPLAIN ANALYZE</li>
<li class="task-list-item"><input type="checkbox" disabled> Check existing index usage statistics</li>
<li class="task-list-item"><input type="checkbox" disabled> Estimate index size and maintenance overhead</li>
<li class="task-list-item"><input type="checkbox" disabled> Plan for index creation during low-traffic periods</li>
<li class="task-list-item"><input type="checkbox" disabled> Prepare rollback procedures</li>
</ul>
<h4>Post-Implementation Checklist</h4>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Monitor query performance improvements</li>
<li class="task-list-item"><input type="checkbox" disabled> Check index usage statistics</li>
<li class="task-list-item"><input type="checkbox" disabled> Verify no regression in write performance</li>
<li class="task-list-item"><input type="checkbox" disabled> Monitor disk space usage</li>
<li class="task-list-item"><input type="checkbox" disabled> Update documentation</li>
<li class="task-list-item"><input type="checkbox" disabled> Schedule regular index maintenance</li>
</ul>
<h3>Production Deployment Guidelines</h3>
<h4>Deployment Strategy</h4>
<ol>
<li><strong>Test Environment</strong>: Replicate production data volume and query patterns</li>
<li><strong>Staging Deployment</strong>: Deploy to staging with production-like traffic</li>
<li><strong>Canary Deployment</strong>: Deploy to subset of production servers</li>
<li><strong>Full Deployment</strong>: Roll out to all production servers</li>
<li><strong>Monitor and Optimize</strong>: Continuous monitoring and adjustment</li>
</ol>
<h4>Monitoring Checklist</h4>
<pre><code class="language-bash">#!/bin/bash
# Production index monitoring script

DB_NAME="production_db"
ALERT_EMAIL="ops-team@company.com"
LOG_FILE="/var/log/db-index-monitor.log"

# Check for slow queries
SLOW_QUERIES=$(psql -d $DB_NAME -t -c "
SELECT COUNT(*) 
FROM pg_stat_statements 
WHERE mean_time > 1000  -- Queries taking more than 1 second
")

if [ "$SLOW_QUERIES" -gt 5 ]; then
    echo "$(date): WARNING: $SLOW_QUERIES slow queries detected" >> $LOG_FILE
    # Send alert email
fi

# Check for unused indexes
UNUSED_INDEXES=$(psql -d $DB_NAME -t -c "
SELECT COUNT(*) 
FROM pg_stat_user_indexes 
WHERE idx_scan = 0 
  AND pg_relation_size(indexrelid) > 100000000  -- 100MB+
")

if [ "$UNUSED_INDEXES" -gt 0 ]; then
    echo "$(date): INFO: $UNUSED_INDEXES large unused indexes found" >> $LOG_FILE
fi

# Check index fragmentation (example for SQL Server)
# Adapt for your database system

echo "$(date): Index monitoring completed" >> $LOG_FILE
</code></pre>
<h2>Best Practices Summary</h2>
<h3>Design Principles</h3>
<ol>
<li><strong>Understand Your Workload</strong>: OLTP vs OLAP vs Mixed workloads require different strategies</li>
<li><strong>Start Simple</strong>: Begin with basic indexes, optimize based on actual usage patterns</li>
<li><strong>Measure Everything</strong>: Use query analysis tools and performance monitoring</li>
<li><strong>Test Thoroughly</strong>: Always test index changes in production-like environments</li>
</ol>
<h3>Implementation Guidelines</h3>
<ol>
<li><strong>Index Selectivity</strong>: Create indexes on high-selectivity columns first</li>
<li><strong>Composite Index Order</strong>: Follow the ESR rule (Equality, Sort, Range)</li>
<li><strong>Covering Indexes</strong>: Include frequently accessed columns to avoid table lookups</li>
<li><strong>Maintenance Windows</strong>: Plan index operations during low-traffic periods</li>
</ol>
<h3>Monitoring and Maintenance</h3>
<ol>
<li><strong>Regular Health Checks</strong>: Monitor index usage, fragmentation, and performance</li>
<li><strong>Automated Maintenance</strong>: Set up automated statistics updates and index rebuilding</li>
<li><strong>Capacity Planning</strong>: Monitor index growth and plan for storage requirements</li>
<li><strong>Documentation</strong>: Keep detailed records of index changes and their impact</li>
</ol>
<h3>Performance Optimization</h3>
<ol>
<li><strong>Query Optimization</strong>: Optimize queries to make effective use of indexes</li>
<li><strong>Connection Management</strong>: Use connection pooling and proper timeout settings</li>
<li><strong>Caching Strategies</strong>: Implement appropriate caching at multiple levels</li>
<li><strong>Read Replicas</strong>: Use read replicas to distribute read workload</li>
</ol>
<h2>Conclusion</h2>
<p>Database indexing is a critical skill for building high-performance applications. This comprehensive series has covered:</p>
<ul>
<li><strong>Fundamentals</strong>: Index types, structures, and core concepts</li>
<li><strong>SQL Databases</strong>: Advanced indexing across MySQL, PostgreSQL, SQL Server, and Oracle</li>
<li><strong>NoSQL Systems</strong>: Indexing strategies for MongoDB, Cassandra, Redis, and others</li>
<li><strong>Advanced Techniques</strong>: Composite indexes, partitioning, and specialized index types</li>
<li><strong>Monitoring</strong>: Performance tracking, automated maintenance, and health monitoring</li>
<li><strong>Advanced Features</strong>: Columnar storage, vector indexes, and big data strategies</li>
<li><strong>Client Optimization</strong>: Connection pooling, caching, and application-level optimization</li>
<li><strong>Real-World Cases</strong>: Production examples with measurable performance improvements</li>
</ul>
<p>The key to success is understanding your specific workload, measuring performance systematically, and iterating based on real-world results. Index optimization is an ongoing process that requires continuous monitoring and adjustment as your application grows and evolves.</p>
<p>Remember: the best index strategy is one that's tailored to your specific use case, properly tested, and continuously monitored for effectiveness.</p>
2:[["$","$Lb",null,{"initialPost":{"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$c","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"6 min read","coverImage":"$undefined","fixedUrl":"$undefined","series":{"name":"Database Indexes Mastery","order":1,"total":8,"prev":null,"next":"/posts/database-indexes-guide/part-2"}},"allSeriesParts":[{"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$d","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"6 min read","coverImage":"$undefined","fixedUrl":"$undefined","series":{"name":"Database Indexes Mastery","order":1,"total":8,"prev":null,"next":"/posts/database-indexes-guide/part-2"}},{"slug":"database-indexes-guide/part-2","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$e","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"7 min read","coverImage":"$undefined","series":{"name":"Database Indexes Mastery","order":2,"total":8,"prev":"/posts/database-indexes-guide","next":"/posts/database-indexes-guide/part-3","parts":[{"order":1,"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts"},{"order":2,"slug":"database-indexes-guide/part-2","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle"},{"order":3,"slug":"database-indexes-guide/part-3","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB"},{"order":4,"slug":"database-indexes-guide/part-4","title":"Composite Indexes and Advanced Query Optimization Techniques"},{"order":5,"slug":"database-indexes-guide/part-5","title":"Index Performance Monitoring, Maintenance & Troubleshooting"},{"order":6,"slug":"database-indexes-guide/part-6","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes"},{"order":7,"slug":"database-indexes-guide/part-7","title":"Client-Side Optimization and Application-Level Caching Strategies"},{"order":8,"slug":"database-indexes-guide/part-8","title":"Database Indexing Case Studies: Real-World Scenarios & Solutions"}]}},{"slug":"database-indexes-guide/part-3","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$f","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"9 min read","coverImage":"$undefined","series":{"name":"Database Indexes Mastery","order":3,"total":8,"prev":"/posts/database-indexes-guide/part-2","next":"/posts/database-indexes-guide/part-4","parts":[{"order":1,"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts"},{"order":2,"slug":"database-indexes-guide/part-2","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle"},{"order":3,"slug":"database-indexes-guide/part-3","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB"},{"order":4,"slug":"database-indexes-guide/part-4","title":"Composite Indexes and Advanced Query Optimization Techniques"},{"order":5,"slug":"database-indexes-guide/part-5","title":"Index Performance Monitoring, Maintenance & Troubleshooting"},{"order":6,"slug":"database-indexes-guide/part-6","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes"},{"order":7,"slug":"database-indexes-guide/part-7","title":"Client-Side Optimization and Application-Level Caching Strategies"},{"order":8,"slug":"database-indexes-guide/part-8","title":"Database Indexing Case Studies: Real-World Scenarios & Solutions"}]}},{"slug":"database-indexes-guide/part-4","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$10","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"10 min read","coverImage":"$undefined","series":{"name":"Database Indexes Mastery","order":4,"total":8,"prev":"/posts/database-indexes-guide/part-3","next":"/posts/database-indexes-guide/part-5","parts":[{"order":1,"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts"},{"order":2,"slug":"database-indexes-guide/part-2","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle"},{"order":3,"slug":"database-indexes-guide/part-3","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB"},{"order":4,"slug":"database-indexes-guide/part-4","title":"Composite Indexes and Advanced Query Optimization Techniques"},{"order":5,"slug":"database-indexes-guide/part-5","title":"Index Performance Monitoring, Maintenance & Troubleshooting"},{"order":6,"slug":"database-indexes-guide/part-6","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes"},{"order":7,"slug":"database-indexes-guide/part-7","title":"Client-Side Optimization and Application-Level Caching Strategies"},{"order":8,"slug":"database-indexes-guide/part-8","title":"Database Indexing Case Studies: Real-World Scenarios & Solutions"}]}},{"slug":"database-indexes-guide/part-5","title":"Composite Indexes and Advanced Query Optimization Techniques","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$11","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"10 min read","coverImage":"$undefined","series":{"name":"Database Indexes Mastery","order":5,"total":8,"prev":"/posts/database-indexes-guide/part-4","next":"/posts/database-indexes-guide/part-6","parts":[{"order":1,"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts"},{"order":2,"slug":"database-indexes-guide/part-2","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle"},{"order":3,"slug":"database-indexes-guide/part-3","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB"},{"order":4,"slug":"database-indexes-guide/part-4","title":"Composite Indexes and Advanced Query Optimization Techniques"},{"order":5,"slug":"database-indexes-guide/part-5","title":"Index Performance Monitoring, Maintenance & Troubleshooting"},{"order":6,"slug":"database-indexes-guide/part-6","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes"},{"order":7,"slug":"database-indexes-guide/part-7","title":"Client-Side Optimization and Application-Level Caching Strategies"},{"order":8,"slug":"database-indexes-guide/part-8","title":"Database Indexing Case Studies: Real-World Scenarios & Solutions"}]}},{"slug":"database-indexes-guide/part-6","title":"Index Performance Monitoring, Maintenance & Troubleshooting","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$12","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"10 min read","coverImage":"$undefined","series":{"name":"Database Indexes Mastery","order":6,"total":8,"prev":"/posts/database-indexes-guide/part-5","next":"/posts/database-indexes-guide/part-7","parts":[{"order":1,"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts"},{"order":2,"slug":"database-indexes-guide/part-2","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle"},{"order":3,"slug":"database-indexes-guide/part-3","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB"},{"order":4,"slug":"database-indexes-guide/part-4","title":"Composite Indexes and Advanced Query Optimization Techniques"},{"order":5,"slug":"database-indexes-guide/part-5","title":"Index Performance Monitoring, Maintenance & Troubleshooting"},{"order":6,"slug":"database-indexes-guide/part-6","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes"},{"order":7,"slug":"database-indexes-guide/part-7","title":"Client-Side Optimization and Application-Level Caching Strategies"},{"order":8,"slug":"database-indexes-guide/part-8","title":"Database Indexing Case Studies: Real-World Scenarios & Solutions"}]}},{"slug":"database-indexes-guide/part-7","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$13","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"14 min read","coverImage":"$undefined","series":{"name":"Database Indexes Mastery","order":7,"total":8,"prev":"/posts/database-indexes-guide/part-6","next":"/posts/database-indexes-guide/part-8","parts":[{"order":1,"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts"},{"order":2,"slug":"database-indexes-guide/part-2","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle"},{"order":3,"slug":"database-indexes-guide/part-3","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB"},{"order":4,"slug":"database-indexes-guide/part-4","title":"Composite Indexes and Advanced Query Optimization Techniques"},{"order":5,"slug":"database-indexes-guide/part-5","title":"Index Performance Monitoring, Maintenance & Troubleshooting"},{"order":6,"slug":"database-indexes-guide/part-6","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes"},{"order":7,"slug":"database-indexes-guide/part-7","title":"Client-Side Optimization and Application-Level Caching Strategies"},{"order":8,"slug":"database-indexes-guide/part-8","title":"Database Indexing Case Studies: Real-World Scenarios & Solutions"}]}},{"slug":"database-indexes-guide/part-8","title":"Client-Side Optimization and Application-Level Caching Strategies","date":"2024-03-20","excerpt":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.","content":"$14","author":"Abstract Algorithms","tags":["databases","indexes","performance","sql","nosql","b-tree","optimization"],"readingTime":"16 min read","coverImage":"$undefined","series":{"name":"Database Indexes Mastery","order":8,"total":8,"prev":"/posts/database-indexes-guide/part-7","next":null,"parts":[{"order":1,"slug":"database-indexes-guide","title":"Database Indexes Fundamentals: Types, Structure & Core Concepts"},{"order":2,"slug":"database-indexes-guide/part-2","title":"SQL Database Indexing Strategies: MySQL, PostgreSQL, SQL Server & Oracle"},{"order":3,"slug":"database-indexes-guide/part-3","title":"NoSQL Database Indexing: MongoDB, Cassandra, Redis & DynamoDB"},{"order":4,"slug":"database-indexes-guide/part-4","title":"Composite Indexes and Advanced Query Optimization Techniques"},{"order":5,"slug":"database-indexes-guide/part-5","title":"Index Performance Monitoring, Maintenance & Troubleshooting"},{"order":6,"slug":"database-indexes-guide/part-6","title":"Advanced Indexing Techniques: Partitioning & Specialized Indexes"},{"order":7,"slug":"database-indexes-guide/part-7","title":"Client-Side Optimization and Application-Level Caching Strategies"},{"order":8,"slug":"database-indexes-guide/part-8","title":"Database Indexing Case Studies: Real-World Scenarios & Solutions"}]}}]}],["$","div",null,{"className":"max-w-6xl mx-auto px-6 pb-8","children":["$","div",null,{"className":"bg-white rounded-xl shadow-sm border border-gray-200 p-8","children":["$","$L15",null,{}]}]}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"Database Indexes Fundamentals: Types, Structure & Core Concepts\",\"description\":\"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance.\",\"datePublished\":\"2024-03-20\",\"dateModified\":\"2024-03-20\",\"author\":{\"@type\":\"Person\",\"name\":\"Abstract Algorithms\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Abstract Algorithms\",\"url\":\"https://abstractalgorithms.github.io\"},\"url\":\"https://abstractalgorithms.github.io/posts/database-indexes-guide\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://abstractalgorithms.github.io/posts/database-indexes-guide\"}}"}}]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Database Indexes Fundamentals: Types, Structure & Core Concepts | Abstract Algorithms"}],["$","meta","3",{"name":"description","content":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance."}],["$","meta","4",{"name":"author","content":"Abstract Algorithms"}],["$","meta","5",{"name":"keywords","content":"algorithms,data structures,system design,software engineering,programming,computer science,performance optimization,big o notation,hash tables,database indexing"}],["$","meta","6",{"name":"creator","content":"Abstract Algorithms"}],["$","meta","7",{"name":"publisher","content":"Abstract Algorithms"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","meta","10",{"property":"og:title","content":"Database Indexes Fundamentals: Types, Structure & Core Concepts"}],["$","meta","11",{"property":"og:description","content":"Master the fundamentals of database indexes. Learn what indexes are, different types (B-Tree, Hash, Bitmap), how they work internally, and when to use each type for optimal database performance."}],["$","meta","12",{"property":"og:type","content":"article"}],["$","meta","13",{"property":"article:published_time","content":"2024-03-20"}],["$","meta","14",{"property":"article:author","content":"Abstract Algorithms"}],["$","meta","15",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","16",{"name":"twitter:title","content":"Abstract Algorithms"}],["$","meta","17",{"name":"twitter:description","content":"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices"}],["$","link","18",{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link","19",{"rel":"icon","href":"/icon.svg","type":"image/svg+xml","sizes":"32x32"}],["$","link","20",{"rel":"apple-touch-icon","href":"/apple-icon.svg","type":"image/svg+xml","sizes":"180x180"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
