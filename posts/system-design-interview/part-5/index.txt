3:I[4707,[],""]
6:I[6423,[],""]
7:I[981,["8592","static/chunks/common-1942b2e5063f4af5.js","3185","static/chunks/app/layout-f803094fc502a10d.js"],"AuthProvider"]
8:I[8931,["8592","static/chunks/common-1942b2e5063f4af5.js","3185","static/chunks/app/layout-f803094fc502a10d.js"],"default"]
9:I[917,["7601","static/chunks/app/error-1745ca505ccb7f84.js"],"default"]
a:I[5618,["9160","static/chunks/app/not-found-5aff7e7753541a4f.js"],"default"]
4:["slug","system-design-interview","d"]
5:["part","part-5","d"]
0:["sQHX0ZM4pyGaRbLhzPBh-",[[["",{"children":["posts",{"children":[["slug","system-design-interview","d"],{"children":[["part","part-5","d"],{"children":["__PAGE__?{\"slug\":\"system-design-interview\",\"part\":\"part-5\"}",{}]}]}]}]},"$undefined","$undefined",true],["",{"children":["posts",{"children":[["slug","system-design-interview","d"],{"children":[["part","part-5","d"],{"children":["__PAGE__",{},[["$L1","$L2",null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/275ed64cc4367444.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/f2c5f2458408eb15.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"Abstract Algorithms\",\"description\":\"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices\",\"url\":\"https://abstractalgorithms.github.io\",\"potentialAction\":{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https://abstractalgorithms.github.io/posts/{search_term_string}\"},\"query-input\":\"required name=search_term_string\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Abstract Algorithms\",\"url\":\"https://abstractalgorithms.github.io\"}}"}}],["$","link",null,{"rel":"manifest","href":"/manifest.json"}],["$","meta",null,{"name":"theme-color","content":"#00D885"}],["$","meta",null,{"name":"google-site-verification","content":"D5v1M3nD8oO9DNaZKujCwBLNNqf35CTJo114uv8yMNU"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-VZR168MHE2"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n            window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-VZR168MHE2');\n          "}}]]}],["$","body",null,{"className":"__className_e8ce0c","children":["$","$L7",null,{"children":["$","$L8",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$9","errorStyles":[],"errorScripts":[],"template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","$La",null,{}],"notFoundStyles":[]}]}]}]}]]}]],null],null],["$Lb",null]]]]
c:I[5878,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"Image"]
d:I[2972,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],""]
e:I[1343,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"default"]
f:I[9798,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"default"]
10:I[6883,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"default"]
11:Tfb9,<h2>Why Use Hash Tables?</h2>
<p>Hash tables are ideal for scenarios where you need to:</p>
<ul>
<li>Quickly look up values by a unique key (e.g., username â†’ user profile)</li>
<li>Count occurrences of items (e.g., word frequency in a document)</li>
<li>Implement sets, caches, or associative arrays</li>
<li>Index data for fast retrieval (e.g., database indexes, symbol tables in compilers)</li>
</ul>
<p><strong>Example Applications:</strong></p>
<ul>
<li>Caching web pages or database queries</li>
<li>Implementing sets/maps in programming languages (e.g., Python's <code>dict</code>, JavaScript's <code>Object</code>/<code>Map</code>)</li>
<li>Counting unique visitors or items</li>
<li>Storing configuration or environment variables</li>
</ul>
<h2>Anatomy of a Hash Table</h2>
<ol>
<li><strong>Hash Function</strong>: Transforms keys into array indices. A robust function minimizes collisions and distributes keys uniformly.</li>
<li><strong>Buckets / Slots</strong>: Underlying array where values reside.</li>
<li><strong>Collision Resolution</strong>: Techniques like chaining or open addressing to handle index conflicts.</li>
</ol>
<h2>How Hash Functions Work</h2>
<p>A hash function takes an input (the key) and returns an integer (the hash code), which is then mapped to an index in the underlying array. Good hash functions:</p>
<ul>
<li>Are deterministic (same input always gives same output)</li>
<li>Distribute keys uniformly to minimize clustering</li>
<li>Are fast to compute</li>
</ul>
<h3>Example: Simple Modulo Hash</h3>
<pre><code class="language-javascript">function simpleHash(key, tableSize) {
  let hash = 0;
  for (let char of key) {
    hash = (hash * 31 + char.charCodeAt(0)) % tableSize;
  }
  return hash;
}
</code></pre>
<blockquote>
<p>The choice of multiplier (e.g., 31) affects distribution; primes often yield better spreads.</p>
</blockquote>
<h3>Real-World Hash Functions</h3>
<ul>
<li><strong>MurmurHash, CityHash, FNV-1a</strong>: Used in production systems for better distribution and speed.</li>
<li><strong>Cryptographic hashes (SHA-256, MD5)</strong>: Used for security, not for hash tables (too slow).</li>
</ul>
<h2>Handling Collisions</h2>
<p>When two keys hash to the same index, a collision occurs. There are two main strategies:</p>
<h3>Chaining</h3>
<p>Each bucket holds a list of entries. Collisions are handled by appending to the list.</p>
<pre><code class="language-javascript">class HashTableChain {
  constructor(size = 42) {
    this.buckets = Array.from({ length: size }, () => []);
  }

  insert(key, value) {
    const index = simpleHash(key, this.buckets.length);
    this.buckets[index].push([key, value]);
  }

  // ...existing code...
}
</code></pre>
<h3>Open Addressing (Linear Probing)</h3>
<p>All entries are stored in the array itself. On collision, the algorithm searches for the next available slot.</p>
<p>{/* Linear probing illustration would go here */}</p>
<pre><code class="language-javascript">class HashTableProbing {
  constructor(size = 42) {
    this.table = new Array(size).fill(null);
  }

  // ...existing code...
}
</code></pre>
<h2>Example Scenario: Username Lookup</h2>
<p>Suppose you want to check if a username is taken:</p>
<ol>
<li>Hash the username to get an index.</li>
<li>Check the bucket (or slot) at that index.</li>
<li>If found, the username is taken; otherwise, it's available.</li>
</ol>
<p>This operation is extremely fast, even with thousands or millions of users.</p>
<h2>Performance Analysis</h2>
<ul>
<li><strong>Average Case</strong>: With a good hash function and low load factor, operations are nearly instantaneous.</li>
<li><strong>Worst Case</strong>: If many keys collide (poor hash function or overloaded table), performance degrades to linear time.</li>
</ul>
<h2>Conclusion</h2>
<p>Well-implemented hash tables power applications that require rapid lookups, from caching layers to in-memory databases. Selecting the right collision strategy and hash function is key to maintaining high performance.</p>
12:T29e8,<h1>System Design Interview Mastery: Complete Guide</h1>
<p>Welcome to the comprehensive System Design Mastery series! This 6-part guide will take you from understanding the fundamentals to solving the most popular system design interview questions asked at top tech companies.</p>
<h2>What You'll Learn</h2>
<p>By the end of this series, you'll master:</p>
<ul>
<li><strong>Systematic Problem-Solving Approach</strong>: A proven methodology to tackle any system design question</li>
<li><strong>Top 5 Interview Questions</strong>: Detailed solutions to the most commonly asked questions</li>
<li><strong>Scalability Patterns</strong>: How to design systems that handle millions of users</li>
<li><strong>Trade-offs Analysis</strong>: Understanding when to choose specific technologies and architectures</li>
<li><strong>Interview Techniques</strong>: How to communicate your design decisions effectively</li>
</ul>
<h2>Series Overview</h2>
<h3>Part 1: Introduction &#x26; Methodology (This Part)</h3>
<p>Learn the systematic approach to system design interviews and core concepts.</p>
<h3>Part 2: Design a URL Shortener (TinyURL)</h3>
<p>Master the fundamentals with this classic system design problem.</p>
<h3>Part 3: Design a Chat System (WhatsApp)</h3>
<p>Learn real-time communication patterns and WebSocket architecture.</p>
<h3>Part 4: Design a Social Media Feed (Twitter)</h3>
<p>Understand content delivery, caching, and timeline generation.</p>
<h3>Part 5: Design a Video Streaming Service (YouTube)</h3>
<p>Explore CDNs, video processing, and large-scale storage.</p>
<h3>Part 6: Design a Distributed Cache (Redis)</h3>
<p>Deep dive into caching strategies and data consistency.</p>
<h2>The System Design Interview Process</h2>
<p>Understanding the interview format is crucial for success. Most system design interviews follow a predictable structure that allows candidates to demonstrate their architectural thinking and problem-solving skills.</p>
<h3>Key Interview Phases</h3>
<ol>
<li>
<p><strong>Requirements Clarification (5-10 minutes)</strong></p>
<ul>
<li>Define functional requirements</li>
<li>Identify non-functional requirements</li>
<li>Establish scale and constraints</li>
</ul>
</li>
<li>
<p><strong>High-Level Design (15-20 minutes)</strong></p>
<ul>
<li>Sketch the overall architecture</li>
<li>Identify major components</li>
<li>Define data flow</li>
</ul>
</li>
<li>
<p><strong>Detailed Design (15-20 minutes)</strong></p>
<ul>
<li>Deep dive into critical components</li>
<li>Database schema design</li>
<li>API design</li>
</ul>
</li>
<li>
<p><strong>Scale and Optimize (10-15 minutes)</strong></p>
<ul>
<li>Address bottlenecks</li>
<li>Discuss caching strategies</li>
<li>Handle edge cases</li>
</ul>
</li>
</ol>
<h2>The Universal Template</h2>
<p>Every system design problem can be approached using this template:</p>
<h3>1. Functional Requirements</h3>
<p><strong>Actors</strong>: Define who will use the system</p>
<ul>
<li>Reader</li>
<li>Writer</li>
<li>Admin</li>
</ul>
<p><strong>Use Cases</strong>: Define how actors interact with the system</p>
<ul>
<li>How the Reader will use the system</li>
<li>How the Writer will use the system</li>
<li>Administrative functions</li>
</ul>
<p><strong>Features</strong>: List specific functionality</p>
<ul>
<li>What features are needed by each actor</li>
<li>What is explicitly out of scope</li>
</ul>
<h3>2. Non-Functional Requirements</h3>
<p>Define NFR expectations for all actors:</p>
<ul>
<li><strong>Scalability</strong>: How many users? Growth expectations?</li>
<li><strong>Availability</strong>: Uptime requirements (99.9%, 99.99%?)</li>
<li><strong>Performance</strong>: Latency expectations for reads/writes</li>
<li><strong>Data Consistency</strong>: Strong vs eventual consistency needs</li>
</ul>
<h3>3. Estimations</h3>
<p><strong>User Metrics</strong>:</p>
<ul>
<li>Daily Active Users (DAU)</li>
<li>Monthly Active Users (MAU)</li>
</ul>
<p><strong>Throughput</strong>:</p>
<ul>
<li>Queries Per Second (QPS) for reads</li>
<li>Queries Per Second (QPS) for writes</li>
<li>Read/Write ratio</li>
</ul>
<p><strong>Storage Estimations</strong>:</p>
<ul>
<li>Data per user/action</li>
<li>Daily storage needs</li>
<li>Annual storage needs</li>
<li>5-10 year projections</li>
</ul>
<p><strong>Memory Estimations</strong>:</p>
<ul>
<li>Cache requirements</li>
<li>RAM needs per server</li>
<li>Disk storage requirements</li>
</ul>
<p><strong>Scale Reference</strong>:</p>
<table>
<thead>
<tr>
<th>Unit</th>
<th>Decimal</th>
<th>Storage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Million</td>
<td>10^6</td>
<td>Megabytes</td>
</tr>
<tr>
<td>Billion</td>
<td>10^9</td>
<td>Gigabytes</td>
</tr>
<tr>
<td>Trillion</td>
<td>10^12</td>
<td>Terabytes</td>
</tr>
<tr>
<td>Quadrillion</td>
<td>10^15</td>
<td>Petabytes</td>
</tr>
</tbody>
</table>
<h3>4. Design Goals</h3>
<p><strong>Performance Requirements</strong>:</p>
<ul>
<li>Latency targets</li>
<li>Throughput requirements</li>
<li>Consistency vs Availability trade-offs</li>
</ul>
<p><strong>Architecture Patterns</strong>:</p>
<ul>
<li>Pipe and Filter Pattern</li>
<li>Event Driven Architecture</li>
<li>Pub/Sub Messaging</li>
<li>Streaming Processing</li>
</ul>
<p><strong>Usage Patterns</strong>:</p>
<table>
<thead>
<tr>
<th>Workload Type</th>
<th>Example</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read Heavy</td>
<td>Social Media</td>
<td>High read traffic from users browsing content</td>
</tr>
<tr>
<td>Write Heavy</td>
<td>Logging, Transactions</td>
<td>Frequent write operations for data capture</td>
</tr>
<tr>
<td>Balanced</td>
<td>E-Commerce</td>
<td>Mix of reads (browsing) and writes (orders)</td>
</tr>
<tr>
<td>Batch Processing</td>
<td>Analytics</td>
<td>Large data volumes processed in scheduled batches</td>
</tr>
<tr>
<td>Real-time</td>
<td>Trading, Monitoring</td>
<td>Immediate response to events required</td>
</tr>
</tbody>
</table>
<p><strong>Data Access Patterns</strong>:</p>
<table>
<thead>
<tr>
<th>Access Type</th>
<th>Use Case</th>
<th>Additional Information</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sequential Access</td>
<td>File Processing</td>
<td>Read/write data in order</td>
</tr>
<tr>
<td>Random Access</td>
<td>Database Lookup</td>
<td>Access specific data by key/index</td>
</tr>
<tr>
<td>Write Once, Read Many</td>
<td>Archival, Config</td>
<td>Data written once, read frequently</td>
</tr>
<tr>
<td>Pattern Matching</td>
<td>Log Analysis</td>
<td>Extract patterns using regex or similar</td>
</tr>
<tr>
<td>Range Queries</td>
<td>Time-series Data</td>
<td>Query data within specific ranges</td>
</tr>
</tbody>
</table>
<h2>Core Concepts to Master</h2>
<h3>Scalability Fundamentals</h3>
<p><strong>Horizontal Scaling (Scale Out)</strong>:</p>
<ul>
<li>Add more servers to handle increased load</li>
<li>Better fault tolerance and cost-effectiveness</li>
<li>Examples: Web servers, microservices</li>
</ul>
<p><strong>Vertical Scaling (Scale Up)</strong>:</p>
<ul>
<li>Increase power of existing machines</li>
<li>Simpler to implement but has physical limits</li>
<li>Examples: Database upgrades, CPU/RAM increases</li>
</ul>
<h3>Database Strategies</h3>
<p><strong>SQL vs NoSQL</strong>:</p>
<ul>
<li><strong>SQL</strong>: ACID properties, complex queries, structured data</li>
<li><strong>NoSQL</strong>: Horizontal scaling, flexible schema, specific use cases</li>
</ul>
<p><strong>Database Patterns</strong>:</p>
<ul>
<li><strong>Master-Slave Replication</strong>: Read scaling</li>
<li><strong>Master-Master Replication</strong>: Write scaling with conflicts</li>
<li><strong>Database Sharding</strong>: Horizontal partitioning</li>
<li><strong>Federation</strong>: Split databases by function</li>
</ul>
<h3>Caching Strategies</h3>
<p><strong>Cache Patterns</strong>:</p>
<ul>
<li><strong>Cache-Aside</strong>: Application manages cache</li>
<li><strong>Write-Through</strong>: Write to cache and database simultaneously</li>
<li><strong>Write-Behind</strong>: Write to cache first, database later</li>
<li><strong>Refresh-Ahead</strong>: Proactively refresh cache before expiration</li>
</ul>
<p><strong>Cache Levels</strong>:</p>
<ul>
<li>Browser cache</li>
<li>CDN (Content Delivery Network)</li>
<li>Load balancer cache</li>
<li>Application cache</li>
<li>Database cache</li>
</ul>
<h3>Communication Patterns</h3>
<p><strong>Synchronous Communication</strong>:</p>
<ul>
<li>HTTP/HTTPS requests</li>
<li>RPC (Remote Procedure Calls)</li>
<li>GraphQL</li>
</ul>
<p><strong>Asynchronous Communication</strong>:</p>
<ul>
<li>Message queues (RabbitMQ, Apache Kafka)</li>
<li>Pub/Sub systems</li>
<li>Event streaming</li>
</ul>
<h2>Common Design Patterns</h2>
<h3>Microservices Architecture</h3>
<ul>
<li>Service decomposition</li>
<li>API Gateway pattern</li>
<li>Service discovery</li>
<li>Circuit breaker pattern</li>
</ul>
<h3>Event-Driven Architecture</h3>
<ul>
<li>Event sourcing</li>
<li>CQRS (Command Query Responsibility Segregation)</li>
<li>Saga pattern for distributed transactions</li>
</ul>
<h3>Data Management Patterns</h3>
<ul>
<li>Database per service</li>
<li>Shared database anti-pattern</li>
<li>Event-driven data synchronization</li>
</ul>
<h2>Preparation Tips</h2>
<h3>Study Strategy</h3>
<ol>
<li><strong>Understand fundamentals</strong>: Master basic concepts before diving into complex problems</li>
<li><strong>Practice systematically</strong>: Use the template for every problem</li>
<li><strong>Learn from real systems</strong>: Study how actual systems like Google, Facebook, and Amazon work</li>
<li><strong>Think about trade-offs</strong>: Every design decision has pros and cons</li>
<li><strong>Practice communication</strong>: Explain your thinking process clearly</li>
</ol>
<h3>Common Mistakes to Avoid</h3>
<ul>
<li>Jumping to solution without understanding requirements</li>
<li>Over-engineering the initial design</li>
<li>Ignoring non-functional requirements</li>
<li>Not considering scalability from the start</li>
<li>Poor time management during the interview</li>
</ul>
<h2>System Design Fundamentals Quiz</h2>
<p>Before diving into specific use cases, test your understanding of the core system design concepts. The interactive quiz will appear at the end of this series introduction.</p>
<h2>What's Next?</h2>
<p>In the next part, we'll apply this methodology to design a URL shortener service like TinyURL. This classic problem will help you practice the systematic approach and understand how to break down complex requirements into manageable components.</p>
<p>Each subsequent part will tackle increasingly complex problems, building your confidence and expertise in system design interviews.</p>
<p><strong>Ready to start?</strong> Let's dive into Part 2 and design our first system!</p>
13:T3117,<h1>Design a URL Shortener (TinyURL)</h1>
<p>In this part, we'll apply our systematic methodology to design a URL shortener service like TinyURL or bit.ly. This is one of the most popular system design interview questions because it covers fundamental concepts while being simple enough to design in 45 minutes.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>URL Creator</strong>: Users who want to shorten long URLs</li>
<li><strong>URL Consumer</strong>: Users who click on shortened URLs</li>
<li><strong>System Administrator</strong>: Manages the service</li>
</ul>
<h3>Use Cases</h3>
<p><strong>URL Creator</strong>:</p>
<ul>
<li>Create shortened URLs from long URLs</li>
<li>Set custom aliases (optional)</li>
<li>Set expiration dates for URLs</li>
<li>View analytics (click count, geographic data)</li>
</ul>
<p><strong>URL Consumer</strong>:</p>
<ul>
<li>Access original URLs via shortened links</li>
<li>Experience fast redirection (&#x3C;100ms)</li>
</ul>
<p><strong>System Administrator</strong>:</p>
<ul>
<li>Monitor system health and performance</li>
<li>Manage expired URLs and cleanup</li>
<li>Handle abuse and spam detection</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>Shorten long URLs to ~7 character format</li>
<li>Redirect shortened URLs to original URLs</li>
<li>Custom aliases for URLs</li>
<li>URL expiration functionality</li>
<li>Basic analytics (click count)</li>
<li>High availability for redirections</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>User authentication/accounts</li>
<li>Advanced analytics dashboard</li>
<li>Real-time collaboration features</li>
<li>API rate limiting (assume handled by infrastructure)</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support 100 million URLs shortened per month</li>
<li>Handle 10 billion redirections per month</li>
<li>Scale to serve global users</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.9% uptime for URL creation</li>
<li>99.99% uptime for URL redirection</li>
<li>Graceful degradation during failures</li>
</ul>
<h3>Performance</h3>
<ul>
<li>URL creation: &#x3C;200ms response time</li>
<li>URL redirection: &#x3C;100ms response time</li>
<li>Handle traffic spikes during viral content</li>
</ul>
<h3>Data Consistency</h3>
<ul>
<li>Strong consistency for URL creation</li>
<li>Eventual consistency acceptable for analytics</li>
<li>No duplicate shortened URLs</li>
</ul>
<h2>3. Estimations</h2>
<h3>User Metrics</h3>
<ul>
<li><strong>Daily Active Users</strong>: 10 million</li>
<li><strong>URLs created per day</strong>: 3.3 million (100M/month)</li>
<li><strong>Redirections per day</strong>: 333 million (10B/month)</li>
</ul>
<h3>Throughput</h3>
<ul>
<li><strong>URL Creation QPS</strong>: 38 queries/second (3.3M/24/3600)</li>
<li><strong>URL Redirection QPS</strong>: 3,858 queries/second (333M/24/3600)</li>
<li><strong>Peak QPS</strong>: 5x average = 19,290 QPS</li>
<li><strong>Read/Write Ratio</strong>: 100:1 (heavy read workload)</li>
</ul>
<h3>Storage Estimations</h3>
<p><strong>Per URL Storage</strong>:</p>
<ul>
<li>Shortened URL: 7 bytes</li>
<li>Original URL: 500 bytes (average)</li>
<li>Metadata (creation date, expiration, etc.): 100 bytes</li>
<li><strong>Total per URL</strong>: ~600 bytes</li>
</ul>
<p><strong>Storage Growth</strong>:</p>
<ul>
<li><strong>Per Day</strong>: 3.3M × 600 bytes = 2 GB/day</li>
<li><strong>Per Year</strong>: 2 GB × 365 = 730 GB/year</li>
<li><strong>Per 5 Years</strong>: 730 GB × 5 = 3.65 TB</li>
</ul>
<h3>Memory Estimations</h3>
<p><strong>Cache Requirements</strong> (80/20 rule):</p>
<ul>
<li>20% of URLs generate 80% of traffic</li>
<li>Daily hot URLs: 333M × 20% = 66.6M URLs</li>
<li>Cache size: 66.6M × 600 bytes = ~40 GB</li>
<li>With replication: 40 GB × 3 = 120 GB total cache</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Latency</strong>: &#x3C;100ms for redirections, &#x3C;200ms for creation</li>
<li><strong>Throughput</strong>: 20K QPS peak capacity</li>
<li><strong>Consistency</strong>: Strong for writes, eventual for analytics</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Read-Heavy Workload</strong>: Implement aggressive caching</li>
<li><strong>Event-Driven</strong>: Use async processing for analytics</li>
<li><strong>Stateless Services</strong>: Enable horizontal scaling</li>
</ul>
<h3>Data Access Patterns</h3>
<ul>
<li><strong>Random Access</strong>: Database lookups by shortened URL key</li>
<li><strong>Write Once, Read Many</strong>: URLs rarely modified after creation</li>
<li><strong>Cache-Friendly</strong>: High cache hit ratios expected</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Client] → [Load Balancer] → [Web Servers] → [Cache] → [Database]
                                    ↓
                            [Analytics Service] → [Analytics DB]
                                    ↓
                              [Message Queue]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>Load Balancer</strong>: Distributes traffic across web servers</li>
<li><strong>Web Servers</strong>: Handle URL creation and redirection logic</li>
<li><strong>Cache Layer</strong>: Redis cluster for hot URL lookups</li>
<li><strong>Database</strong>: Primary storage for URL mappings</li>
<li><strong>Analytics Service</strong>: Processes click events asynchronously</li>
<li><strong>Message Queue</strong>: Decouples analytics from main flow</li>
</ol>
<h3>API Design</h3>
<p><strong>Create Short URL</strong>:</p>
<pre><code class="language-http">POST /api/v1/shorten
Content-Type: application/json

{
  "long_url": "https://example.com/very/long/path",
  "custom_alias": "mylink", // optional
  "expiration_date": "2024-12-31" // optional
}

Response:
{
  "short_url": "https://short.ly/abc123",
  "long_url": "https://example.com/very/long/path",
  "created_at": "2024-06-17T10:00:00Z",
  "expires_at": "2024-12-31T23:59:59Z"
}
</code></pre>
<p><strong>Redirect URL</strong>:</p>
<pre><code class="language-http">GET /{short_code}

Response: 301 Redirect
Location: https://example.com/very/long/path
</code></pre>
<p><strong>Get Analytics</strong>:</p>
<pre><code class="language-http">GET /api/v1/analytics/{short_code}

Response:
{
  "short_code": "abc123",
  "click_count": 1542,
  "created_at": "2024-06-17T10:00:00Z",
  "last_accessed": "2024-06-17T15:30:00Z"
}
</code></pre>
<h3>Data Schema</h3>
<p><strong>URLs Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE urls (
    short_code VARCHAR(7) PRIMARY KEY,
    long_url TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    click_count BIGINT DEFAULT 0,
    created_by_ip VARCHAR(45)
);

CREATE INDEX idx_expires_at ON urls(expires_at);
</code></pre>
<p><strong>Analytics Events Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE click_events (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    short_code VARCHAR(7) NOT NULL,
    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    user_ip VARCHAR(45),
    user_agent TEXT,
    referer TEXT
);

CREATE INDEX idx_short_code_time ON click_events(short_code, clicked_at);
</code></pre>
<h2>URL Encoding Algorithm</h2>
<h3>Base62 Encoding</h3>
<p>We'll use Base62 encoding (a-z, A-Z, 0-9) to generate short codes:</p>
<pre><code class="language-python">def base62_encode(num):
    base = 62
    alphabet = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    encoded = ""
    
    while num > 0:
        encoded = alphabet[num % base] + encoded
        num //= base
    
    return encoded or alphabet[0]

def base62_decode(encoded):
    base = 62
    alphabet = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    num = 0
    
    for char in encoded:
        num = num * base + alphabet.index(char)
    
    return num
</code></pre>
<h3>Counter-Based Approach</h3>
<ol>
<li>Use auto-incrementing database counter</li>
<li>Encode counter value to Base62</li>
<li>With 7 characters: 62^7 = 3.5 trillion possible URLs</li>
</ol>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No collisions</li>
<li>Predictable, sequential generation</li>
<li>Simple implementation</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Sequential patterns might be guessable</li>
<li>Single point of failure for counter</li>
</ul>
<h3>Alternative: Hash-Based Approach</h3>
<pre><code class="language-python">import hashlib

def generate_short_code(long_url, timestamp):
    data = f"{long_url}{timestamp}"
    hash_value = hashlib.md5(data.encode()).hexdigest()
    
    # Take first 7 characters and convert to Base62
    return hash_value[:7]
</code></pre>
<h2>Detailed Design Deep Dive</h2>
<h3>Caching Strategy</h3>
<p><strong>Multi-Layer Caching</strong>:</p>
<ol>
<li><strong>Browser Cache</strong>: Cache 301 redirects for 1 hour</li>
<li><strong>CDN Cache</strong>: Cache popular URLs at edge locations</li>
<li><strong>Application Cache</strong>: Redis cluster with:
<ul>
<li>TTL: 24 hours for hot URLs</li>
<li>LRU eviction policy</li>
<li>99% hit ratio target</li>
</ul>
</li>
</ol>
<p><strong>Cache Key Strategy</strong>:</p>
<pre><code>Key: "url:{short_code}"
Value: {
  "long_url": "https://example.com/path",
  "expires_at": "2024-12-31T23:59:59Z"
}
</code></pre>
<h3>Database Sharding</h3>
<p><strong>Shard by Short Code</strong>:</p>
<ul>
<li>Consistent hashing on short_code</li>
<li>4 shards initially, plan for 16 shards</li>
<li>Each shard handles ~25% of traffic</li>
</ul>
<p><strong>Shard Key</strong>: First 2 characters of short_code</p>
<ul>
<li>Shard 1: aa-pz</li>
<li>Shard 2: qa-9z</li>
<li>Shard 3: Aa-Pz</li>
<li>Shard 4: Qa-9z</li>
</ul>
<h3>Analytics Processing</h3>
<p><strong>Async Event Processing</strong>:</p>
<ol>
<li>URL click triggers event</li>
<li>Event published to message queue</li>
<li>Analytics service processes events in batches</li>
<li>Updates click counts every 5 minutes</li>
</ol>
<p><strong>Analytics Pipeline</strong>:</p>
<pre><code>[Click Event] → [Kafka Queue] → [Analytics Worker] → [Analytics DB]
                                        ↓
                                [Real-time Dashboard]
</code></pre>
<h2>Scaling Considerations</h2>
<h3>Handling Traffic Spikes</h3>
<p><strong>Auto-Scaling Strategy</strong>:</p>
<ul>
<li>Monitor QPS and response time</li>
<li>Scale web servers horizontally</li>
<li>Pre-warm cache for viral content</li>
<li>Circuit breakers for graceful degradation</li>
</ul>
<h3>Geographic Distribution</h3>
<p><strong>Multi-Region Deployment</strong>:</p>
<ul>
<li>Primary region: US-East (main database)</li>
<li>Secondary regions: EU-West, Asia-Pacific</li>
<li>Read replicas in each region</li>
<li>Global load balancer routes to nearest region</li>
</ul>
<h3>Performance Optimizations</h3>
<ol>
<li><strong>Connection Pooling</strong>: Reuse database connections</li>
<li><strong>Async Processing</strong>: Non-blocking I/O for analytics</li>
<li><strong>Batch Operations</strong>: Group database writes</li>
<li><strong>CDN Integration</strong>: Cache static assets and popular URLs</li>
</ol>
<h2>Security Considerations</h2>
<h3>Spam Prevention</h3>
<ul>
<li>Rate limiting per IP address</li>
<li>URL validation and sanitization</li>
<li>Malicious URL detection</li>
<li>CAPTCHA for suspicious traffic</li>
</ul>
<h3>Data Protection</h3>
<ul>
<li>HTTPS enforcement</li>
<li>SQL injection prevention</li>
<li>Input validation for custom aliases</li>
<li>Access logs for audit trails</li>
</ul>
<h2>URL Shortener Design Quiz</h2>
<p>Test your understanding of URL shortener system design with the interactive quiz that appears after each part of this series.</p>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Read-Heavy Optimization</strong>: Aggressive caching is crucial for URL shorteners</li>
<li><strong>Simple but Scalable</strong>: Start simple, add complexity as needed</li>
<li><strong>Analytics Separation</strong>: Decouple analytics from core functionality</li>
<li><strong>Global Distribution</strong>: CDNs and regional deployments improve performance</li>
<li><strong>Failure Planning</strong>: Design for graceful degradation during traffic spikes</li>
</ol>
<h2>What's Next?</h2>
<p>In Part 3, we'll design a real-time chat system like WhatsApp, which introduces new challenges around WebSocket connections, message delivery guarantees, and online presence management.</p>
14:T46f5,<h1>Design a Chat System (WhatsApp)</h1>
<p>In this part, we'll design a real-time chat system similar to WhatsApp or Slack. This problem introduces complex challenges around real-time communication, message delivery, and online presence management.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>Chat User</strong>: Sends and receives messages</li>
<li><strong>Group Admin</strong>: Manages group chats</li>
<li><strong>System</strong>: Handles presence and delivery</li>
</ul>
<h3>Use Cases</h3>
<p><strong>Chat User</strong>:</p>
<ul>
<li>Send one-on-one messages</li>
<li>Participate in group chats (up to 500 members)</li>
<li>See online/offline status of contacts</li>
<li>Receive messages in real-time</li>
<li>View message delivery status (sent, delivered, read)</li>
<li>Share media files (images, videos, documents)</li>
</ul>
<p><strong>Group Admin</strong>:</p>
<ul>
<li>Create and manage group chats</li>
<li>Add/remove participants</li>
<li>Set group permissions</li>
</ul>
<p><strong>System Functions</strong>:</p>
<ul>
<li>Deliver messages reliably</li>
<li>Maintain message ordering</li>
<li>Handle offline message delivery</li>
<li>Manage user presence status</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>One-on-one messaging</li>
<li>Group messaging (up to 500 members)</li>
<li>Real-time message delivery</li>
<li>Message delivery status</li>
<li>Online presence indicators</li>
<li>Media file sharing</li>
<li>Message history storage</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>Voice/video calling</li>
<li>Message encryption (assume handled by client)</li>
<li>Advanced group features (channels, threads)</li>
<li>Message search functionality</li>
<li>Push notifications (assume external service)</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support 1 billion users globally</li>
<li>Handle 50 billion messages per day</li>
<li>Support 10 million concurrent users</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.9% uptime for message delivery</li>
<li>Graceful degradation during failures</li>
<li>Message ordering must be preserved</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Message delivery: &#x3C;100ms in same region</li>
<li>Cross-region delivery: &#x3C;300ms</li>
<li>Group message fanout: &#x3C;500ms</li>
<li>Support real-time presence updates</li>
</ul>
<h3>Data Consistency</h3>
<ul>
<li>Strong consistency for message ordering</li>
<li>Eventual consistency for presence status</li>
<li>At-least-once message delivery guarantee</li>
</ul>
<h2>3. Estimations</h2>
<h3>User Metrics</h3>
<ul>
<li><strong>Total Users</strong>: 1 billion</li>
<li><strong>Daily Active Users</strong>: 500 million</li>
<li><strong>Concurrent Users</strong>: 10 million peak</li>
<li><strong>Average sessions per user</strong>: 4 per day</li>
</ul>
<h3>Message Volume</h3>
<ul>
<li><strong>Messages per day</strong>: 50 billion</li>
<li><strong>Messages per second</strong>: 578K average</li>
<li><strong>Peak QPS</strong>: 1.2 million</li>
<li><strong>Group messages</strong>: 20% of total volume</li>
</ul>
<h3>Storage Estimations</h3>
<p><strong>Per Message Storage</strong>:</p>
<ul>
<li>Message ID: 8 bytes</li>
<li>Sender ID: 8 bytes</li>
<li>Receiver/Group ID: 8 bytes</li>
<li>Message content: 100 bytes average</li>
<li>Metadata: 50 bytes</li>
<li><strong>Total per message</strong>: ~200 bytes</li>
</ul>
<p><strong>Storage Growth</strong>:</p>
<ul>
<li><strong>Per Day</strong>: 50B × 200 bytes = 10 TB/day</li>
<li><strong>Per Year</strong>: 10 TB × 365 = 3.65 PB/year</li>
<li><strong>Per 5 Years</strong>: 18.25 PB (with compression ~9 PB)</li>
</ul>
<h3>Connection Estimations</h3>
<ul>
<li><strong>WebSocket connections</strong>: 10 million concurrent</li>
<li><strong>Memory per connection</strong>: 10KB</li>
<li><strong>Total connection memory</strong>: 100 GB</li>
<li><strong>Servers needed</strong>: 200 servers (500MB per server)</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Latency</strong>: &#x3C;100ms same region, &#x3C;300ms cross-region</li>
<li><strong>Throughput</strong>: 1.2M messages/second peak</li>
<li><strong>Availability</strong>: 99.9% uptime</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Event-Driven</strong>: Message routing and delivery</li>
<li><strong>Pub/Sub</strong>: Real-time message distribution</li>
<li><strong>Microservices</strong>: Decomposed by functionality</li>
</ul>
<h3>Usage Patterns</h3>
<ul>
<li><strong>Real-time Processing</strong>: Immediate message delivery</li>
<li><strong>Write Heavy</strong>: High message ingestion rate</li>
<li><strong>Connection Heavy</strong>: Millions of persistent connections</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Mobile/Web Client] ↔ [WebSocket Gateway] → [Message Service] → [Message Queue]
                                ↓                    ↓              ↓
                        [Presence Service] → [User Service] → [Database Cluster]
                                ↓                    ↓              ↓
                        [Notification Service] → [Analytics] → [Message Storage]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>WebSocket Gateway</strong>: Manages persistent connections</li>
<li><strong>Message Service</strong>: Core message processing logic</li>
<li><strong>Presence Service</strong>: Tracks user online status</li>
<li><strong>User Service</strong>: User profiles and friend lists</li>
<li><strong>Message Queue</strong>: Reliable message delivery</li>
<li><strong>Database Cluster</strong>: Distributed message storage</li>
</ol>
<h3>API Design</h3>
<p><strong>WebSocket Events</strong>:</p>
<p><strong>Send Message</strong>:</p>
<pre><code class="language-json">{
  "type": "send_message",
  "data": {
    "message_id": "msg_123456",
    "chat_id": "chat_789",
    "content": "Hello World!",
    "message_type": "text",
    "timestamp": "2024-06-17T10:00:00Z"
  }
}
</code></pre>
<p><strong>Receive Message</strong>:</p>
<pre><code class="language-json">{
  "type": "new_message",
  "data": {
    "message_id": "msg_123456",
    "chat_id": "chat_789",
    "sender_id": "user_456",
    "content": "Hello World!",
    "timestamp": "2024-06-17T10:00:00Z",
    "delivery_status": "delivered"
  }
}
</code></pre>
<p><strong>Presence Update</strong>:</p>
<pre><code class="language-json">{
  "type": "presence_update",
  "data": {
    "user_id": "user_456",
    "status": "online",
    "last_seen": "2024-06-17T10:00:00Z"
  }
}
</code></pre>
<p><strong>REST APIs</strong>:</p>
<p><strong>Create Chat</strong>:</p>
<pre><code class="language-http">POST /api/v1/chats
{
  "type": "group",
  "name": "Project Team",
  "participants": ["user_123", "user_456", "user_789"]
}
</code></pre>
<p><strong>Get Chat History</strong>:</p>
<pre><code class="language-http">GET /api/v1/chats/{chat_id}/messages?limit=50&#x26;before=msg_123
</code></pre>
<h3>Database Schema</h3>
<p><strong>Users Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_seen TIMESTAMP,
    status ENUM('online', 'offline', 'away') DEFAULT 'offline'
);
</code></pre>
<p><strong>Chats Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE chats (
    chat_id BIGINT PRIMARY KEY,
    chat_type ENUM('direct', 'group') NOT NULL,
    name VARCHAR(255),
    created_by BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
</code></pre>
<p><strong>Messages Table</strong> (Partitioned by chat_id):</p>
<pre><code class="language-sql">CREATE TABLE messages (
    message_id BIGINT PRIMARY KEY,
    chat_id BIGINT NOT NULL,
    sender_id BIGINT NOT NULL,
    content TEXT NOT NULL,
    message_type ENUM('text', 'image', 'file') DEFAULT 'text',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_chat_time (chat_id, created_at),
    INDEX idx_sender (sender_id)
) PARTITION BY HASH(chat_id) PARTITIONS 100;
</code></pre>
<p><strong>Chat Participants Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE chat_participants (
    chat_id BIGINT,
    user_id BIGINT,
    joined_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    role ENUM('member', 'admin') DEFAULT 'member',
    
    PRIMARY KEY (chat_id, user_id),
    INDEX idx_user_chats (user_id)
);
</code></pre>
<h2>Detailed Design Deep Dive</h2>
<h3>WebSocket Connection Management</h3>
<p><strong>Connection Gateway</strong>:</p>
<pre><code class="language-python">class ConnectionGateway:
    def __init__(self):
        self.connections = {}  # user_id -> connection
        self.user_servers = {}  # user_id -> server_id
    
    def handle_connection(self, user_id, websocket):
        # Store connection mapping
        self.connections[user_id] = websocket
        self.user_servers[user_id] = self.server_id
        
        # Update presence service
        self.presence_service.set_online(user_id, self.server_id)
        
        # Subscribe to user's message queue
        self.message_queue.subscribe(f"user_{user_id}", self.deliver_message)
    
    def deliver_message(self, message):
        user_id = message['recipient_id']
        if user_id in self.connections:
            self.connections[user_id].send(message)
        else:
            # User offline, store for later delivery
            self.offline_storage.store(user_id, message)
</code></pre>
<p><strong>Load Balancing Connections</strong>:</p>
<ul>
<li>Consistent hashing by user_id</li>
<li>Session affinity for WebSocket connections</li>
<li>Health checks and failover</li>
</ul>
<h3>Message Processing Pipeline</h3>
<p><strong>Message Flow</strong>:</p>
<ol>
<li>Client sends message via WebSocket</li>
<li>Gateway validates and adds metadata</li>
<li>Message service processes and stores</li>
<li>Fanout service delivers to recipients</li>
<li>Delivery confirmation sent back</li>
</ol>
<p><strong>Message Service</strong>:</p>
<pre><code class="language-python">class MessageService:
    def process_message(self, message):
        # 1. Validate message
        if not self.validate_message(message):
            return {"error": "Invalid message"}
        
        # 2. Generate unique message ID
        message['message_id'] = self.generate_id()
        message['timestamp'] = datetime.utcnow()
        
        # 3. Store message
        self.store_message(message)
        
        # 4. Fanout to recipients
        recipients = self.get_chat_participants(message['chat_id'])
        for recipient_id in recipients:
            if recipient_id != message['sender_id']:
                self.message_queue.publish(f"user_{recipient_id}", message)
        
        # 5. Return acknowledgment
        return {"status": "sent", "message_id": message['message_id']}
</code></pre>
<h3>Group Message Fanout</h3>
<p><strong>Fanout Strategies</strong>:</p>
<p><strong>Pull Model</strong> (Recommended for large groups):</p>
<pre><code class="language-python">def fanout_pull_model(message, chat_id):
    # Store message once
    message_storage.store(message)
    
    # Notify online participants
    online_users = presence_service.get_online_users(chat_id)
    for user_id in online_users:
        notification_queue.publish(f"user_{user_id}", {
            "type": "new_message_notification",
            "chat_id": chat_id,
            "message_id": message['message_id']
        })
</code></pre>
<p><strong>Push Model</strong> (For small groups &#x3C;50 members):</p>
<pre><code class="language-python">def fanout_push_model(message, chat_id):
    participants = chat_service.get_participants(chat_id)
    
    for user_id in participants:
        if user_id != message['sender_id']:
            # Send full message to each participant
            message_queue.publish(f"user_{user_id}", message)
</code></pre>
<h3>Presence Service</h3>
<p><strong>Real-time Presence Updates</strong>:</p>
<pre><code class="language-python">class PresenceService:
    def __init__(self):
        self.redis_client = redis.Redis()
        self.heartbeat_interval = 30  # seconds
    
    def set_online(self, user_id, server_id):
        self.redis_client.hset("user_presence", user_id, json.dumps({
            "status": "online",
            "server_id": server_id,
            "last_seen": time.time()
        }))
        
        # Notify contacts about status change
        contacts = self.get_user_contacts(user_id)
        for contact_id in contacts:
            self.notify_presence_change(contact_id, user_id, "online")
    
    def heartbeat(self, user_id):
        # Update last seen timestamp
        presence = self.get_presence(user_id)
        if presence:
            presence['last_seen'] = time.time()
            self.redis_client.hset("user_presence", user_id, json.dumps(presence))
    
    def cleanup_offline_users(self):
        # Background job to mark users offline after timeout
        current_time = time.time()
        for user_id, presence_data in self.redis_client.hgetall("user_presence").items():
            presence = json.loads(presence_data)
            if current_time - presence['last_seen'] > 60:  # 1 minute timeout
                self.set_offline(user_id)
</code></pre>
<h3>Message Ordering and Delivery</h3>
<p><strong>Message Ordering</strong>:</p>
<ul>
<li>Use logical timestamps (Lamport clocks)</li>
<li>Sequence numbers per chat</li>
<li>Vector clocks for concurrent updates</li>
</ul>
<p><strong>Delivery Guarantees</strong>:</p>
<pre><code class="language-python">class MessageDelivery:
    def deliver_with_retry(self, user_id, message, max_retries=3):
        for attempt in range(max_retries):
            try:
                if self.is_user_online(user_id):
                    self.send_via_websocket(user_id, message)
                else:
                    self.store_for_offline_delivery(user_id, message)
                
                # Wait for acknowledgment
                if self.wait_for_ack(message['message_id'], timeout=5):
                    return True
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    # Final failure - store in dead letter queue
                    self.dead_letter_queue.store(user_id, message)
                    return False
                
                # Exponential backoff
                time.sleep(2 ** attempt)
        
        return False
</code></pre>
<h2>Scaling Considerations</h2>
<h3>Database Sharding</h3>
<p><strong>Shard by Chat ID</strong>:</p>
<pre><code class="language-python">def get_shard(chat_id):
    return chat_id % NUM_SHARDS

def route_message(message):
    shard = get_shard(message['chat_id'])
    return message_databases[shard]
</code></pre>
<p><strong>Hot Partition Problem</strong>:</p>
<ul>
<li>Very active group chats can overwhelm a single shard</li>
<li>Solution: Further partition by time ranges</li>
<li>Move viral chats to dedicated high-performance shards</li>
</ul>
<h3>Caching Strategy</h3>
<p><strong>Multi-Level Caching</strong>:</p>
<ol>
<li><strong>L1 Cache</strong>: Recent messages in application memory</li>
<li><strong>L2 Cache</strong>: Redis cluster for chat metadata</li>
<li><strong>L3 Cache</strong>: Chat participant lists</li>
</ol>
<pre><code class="language-python">class MessageCache:
    def get_recent_messages(self, chat_id, limit=50):
        # Try L1 cache first
        cache_key = f"recent_messages:{chat_id}"
        messages = self.memory_cache.get(cache_key)
        
        if not messages:
            # Try L2 cache (Redis)
            messages = self.redis_cache.get(cache_key)
            
            if not messages:
                # Fetch from database
                messages = self.database.get_messages(chat_id, limit)
                
                # Cache in both levels
                self.redis_cache.set(cache_key, messages, ttl=300)
            
            self.memory_cache.set(cache_key, messages, ttl=60)
        
        return messages
</code></pre>
<h3>Geographic Distribution</h3>
<p><strong>Multi-Region Architecture</strong>:</p>
<ul>
<li>WebSocket gateways in each region</li>
<li>Message routing based on user location</li>
<li>Cross-region message replication</li>
<li>Regional presence services with global sync</li>
</ul>
<h2>Chat System Design Quiz</h2>
<p>Test your understanding of real-time chat system design with the interactive quiz that appears after each part of this series.</p>
<h2>Security and Privacy</h2>
<h3>Message Security</h3>
<ul>
<li>End-to-end encryption (client-side)</li>
<li>Message integrity verification</li>
<li>Forward secrecy for key rotation</li>
</ul>
<h3>Privacy Protection</h3>
<ul>
<li>Message retention policies</li>
<li>User data anonymization</li>
<li>GDPR compliance for data deletion</li>
</ul>
<h3>Abuse Prevention</h3>
<ul>
<li>Rate limiting for spam prevention</li>
<li>Content moderation pipelines</li>
<li>User reporting mechanisms</li>
</ul>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Real-time Architecture</strong>: WebSockets enable bidirectional communication</li>
<li><strong>Message Ordering</strong>: Critical for user experience, requires careful design</li>
<li><strong>Presence Management</strong>: Efficient tracking reduces system overhead</li>
<li><strong>Fanout Strategies</strong>: Choose between push/pull based on group size</li>
<li><strong>Graceful Degradation</strong>: System should handle failures without data loss</li>
</ol>
<h2>What's Next?</h2>
<p>In Part 4, we'll design a social media feed system like Twitter, which introduces challenges around content ranking, timeline generation, and handling viral content.</p>
15:T5446,<h1>Design a Social Media Feed (Twitter)</h1>
<p>In this part, we'll design a social media feed system like Twitter. This problem introduces complex challenges around content ranking, timeline generation, viral content handling, and personalized content delivery.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>User</strong>: Posts and consumes content</li>
<li><strong>Content Creator</strong>: Influential users with many followers</li>
<li><strong>Content Moderator</strong>: Reviews flagged content</li>
<li><strong>System</strong>: Manages recommendations and trending</li>
</ul>
<h3>Use Cases</h3>
<p><strong>User</strong>:</p>
<ul>
<li>Post tweets (text, images, videos)</li>
<li>Follow/unfollow other users</li>
<li>View personalized timeline</li>
<li>Like, retweet, and comment on posts</li>
<li>Search for tweets and users</li>
<li>View trending topics</li>
</ul>
<p><strong>Content Creator</strong>:</p>
<ul>
<li>Publish content to large audiences</li>
<li>View analytics and engagement metrics</li>
<li>Promote content</li>
</ul>
<p><strong>Content Moderator</strong>:</p>
<ul>
<li>Review reported content</li>
<li>Take action on policy violations</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>Post tweets (280 characters, media support)</li>
<li>Follow/unfollow users</li>
<li>Home timeline (personalized feed)</li>
<li>User timeline (user's own tweets)</li>
<li>Like, retweet, reply functionality</li>
<li>Trending topics and hashtags</li>
<li>Search functionality</li>
<li>Basic analytics</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>Direct messaging (covered in Part 3)</li>
<li>Live streaming</li>
<li>Advanced recommendation algorithms</li>
<li>Advertisement system</li>
<li>Advanced analytics dashboard</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support 500 million users</li>
<li>Handle 300 million tweets per day</li>
<li>Support 100 million daily active users</li>
<li>Handle traffic spikes during viral events</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.9% uptime for timeline generation</li>
<li>99.99% uptime for tweet reading</li>
<li>Graceful degradation during peak traffic</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Timeline generation: &#x3C;200ms</li>
<li>Tweet posting: &#x3C;100ms</li>
<li>Search results: &#x3C;300ms</li>
<li>Handle 300K tweets/second during peak</li>
</ul>
<h3>Data Consistency</h3>
<ul>
<li>Eventual consistency for timeline updates</li>
<li>Strong consistency for user actions (follow/unfollow)</li>
<li>Tweet immutability after posting</li>
</ul>
<h2>3. Estimations</h2>
<h3>User Metrics</h3>
<ul>
<li><strong>Total Users</strong>: 500 million</li>
<li><strong>Daily Active Users</strong>: 100 million</li>
<li><strong>Average tweets per user per day</strong>: 3</li>
<li><strong>Average follows per user</strong>: 200</li>
<li><strong>Heavy users (celebrities)</strong>: 1% with 1M+ followers</li>
</ul>
<h3>Tweet Volume</h3>
<ul>
<li><strong>Tweets per day</strong>: 300 million</li>
<li><strong>Tweets per second</strong>: 3,472 average</li>
<li><strong>Peak TPS</strong>: 17,360 (5x average)</li>
<li><strong>Tweet fanout ratio</strong>: 1:200 (average followers)</li>
</ul>
<h3>Storage Estimations</h3>
<p><strong>Per Tweet Storage</strong>:</p>
<ul>
<li>Tweet ID: 8 bytes</li>
<li>User ID: 8 bytes</li>
<li>Content: 300 bytes (average with metadata)</li>
<li>Media URLs: 100 bytes</li>
<li>Timestamps: 16 bytes</li>
<li><strong>Total per tweet</strong>: ~450 bytes</li>
</ul>
<p><strong>Storage Growth</strong>:</p>
<ul>
<li><strong>Per Day</strong>: 300M × 450 bytes = 135 GB/day</li>
<li><strong>Per Year</strong>: 135 GB × 365 = 49 TB/year</li>
<li><strong>Per 5 Years</strong>: 245 TB</li>
</ul>
<p><strong>Timeline Cache Storage</strong>:</p>
<ul>
<li>Cache top 1000 tweets per user</li>
<li>100M users × 1000 tweets × 450 bytes = 45 TB cache</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Timeline Generation</strong>: &#x3C;200ms for cached timelines</li>
<li><strong>Tweet Publishing</strong>: &#x3C;100ms response time</li>
<li><strong>Search</strong>: &#x3C;300ms for result delivery</li>
<li><strong>Viral Content</strong>: Handle 100K retweets/minute</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Event-Driven</strong>: Tweet fanout and timeline updates</li>
<li><strong>CQRS</strong>: Separate read and write models</li>
<li><strong>Cache-Heavy</strong>: Aggressive caching for read performance</li>
</ul>
<h3>Usage Patterns</h3>
<ul>
<li><strong>Read Heavy</strong>: 300:1 read to write ratio</li>
<li><strong>Real-time</strong>: Immediate timeline updates</li>
<li><strong>Spike Traffic</strong>: Viral content creates traffic spikes</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Client] → [Load Balancer] → [API Gateway] → [Tweet Service]
                                    ↓           ↓
                            [Timeline Service] [User Service]
                                    ↓           ↓
                            [Fanout Service] → [Cache Layer]
                                    ↓           ↓
                            [Message Queue] → [Database Cluster]
                                    ↓           ↓
                            [Search Service] [Media Service]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>API Gateway</strong>: Routes requests and handles authentication</li>
<li><strong>Tweet Service</strong>: Handles tweet creation and retrieval</li>
<li><strong>Timeline Service</strong>: Generates and serves user timelines</li>
<li><strong>Fanout Service</strong>: Distributes tweets to followers</li>
<li><strong>User Service</strong>: Manages user profiles and relationships</li>
<li><strong>Search Service</strong>: Provides tweet and user search</li>
<li><strong>Cache Layer</strong>: Multi-tier caching for performance</li>
</ol>
<h3>API Design</h3>
<p><strong>Post Tweet</strong>:</p>
<pre><code class="language-http">POST /api/v1/tweets
Authorization: Bearer {token}
{
  "content": "Hello world! #myFirstTweet",
  "media_urls": ["https://cdn.example.com/image1.jpg"],
  "reply_to": null
}

Response:
{
  "tweet_id": "1234567890",
  "user_id": "user_123",
  "content": "Hello world! #myFirstTweet",
  "created_at": "2024-06-17T10:00:00Z",
  "engagement": {
    "likes": 0,
    "retweets": 0,
    "replies": 0
  }
}
</code></pre>
<p><strong>Get Timeline</strong>:</p>
<pre><code class="language-http">GET /api/v1/timeline?type=home&#x26;limit=20&#x26;cursor=tweet_123

Response:
{
  "tweets": [
    {
      "tweet_id": "1234567890",
      "user": {
        "user_id": "user_456",
        "username": "@johndoe",
        "display_name": "John Doe",
        "avatar_url": "https://cdn.example.com/avatar.jpg"
      },
      "content": "Great weather today!",
      "created_at": "2024-06-17T10:00:00Z",
      "engagement": {
        "likes": 42,
        "retweets": 15,
        "replies": 8
      },
      "media": []
    }
  ],
  "next_cursor": "tweet_456",
  "has_more": true
}
</code></pre>
<p><strong>Follow User</strong>:</p>
<pre><code class="language-http">POST /api/v1/users/{user_id}/follow

Response:
{
  "following": true,
  "follower_count": 1543,
  "following_count": 287
}
</code></pre>
<h3>Database Schema</h3>
<p><strong>Users Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    display_name VARCHAR(100),
    bio TEXT,
    avatar_url VARCHAR(500),
    verified BOOLEAN DEFAULT FALSE,
    follower_count INT DEFAULT 0,
    following_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
</code></pre>
<p><strong>Tweets Table</strong> (Partitioned by created_at):</p>
<pre><code class="language-sql">CREATE TABLE tweets (
    tweet_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    content TEXT NOT NULL,
    reply_to BIGINT,
    retweet_of BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    like_count INT DEFAULT 0,
    retweet_count INT DEFAULT 0,
    reply_count INT DEFAULT 0,
    
    INDEX idx_user_time (user_id, created_at),
    INDEX idx_reply_to (reply_to),
    FOREIGN KEY (user_id) REFERENCES users(user_id)
) PARTITION BY RANGE (UNIX_TIMESTAMP(created_at));
</code></pre>
<p><strong>Follows Table</strong> (Sharded by follower_id):</p>
<pre><code class="language-sql">CREATE TABLE follows (
    follower_id BIGINT,
    following_id BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (follower_id, following_id),
    INDEX idx_following (following_id, follower_id)
);
</code></pre>
<p><strong>Timeline Cache Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE user_timelines (
    user_id BIGINT,
    tweet_id BIGINT,
    score DECIMAL(10,2), -- for ranking
    created_at TIMESTAMP,
    
    PRIMARY KEY (user_id, score, tweet_id),
    INDEX idx_user_time (user_id, created_at)
);
</code></pre>
<h2>Timeline Generation Strategies</h2>
<h3>Push Model (Write-Heavy)</h3>
<p><strong>Tweet Fanout on Write</strong>:</p>
<pre><code class="language-python">class PushTimelineService:
    def fanout_tweet(self, tweet, user_id):
        # Get all followers
        followers = self.user_service.get_followers(user_id)
        
        # Add tweet to each follower's timeline
        for follower_id in followers:
            self.timeline_cache.add_to_timeline(follower_id, tweet)
            
            # Limit timeline size (keep only latest 1000 tweets)
            self.timeline_cache.trim_timeline(follower_id, max_size=1000)
    
    def get_timeline(self, user_id, limit=20):
        # Timeline is pre-computed, just read from cache
        return self.timeline_cache.get_timeline(user_id, limit)
</code></pre>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Fast timeline reads (pre-computed)</li>
<li>Real-time timeline updates</li>
<li>Simple implementation</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Expensive for users with many followers</li>
<li>Storage overhead (duplicate tweets)</li>
<li>Celebrity problem (1M+ followers)</li>
</ul>
<h3>Pull Model (Read-Heavy)</h3>
<p><strong>Timeline Generation on Read</strong>:</p>
<pre><code class="language-python">class PullTimelineService:
    def get_timeline(self, user_id, limit=20):
        # Get users that this user follows
        following = self.user_service.get_following(user_id)
        
        # Get recent tweets from each followed user
        all_tweets = []
        for followed_user_id in following:
            tweets = self.tweet_service.get_user_tweets(
                followed_user_id, 
                limit=100
            )
            all_tweets.extend(tweets)
        
        # Sort by timestamp and return top tweets
        sorted_tweets = sorted(all_tweets, key=lambda x: x.created_at, reverse=True)
        return sorted_tweets[:limit]
</code></pre>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No fanout cost for popular users</li>
<li>No storage duplication</li>
<li>Consistent view of latest data</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Slow timeline generation</li>
<li>Database load on read</li>
<li>Difficult to rank by engagement</li>
</ul>
<h3>Hybrid Model (Recommended)</h3>
<p><strong>Smart Fanout Strategy</strong>:</p>
<pre><code class="language-python">class HybridTimelineService:
    def __init__(self):
        self.celebrity_threshold = 1000000  # 1M followers
        
    def fanout_tweet(self, tweet, user_id):
        follower_count = self.user_service.get_follower_count(user_id)
        
        if follower_count > self.celebrity_threshold:
            # Celebrity: don't fanout, use pull on read
            self.celebrity_tweets_cache.add(user_id, tweet)
        else:
            # Regular user: fanout to all followers
            followers = self.user_service.get_followers(user_id)
            for follower_id in followers:
                self.timeline_cache.add_to_timeline(follower_id, tweet)
    
    def get_timeline(self, user_id, limit=20):
        # Get pre-computed timeline
        timeline_tweets = self.timeline_cache.get_timeline(user_id, limit)
        
        # Get tweets from celebrities this user follows
        celebrity_following = self.user_service.get_celebrity_following(user_id)
        celebrity_tweets = []
        
        for celebrity_id in celebrity_following:
            tweets = self.celebrity_tweets_cache.get_recent_tweets(celebrity_id, 10)
            celebrity_tweets.extend(tweets)
        
        # Merge and sort all tweets
        all_tweets = timeline_tweets + celebrity_tweets
        sorted_tweets = sorted(all_tweets, key=lambda x: x.created_at, reverse=True)
        
        return sorted_tweets[:limit]
</code></pre>
<h2>Detailed Design Deep Dive</h2>
<h3>Fanout Service Architecture</h3>
<p><strong>Async Fanout Processing</strong>:</p>
<pre><code class="language-python">class FanoutService:
    def __init__(self):
        self.message_queue = MessageQueue()
        self.batch_size = 1000
        
    def queue_fanout(self, tweet):
        # Queue fanout job for async processing
        fanout_job = {
            "tweet_id": tweet.id,
            "user_id": tweet.user_id,
            "timestamp": tweet.created_at
        }
        self.message_queue.publish("fanout_queue", fanout_job)
    
    def process_fanout_batch(self, jobs):
        # Process multiple fanout jobs in batch
        for job in jobs:
            followers = self.get_followers_batch(job.user_id)
            
            # Batch insert into timeline cache
            timeline_entries = []
            for follower_id in followers:
                timeline_entries.append({
                    "user_id": follower_id,
                    "tweet_id": job.tweet_id,
                    "score": self.calculate_score(job),
                    "created_at": job.timestamp
                })
            
            self.timeline_cache.batch_insert(timeline_entries)
</code></pre>
<h3>Caching Strategy</h3>
<p><strong>Multi-Layer Cache Architecture</strong>:</p>
<ol>
<li><strong>L1 Cache</strong>: Application-level cache (Recent timelines)</li>
<li><strong>L2 Cache</strong>: Redis cluster (User timelines, tweet data)</li>
<li><strong>L3 Cache</strong>: CDN (Media files, static content)</li>
</ol>
<pre><code class="language-python">class CacheManager:
    def __init__(self):
        self.l1_cache = LRUCache(max_size=10000)  # In-memory
        self.l2_cache = RedisCluster()
        self.l3_cache = CDN()
    
    def get_timeline(self, user_id, limit=20):
        cache_key = f"timeline:{user_id}:{limit}"
        
        # Try L1 cache first
        timeline = self.l1_cache.get(cache_key)
        if timeline:
            return timeline
            
        # Try L2 cache (Redis)
        timeline = self.l2_cache.get(cache_key)
        if timeline:
            self.l1_cache.set(cache_key, timeline, ttl=60)
            return timeline
            
        # Generate timeline and cache
        timeline = self.timeline_service.generate_timeline(user_id, limit)
        
        self.l2_cache.set(cache_key, timeline, ttl=300)
        self.l1_cache.set(cache_key, timeline, ttl=60)
        
        return timeline
</code></pre>
<h3>Search Service</h3>
<p><strong>Elasticsearch Integration</strong>:</p>
<pre><code class="language-python">class SearchService:
    def __init__(self):
        self.elasticsearch = Elasticsearch()
        
    def index_tweet(self, tweet):
        doc = {
            "tweet_id": tweet.id,
            "user_id": tweet.user_id,
            "username": tweet.user.username,
            "content": tweet.content,
            "hashtags": self.extract_hashtags(tweet.content),
            "mentions": self.extract_mentions(tweet.content),
            "created_at": tweet.created_at,
            "engagement_score": self.calculate_engagement_score(tweet)
        }
        
        self.elasticsearch.index(
            index="tweets",
            id=tweet.id,
            body=doc
        )
    
    def search_tweets(self, query, limit=20, offset=0):
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        {"match": {"content": {"query": query, "boost": 2}}},
                        {"match": {"hashtags": {"query": query, "boost": 3}}},
                        {"match": {"username": {"query": query, "boost": 1.5}}}
                    ]
                }
            },
            "sort": [
                {"engagement_score": {"order": "desc"}},
                {"created_at": {"order": "desc"}}
            ],
            "size": limit,
            "from": offset
        }
        
        return self.elasticsearch.search(index="tweets", body=search_body)
</code></pre>
<h3>Trending Topics</h3>
<p><strong>Real-time Trend Detection</strong>:</p>
<pre><code class="language-python">class TrendingService:
    def __init__(self):
        self.redis = Redis()
        self.trend_window = 3600  # 1 hour window
        
    def update_hashtag_count(self, hashtag):
        current_hour = int(time.time() // self.trend_window)
        key = f"hashtag_count:{current_hour}:{hashtag}"
        
        # Increment count for current hour
        self.redis.incr(key)
        self.redis.expire(key, self.trend_window * 2)  # Keep 2 hours
        
        # Update global trending scores
        self.update_trending_score(hashtag)
    
    def get_trending_topics(self, limit=10):
        # Get top hashtags by score
        return self.redis.zrevrange("trending_hashtags", 0, limit-1, withscores=True)
    
    def calculate_trend_score(self, hashtag, current_count, historical_avg):
        # Simple trending algorithm
        if historical_avg == 0:
            return current_count
        
        trend_ratio = current_count / historical_avg
        velocity_score = trend_ratio * math.log(current_count + 1)
        
        return velocity_score
</code></pre>
<h2>Scaling Considerations</h2>
<h3>Database Sharding</h3>
<p><strong>Tweets Sharding Strategy</strong>:</p>
<pre><code class="language-python">def get_tweet_shard(tweet_id):
    # Shard by tweet_id for even distribution
    return tweet_id % NUM_TWEET_SHARDS

def get_user_shard(user_id):
    # Shard by user_id for user-related data
    return user_id % NUM_USER_SHARDS
</code></pre>
<p><strong>Timeline Sharding</strong>:</p>
<pre><code class="language-python">def get_timeline_shard(user_id):
    # Shard user timelines by user_id
    return user_id % NUM_TIMELINE_SHARDS
</code></pre>
<h3>Handling Viral Content</h3>
<p><strong>Circuit Breaker for Fanout</strong>:</p>
<pre><code class="language-python">class ViralContentHandler:
    def __init__(self):
        self.fanout_threshold = 100000  # 100K followers
        self.circuit_breaker = CircuitBreaker()
        
    def handle_viral_tweet(self, tweet, user_id):
        follower_count = self.user_service.get_follower_count(user_id)
        
        if follower_count > self.fanout_threshold:
            # Skip immediate fanout for viral content
            self.queue_delayed_fanout(tweet, delay=60)  # 1 minute delay
            
            # Use pull model for immediate reads
            self.celebrity_cache.add_hot_tweet(user_id, tweet)
        else:
            # Normal fanout
            self.fanout_service.fanout_tweet(tweet, user_id)
</code></pre>
<h3>Media Handling</h3>
<p><strong>CDN Strategy for Media</strong>:</p>
<pre><code class="language-python">class MediaService:
    def __init__(self):
        self.cdn = CloudFrontCDN()
        self.storage = S3Storage()
        
    def upload_media(self, media_file, user_id):
        # Generate unique filename
        filename = f"{user_id}/{uuid.uuid4()}.{media_file.extension}"
        
        # Upload to S3
        s3_url = self.storage.upload(filename, media_file)
        
        # Generate CDN URL
        cdn_url = self.cdn.get_url(filename)
        
        return {
            "media_id": str(uuid.uuid4()),
            "original_url": s3_url,
            "cdn_url": cdn_url,
            "thumbnail_url": self.generate_thumbnail(cdn_url)
        }
</code></pre>
<h2>Social Media Feed Design Quiz</h2>
<p>Test your understanding of social media feed system design with the interactive quiz that appears after each part of this series.</p>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Hybrid Approach</strong>: Combine push and pull models based on user characteristics</li>
<li><strong>Aggressive Caching</strong>: Multi-layer caching is essential for read performance</li>
<li><strong>Async Processing</strong>: Use message queues for fanout and background processing</li>
<li><strong>Viral Content</strong>: Design circuit breakers and fallback mechanisms</li>
<li><strong>Search Integration</strong>: Elasticsearch enables fast, relevant search results</li>
</ol>
<h2>What's Next?</h2>
<p>In Part 5, we'll design a video streaming service like YouTube, which introduces challenges around large file storage, content delivery networks, and video processing pipelines.</p>
16:T6ae9,<h1>Design a Distributed Cache (Redis)</h1>
<p>In this final part, we'll design a distributed cache system like Redis or Memcached. This introduces fundamental concepts of distributed systems including data consistency, partitioning, replication, and coordination.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>Application Client</strong>: Reads and writes cache data</li>
<li><strong>Cache Administrator</strong>: Monitors and manages cache cluster</li>
<li><strong>System</strong>: Handles replication and failover</li>
</ul>
<h3>Use Cases</h3>
<p><strong>Application Client</strong>:</p>
<ul>
<li>Store key-value pairs with TTL</li>
<li>Retrieve values by key</li>
<li>Delete specific keys</li>
<li>Perform atomic operations (increment, append)</li>
<li>Execute batch operations</li>
<li>Subscribe to key events</li>
</ul>
<p><strong>Cache Administrator</strong>:</p>
<ul>
<li>Monitor cluster health and performance</li>
<li>Add/remove nodes from cluster</li>
<li>Configure replication settings</li>
<li>Manage memory usage and eviction policies</li>
</ul>
<p><strong>System Functions</strong>:</p>
<ul>
<li>Automatic failover and recovery</li>
<li>Data replication across nodes</li>
<li>Load balancing and sharding</li>
<li>Memory management and eviction</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>Basic operations (GET, SET, DELETE)</li>
<li>TTL (Time To Live) support</li>
<li>Data partitioning across nodes</li>
<li>Replication for high availability</li>
<li>Atomic operations and transactions</li>
<li>Pub/Sub messaging</li>
<li>Memory management and eviction</li>
<li>Cluster management</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>Complex data structures (sorted sets, streams)</li>
<li>Persistence to disk</li>
<li>Advanced scripting (Lua scripts)</li>
<li>Advanced security features</li>
<li>Cross-datacenter replication</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support thousands of nodes in a cluster</li>
<li>Handle millions of operations per second</li>
<li>Linear scaling with node addition</li>
<li>Support for multiple data centers</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.99% uptime</li>
<li>Automatic failover &#x3C;30 seconds</li>
<li>No single point of failure</li>
<li>Graceful degradation during failures</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Sub-millisecond latency for cache hits</li>
<li>Support 100K+ ops/sec per node</li>
<li>Efficient memory utilization (>90%)</li>
<li>Minimal network overhead</li>
</ul>
<h3>Consistency</h3>
<ul>
<li>Strong consistency within partition</li>
<li>Eventual consistency across replicas</li>
<li>Configurable consistency levels</li>
<li>Conflict resolution mechanisms</li>
</ul>
<h2>3. Estimations</h2>
<h3>Usage Metrics</h3>
<ul>
<li><strong>Cache Cluster Size</strong>: 100 nodes</li>
<li><strong>Memory per Node</strong>: 64 GB</li>
<li><strong>Total Cache Capacity</strong>: 6.4 TB</li>
<li><strong>Operations per Second</strong>: 10 million</li>
</ul>
<h3>Performance Metrics</h3>
<ul>
<li><strong>Average Key Size</strong>: 100 bytes</li>
<li><strong>Average Value Size</strong>: 1 KB</li>
<li><strong>Cache Hit Ratio</strong>: 95%</li>
<li><strong>Network Bandwidth</strong>: 10 Gbps per node</li>
</ul>
<h3>Memory Estimations</h3>
<p><strong>Per Node Storage</strong>:</p>
<ul>
<li>Available Memory: 64 GB</li>
<li>OS and Overhead: 4 GB</li>
<li>Cache Data: 60 GB</li>
<li><strong>Effective Storage</strong>: ~50 million key-value pairs per node</li>
</ul>
<p><strong>Cluster Totals</strong>:</p>
<ul>
<li><strong>Total Effective Storage</strong>: 5 billion key-value pairs</li>
<li><strong>Memory Efficiency</strong>: 90% (accounting for fragmentation)</li>
<li><strong>Replication Factor</strong>: 3x for high availability</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Latency</strong>: &#x3C;1ms for local operations</li>
<li><strong>Throughput</strong>: 100K ops/sec per node</li>
<li><strong>Memory Efficiency</strong>: >90% utilization</li>
<li><strong>Network Efficiency</strong>: Minimal cross-node communication</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Consistent Hashing</strong>: For data partitioning</li>
<li><strong>Master-Slave Replication</strong>: For data consistency</li>
<li><strong>Gossip Protocol</strong>: For cluster coordination</li>
</ul>
<h3>Usage Patterns</h3>
<ul>
<li><strong>Read Heavy</strong>: 80% reads, 20% writes</li>
<li><strong>Hot Keys</strong>: Power-law distribution of key access</li>
<li><strong>TTL Patterns</strong>: Mix of short and long-lived data</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Client] → [Smart Client/Proxy] → [Cache Node 1] ← [Replica 1A]
                    ↓                     ↓              ↓
                [Cache Node 2] ← [Replica 2A] ← [Coordinator]
                    ↓                     ↓              ↓
                [Cache Node 3] ← [Replica 3A] ← [Gossip Network]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>Cache Nodes</strong>: Store actual key-value data</li>
<li><strong>Cluster Coordinator</strong>: Manages cluster membership</li>
<li><strong>Smart Client</strong>: Routes requests to correct nodes</li>
<li><strong>Replication Manager</strong>: Handles data replication</li>
<li><strong>Gossip Protocol</strong>: Disseminates cluster state</li>
<li><strong>Memory Manager</strong>: Handles eviction and garbage collection</li>
</ol>
<h3>Data Distribution Strategy</h3>
<p><strong>Consistent Hashing</strong>:</p>
<pre><code class="language-python">class ConsistentHashing:
    def __init__(self, nodes, virtual_nodes=150):
        self.virtual_nodes = virtual_nodes
        self.ring = {}
        self.sorted_keys = []
        
        for node in nodes:
            self.add_node(node)
    
    def hash(self, key):
        return hashlib.md5(key.encode()).hexdigest()
    
    def add_node(self, node):
        for i in range(self.virtual_nodes):
            virtual_key = self.hash(f"{node}:{i}")
            self.ring[virtual_key] = node
        
        self.sorted_keys = sorted(self.ring.keys())
    
    def get_node(self, key):
        if not self.ring:
            return None
            
        hash_key = self.hash(key)
        
        # Find the first node clockwise
        for ring_key in self.sorted_keys:
            if hash_key &#x3C;= ring_key:
                return self.ring[ring_key]
        
        # Wrap around to the first node
        return self.ring[self.sorted_keys[0]]
    
    def remove_node(self, node):
        for i in range(self.virtual_nodes):
            virtual_key = self.hash(f"{node}:{i}")
            if virtual_key in self.ring:
                del self.ring[virtual_key]
        
        self.sorted_keys = sorted(self.ring.keys())
</code></pre>
<h2>Cache Node Implementation</h2>
<h3>Core Cache Operations</h3>
<pre><code class="language-python">class CacheNode:
    def __init__(self, node_id, max_memory=64*1024*1024*1024):  # 64GB
        self.node_id = node_id
        self.max_memory = max_memory
        self.data = {}
        self.ttl_data = {}
        self.access_times = {}
        self.memory_usage = 0
        self.eviction_policy = LRUEvictionPolicy()
        
    def get(self, key):
        # Check if key exists and not expired
        if key not in self.data:
            return None
            
        if self.is_expired(key):
            self.delete(key)
            return None
        
        # Update access time for LRU
        self.access_times[key] = time.time()
        
        return self.data[key]
    
    def set(self, key, value, ttl=None):
        # Check memory constraints
        value_size = self.calculate_size(value)
        
        if key in self.data:
            # Update existing key
            old_size = self.calculate_size(self.data[key])
            self.memory_usage += (value_size - old_size)
        else:
            # New key
            self.memory_usage += value_size + self.calculate_size(key)
        
        # Evict if necessary
        while self.memory_usage > self.max_memory:
            evicted_key = self.eviction_policy.evict(self.data, self.access_times)
            if evicted_key:
                self.delete(evicted_key)
            else:
                break  # No more keys to evict
        
        # Store the data
        self.data[key] = value
        self.access_times[key] = time.time()
        
        # Set TTL if provided
        if ttl:
            self.ttl_data[key] = time.time() + ttl
    
    def delete(self, key):
        if key in self.data:
            value_size = self.calculate_size(self.data[key])
            key_size = self.calculate_size(key)
            
            del self.data[key]
            del self.access_times[key]
            
            if key in self.ttl_data:
                del self.ttl_data[key]
            
            self.memory_usage -= (value_size + key_size)
            return True
        
        return False
    
    def is_expired(self, key):
        if key not in self.ttl_data:
            return False
        
        return time.time() > self.ttl_data[key]
    
    def cleanup_expired_keys(self):
        """Background task to clean up expired keys"""
        current_time = time.time()
        expired_keys = []
        
        for key, expiry_time in self.ttl_data.items():
            if current_time > expiry_time:
                expired_keys.append(key)
        
        for key in expired_keys:
            self.delete(key)
</code></pre>
<h3>Eviction Policies</h3>
<pre><code class="language-python">class LRUEvictionPolicy:
    def evict(self, data, access_times):
        if not access_times:
            return None
        
        # Find least recently used key
        lru_key = min(access_times.keys(), key=lambda k: access_times[k])
        return lru_key

class LFUEvictionPolicy:
    def __init__(self):
        self.access_counts = {}
    
    def evict(self, data, access_times):
        if not self.access_counts:
            return None
        
        # Find least frequently used key
        lfu_key = min(self.access_counts.keys(), key=lambda k: self.access_counts[k])
        return lfu_key
    
    def on_access(self, key):
        self.access_counts[key] = self.access_counts.get(key, 0) + 1

class TTLEvictionPolicy:
    def evict(self, data, access_times, ttl_data):
        # Prioritize expired keys
        current_time = time.time()
        
        for key, expiry_time in ttl_data.items():
            if current_time > expiry_time:
                return key
        
        # If no expired keys, fall back to LRU
        return LRUEvictionPolicy().evict(data, access_times)
</code></pre>
<h2>Replication and Consistency</h2>
<h3>Master-Slave Replication</h3>
<pre><code class="language-python">class ReplicationManager:
    def __init__(self, node_id, replication_factor=3):
        self.node_id = node_id
        self.replication_factor = replication_factor
        self.replicas = set()
        self.masters = set()
        
    def add_replica(self, replica_node):
        self.replicas.add(replica_node)
    
    def replicate_write(self, key, value, ttl=None):
        """Replicate write operation to all replicas"""
        operation = {
            "type": "SET",
            "key": key,
            "value": value,
            "ttl": ttl,
            "timestamp": time.time(),
            "node_id": self.node_id
        }
        
        # Synchronous replication to ensure consistency
        successful_replications = 0
        
        for replica in self.replicas:
            try:
                result = replica.apply_operation(operation)
                if result:
                    successful_replications += 1
            except Exception as e:
                # Log replication failure
                self.log_replication_error(replica, operation, str(e))
        
        # Require majority for success (quorum)
        required_replications = (self.replication_factor + 1) // 2
        
        if successful_replications >= required_replications:
            return True
        else:
            # Rollback operation if quorum not reached
            self.rollback_operation(operation)
            return False
    
    def handle_failover(self, failed_node):
        """Handle node failure and promote replica"""
        if failed_node in self.masters:
            # Promote a replica to master
            replica_to_promote = self.select_replica_for_promotion(failed_node)
            if replica_to_promote:
                self.promote_replica_to_master(replica_to_promote, failed_node)
        
        # Update routing tables
        self.update_cluster_topology(failed_node, "FAILED")
</code></pre>
<h3>Conflict Resolution</h3>
<pre><code class="language-python">class ConflictResolver:
    def resolve_write_conflict(self, operations):
        """Resolve conflicts using last-write-wins with vector clocks"""
        
        if len(operations) == 1:
            return operations[0]
        
        # Sort by timestamp (last write wins)
        sorted_ops = sorted(operations, key=lambda op: op['timestamp'])
        latest_operation = sorted_ops[-1]
        
        # For concurrent writes (same timestamp), use node_id as tiebreaker
        concurrent_ops = [op for op in sorted_ops if op['timestamp'] == latest_operation['timestamp']]
        
        if len(concurrent_ops) > 1:
            # Use lexicographic ordering of node_id
            latest_operation = min(concurrent_ops, key=lambda op: op['node_id'])
        
        return latest_operation
    
    def detect_concurrent_writes(self, operation1, operation2):
        """Detect if two operations are concurrent using vector clocks"""
        
        # Simple timestamp-based detection
        time_diff = abs(operation1['timestamp'] - operation2['timestamp'])
        
        # Consider operations concurrent if within 100ms
        return time_diff &#x3C; 0.1
</code></pre>
<h2>Cluster Management</h2>
<h3>Gossip Protocol for Cluster Coordination</h3>
<pre><code class="language-python">class GossipProtocol:
    def __init__(self, node_id, initial_nodes):
        self.node_id = node_id
        self.cluster_state = {}
        self.heartbeat_interval = 1  # 1 second
        self.failure_detection_timeout = 5  # 5 seconds
        
        # Initialize cluster state
        for node in initial_nodes:
            self.cluster_state[node] = {
                "status": "ALIVE",
                "last_seen": time.time(),
                "metadata": {}
            }
    
    def start_gossip(self):
        """Start gossip protocol background tasks"""
        threading.Thread(target=self.gossip_loop, daemon=True).start()
        threading.Thread(target=self.failure_detection_loop, daemon=True).start()
    
    def gossip_loop(self):
        """Periodically gossip cluster state with random nodes"""
        while True:
            try:
                # Select random subset of nodes to gossip with
                alive_nodes = [node for node, state in self.cluster_state.items() 
                              if state["status"] == "ALIVE" and node != self.node_id]
                
                if alive_nodes:
                    random_nodes = random.sample(alive_nodes, min(3, len(alive_nodes)))
                    
                    for node in random_nodes:
                        self.send_gossip_message(node)
                
                time.sleep(self.heartbeat_interval)
                
            except Exception as e:
                self.log_error(f"Gossip loop error: {e}")
    
    def send_gossip_message(self, target_node):
        """Send gossip message to target node"""
        message = {
            "type": "GOSSIP",
            "sender": self.node_id,
            "cluster_state": self.cluster_state,
            "timestamp": time.time()
        }
        
        try:
            response = self.send_message(target_node, message)
            if response:
                self.merge_cluster_state(response["cluster_state"])
        except Exception as e:
            # Mark node as potentially failed
            self.mark_node_suspect(target_node)
    
    def merge_cluster_state(self, remote_state):
        """Merge remote cluster state with local state"""
        for node, remote_info in remote_state.items():
            if node not in self.cluster_state:
                # New node discovered
                self.cluster_state[node] = remote_info
            else:
                # Update if remote info is newer
                local_info = self.cluster_state[node]
                if remote_info["last_seen"] > local_info["last_seen"]:
                    self.cluster_state[node] = remote_info
    
    def failure_detection_loop(self):
        """Detect failed nodes based on heartbeat timeouts"""
        while True:
            current_time = time.time()
            
            for node, state in self.cluster_state.items():
                if node == self.node_id:
                    continue
                
                time_since_seen = current_time - state["last_seen"]
                
                if (time_since_seen > self.failure_detection_timeout and 
                    state["status"] == "ALIVE"):
                    
                    self.mark_node_failed(node)
            
            time.sleep(self.heartbeat_interval)
    
    def mark_node_failed(self, node):
        """Mark node as failed and trigger failover"""
        self.cluster_state[node]["status"] = "FAILED"
        self.cluster_state[node]["last_seen"] = time.time()
        
        # Notify cluster about node failure
        self.broadcast_node_failure(node)
        
        # Trigger rebalancing if necessary
        self.trigger_rebalancing(node)
</code></pre>
<h2>Client Implementation</h2>
<h3>Smart Client with Connection Pooling</h3>
<pre><code class="language-python">class CacheClient:
    def __init__(self, cluster_nodes, pool_size=10):
        self.cluster_nodes = cluster_nodes
        self.consistent_hash = ConsistentHashing(cluster_nodes)
        self.connection_pools = {}
        
        # Create connection pools for each node
        for node in cluster_nodes:
            self.connection_pools[node] = ConnectionPool(node, pool_size)
    
    def get(self, key):
        """Get value for key with automatic retry and failover"""
        target_node = self.consistent_hash.get_node(key)
        replica_nodes = self.get_replica_nodes(key)
        
        # Try primary node first
        try:
            return self.execute_on_node(target_node, "GET", key)
        except NodeUnavailableException:
            # Try replica nodes
            for replica in replica_nodes:
                try:
                    return self.execute_on_node(replica, "GET", key)
                except NodeUnavailableException:
                    continue
            
            raise CacheUnavailableException(f"All nodes unavailable for key: {key}")
    
    def set(self, key, value, ttl=None):
        """Set key-value with replication"""
        target_node = self.consistent_hash.get_node(key)
        replica_nodes = self.get_replica_nodes(key)
        
        # Write to primary node
        success = self.execute_on_node(target_node, "SET", key, value, ttl)
        
        if success:
            # Asynchronously replicate to replicas
            self.async_replicate(replica_nodes, "SET", key, value, ttl)
        
        return success
    
    def execute_on_node(self, node, operation, *args):
        """Execute operation on specific node"""
        connection = self.connection_pools[node].get_connection()
        
        try:
            if operation == "GET":
                return connection.get(args[0])
            elif operation == "SET":
                return connection.set(args[0], args[1], args[2] if len(args) > 2 else None)
            elif operation == "DELETE":
                return connection.delete(args[0])
        finally:
            self.connection_pools[node].return_connection(connection)
    
    def get_replica_nodes(self, key):
        """Get replica nodes for a given key"""
        primary_node = self.consistent_hash.get_node(key)
        
        # Get next N nodes in the ring as replicas
        replicas = []
        nodes = list(self.cluster_nodes)
        primary_index = nodes.index(primary_node)
        
        for i in range(1, 4):  # 3 replicas
            replica_index = (primary_index + i) % len(nodes)
            replicas.append(nodes[replica_index])
        
        return replicas
</code></pre>
<h2>Performance Monitoring</h2>
<h3>Metrics Collection</h3>
<pre><code class="language-python">class CacheMetrics:
    def __init__(self):
        self.hit_count = 0
        self.miss_count = 0
        self.operation_latencies = []
        self.memory_usage = 0
        self.eviction_count = 0
        
    def record_hit(self):
        self.hit_count += 1
    
    def record_miss(self):
        self.miss_count += 1
    
    def record_latency(self, operation, latency_ms):
        self.operation_latencies.append({
            "operation": operation,
            "latency": latency_ms,
            "timestamp": time.time()
        })
        
        # Keep only last 1000 measurements
        if len(self.operation_latencies) > 1000:
            self.operation_latencies = self.operation_latencies[-1000:]
    
    def get_hit_ratio(self):
        total_requests = self.hit_count + self.miss_count
        if total_requests == 0:
            return 0
        return self.hit_count / total_requests
    
    def get_average_latency(self, operation=None):
        if operation:
            latencies = [l["latency"] for l in self.operation_latencies if l["operation"] == operation]
        else:
            latencies = [l["latency"] for l in self.operation_latencies]
        
        if not latencies:
            return 0
        
        return sum(latencies) / len(latencies)
    
    def get_p99_latency(self, operation=None):
        if operation:
            latencies = [l["latency"] for l in self.operation_latencies if l["operation"] == operation]
        else:
            latencies = [l["latency"] for l in self.operation_latencies]
        
        if not latencies:
            return 0
        
        sorted_latencies = sorted(latencies)
        p99_index = int(0.99 * len(sorted_latencies))
        return sorted_latencies[p99_index]
</code></pre>
<h2>Distributed Cache Design Quiz</h2>
<p>Test your understanding of distributed cache system design with the interactive quiz that appears after each part of this series.</p>
<h2>Advanced Features</h2>
<h3>Pub/Sub Implementation</h3>
<pre><code class="language-python">class PubSubManager:
    def __init__(self):
        self.subscriptions = {}  # channel -> set of subscribers
        self.pattern_subscriptions = {}  # pattern -> set of subscribers
        
    def subscribe(self, client_id, channel):
        if channel not in self.subscriptions:
            self.subscriptions[channel] = set()
        self.subscriptions[channel].add(client_id)
    
    def unsubscribe(self, client_id, channel):
        if channel in self.subscriptions:
            self.subscriptions[channel].discard(client_id)
    
    def publish(self, channel, message):
        # Direct channel subscribers
        if channel in self.subscriptions:
            for subscriber in self.subscriptions[channel]:
                self.send_message_to_client(subscriber, channel, message)
        
        # Pattern subscribers
        for pattern, subscribers in self.pattern_subscriptions.items():
            if self.matches_pattern(channel, pattern):
                for subscriber in subscribers:
                    self.send_message_to_client(subscriber, channel, message)
</code></pre>
<h3>Memory Management</h3>
<pre><code class="language-python">class MemoryManager:
    def __init__(self, max_memory):
        self.max_memory = max_memory
        self.current_usage = 0
        self.fragmentation_threshold = 0.1
        
    def should_evict(self):
        return self.current_usage > self.max_memory * 0.9
    
    def calculate_fragmentation(self):
        # Simplified fragmentation calculation
        allocated_memory = sum(sys.getsizeof(obj) for obj in self.data.values())
        return 1 - (allocated_memory / self.current_usage)
    
    def defragment_memory(self):
        if self.calculate_fragmentation() > self.fragmentation_threshold:
            # Trigger garbage collection
            gc.collect()
            
            # Reorganize data structure if needed
            self.reorganize_data_structures()
</code></pre>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Consistent Hashing</strong>: Essential for distributed data partitioning</li>
<li><strong>Replication Strategy</strong>: Balance consistency, availability, and performance</li>
<li><strong>Failure Detection</strong>: Use gossip protocols for robust cluster management</li>
<li><strong>Smart Clients</strong>: Implement client-side logic for routing and failover</li>
<li><strong>Memory Management</strong>: Efficient eviction policies and memory monitoring</li>
</ol>
<h2>Series Conclusion</h2>
<p>Congratulations! You've completed the System Design Mastery series. You've learned to design:</p>
<ol>
<li><strong>URL Shortener</strong>: Read-heavy systems with caching</li>
<li><strong>Chat System</strong>: Real-time communication and WebSockets</li>
<li><strong>Social Media Feed</strong>: Content ranking and viral traffic handling</li>
<li><strong>Video Streaming</strong>: Large file storage and global CDN</li>
<li><strong>Distributed Cache</strong>: Consistency and distributed coordination</li>
</ol>
<h3>Final Interview Tips</h3>
<ol>
<li><strong>Practice Regularly</strong>: Work through problems weekly</li>
<li><strong>Think Out Loud</strong>: Communicate your reasoning clearly</li>
<li><strong>Start Simple</strong>: Begin with basic design, then add complexity</li>
<li><strong>Consider Trade-offs</strong>: Discuss pros and cons of each decision</li>
<li><strong>Learn from Real Systems</strong>: Study how companies like Google, Facebook, and Netflix solve similar problems</li>
</ol>
<h3>Continue Learning</h3>
<ul>
<li>Study real-world system architectures</li>
<li>Read engineering blogs from top tech companies</li>
<li>Practice with system design interview platforms</li>
<li>Build distributed systems to gain hands-on experience</li>
<li>Stay current with emerging technologies and patterns</li>
</ul>
<p><strong>You're now ready to tackle any system design interview with confidence!</strong></p>
2:["$","article",null,{"className":"min-h-screen bg-white","children":[["$","header",null,{"className":"bg-white border-b border-gray-200","children":["$","div",null,{"className":"medium-container py-12","children":[["$","div",null,{"className":"max-w-3xl mx-auto text-center","children":[["$","div",null,{"className":"mb-6","children":[["$","div",null,{"className":"flex flex-wrap justify-center gap-2 mb-6","children":[["$","span","system-design",{"className":"tag","children":"system-design"}],["$","span","interview",{"className":"tag","children":"interview"}],["$","span","scalability",{"className":"tag","children":"scalability"}],["$","span","architecture",{"className":"tag","children":"architecture"}]]}],["$","h1",null,{"className":"text-4xl md:text-5xl font-bold text-gray-900 mb-6 leading-tight","children":"Design a Video Streaming Service (YouTube)"}],["$","p",null,{"className":"text-xl text-gray-600 leading-relaxed","children":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice."}]]}],["$","div",null,{"className":"flex items-center justify-center space-x-6 text-gray-500","children":[["$","div",null,{"className":"flex items-center space-x-2","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-user w-5 h-5","children":[["$","path","975kel",{"d":"M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2"}],["$","circle","17ys0d",{"cx":"12","cy":"7","r":"4"}],"$undefined"]}],["$","span",null,{"className":"font-medium","children":"Abstract Algorithms"}]]}],["$","div",null,{"className":"flex items-center space-x-2","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar w-5 h-5","children":[["$","rect","eu3xkr",{"width":"18","height":"18","x":"3","y":"4","rx":"2","ry":"2"}],["$","line","m3sa8f",{"x1":"16","x2":"16","y1":"2","y2":"6"}],["$","line","18kwsl",{"x1":"8","x2":"8","y1":"2","y2":"6"}],["$","line","xt86sb",{"x1":"3","x2":"21","y1":"10","y2":"10"}],"$undefined"]}],["$","span",null,{"children":"about 1 year ago"}]]}],["$","div",null,{"className":"flex items-center space-x-2","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-clock w-5 h-5","children":[["$","circle","1mglay",{"cx":"12","cy":"12","r":"10"}],["$","polyline","68esgv",{"points":"12 6 12 12 16 14"}],"$undefined"]}],["$","span",null,{"children":"11 min read"}]]}]]}],["$","div",null,{"className":"flex items-center justify-center space-x-4 mt-8","children":[["$","button",null,{"className":"flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-share2 w-4 h-4","children":[["$","circle","gq8acd",{"cx":"18","cy":"5","r":"3"}],["$","circle","w7nqdw",{"cx":"6","cy":"12","r":"3"}],["$","circle","1xt0gg",{"cx":"18","cy":"19","r":"3"}],["$","line","47mynk",{"x1":"8.59","x2":"15.42","y1":"13.51","y2":"17.49"}],["$","line","1n3mei",{"x1":"15.41","x2":"8.59","y1":"6.51","y2":"10.49"}],"$undefined"]}],["$","span",null,{"children":"Share"}]]}],["$","button",null,{"className":"flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-bookmark w-4 h-4","children":[["$","path","1fy3hk",{"d":"m19 21-7-4-7 4V5a2 2 0 0 1 2-2h10a2 2 0 0 1 2 2v16z"}],"$undefined"]}],["$","span",null,{"children":"Save"}]]}]]}]]}],["$","div",null,{"className":"max-w-4xl mx-auto mt-12","children":["$","div",null,{"className":"relative aspect-video rounded-xl overflow-hidden shadow-lg","children":["$","$Lc",null,{"src":"/posts/system-design-interview/assets/intro.png","alt":"Design a Video Streaming Service (YouTube)","fill":true,"className":"object-cover","priority":true}]}]}]]}]}],["$","nav",null,{"className":"flex flex-col items-center my-8","children":[["$","div",null,{"className":"text-sm text-gray-500 mb-2","children":["System Design Mastery"," (Part ",5," of ",6,")"]}],["$","div",null,{"className":"flex gap-4","children":[["$","$Ld",null,{"href":"/posts/system-design-interview/part-4","className":"px-4 py-2 bg-gray-100 rounded hover:bg-gray-200 text-gray-700 font-medium transition-colors","children":"← Previous"}],["$","span",null,{"className":"px-4 py-2 text-green-700 font-semibold","children":"Current"}],["$","$Ld",null,{"href":"/posts/system-design-interview/part-6","className":"px-4 py-2 bg-green-100 rounded hover:bg-green-200 text-green-700 font-medium transition-colors","children":"Next →"}]]}]]}],["$","div",null,{"className":"medium-container py-8","children":["$","div",null,{"className":"max-w-3xl mx-auto","children":[["$","$Le",null,{"slug":"system-design-interview/part-5"}],["$","$Lf",null,{}]]}]}],["$","$L10",null,{"posts":[{"slug":"hash-tables-ultimate-guide","title":"Hash Tables: The Ultimate Guide","date":"2024-04-05","excerpt":"Comprehensive exploration of hash tables, from core concepts to advanced techniques, enhanced with illustrative graphics.","content":"$11","author":"Abstract Algorithms","tags":["data-structures","hash-tables","algorithms","performance"],"readingTime":"4 min read","coverImage":"/posts/hash-tables-ultimate-guide/assets/overview.png","fixedUrl":"$undefined","series":"$undefined"},{"slug":"system-design-interview","title":"System Design Mastery: Complete Guide","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$12","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"6 min read","coverImage":"/posts/system-design-interview/assets/intro.png","fixedUrl":"$undefined","series":{"name":"System Design Mastery","order":1,"total":6,"prev":null,"next":"/posts/system-design-interview/part-2"}},{"slug":"system-design-interview/part-2","title":"Introduction & Methodology","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$13","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"7 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":2,"total":6,"prev":"/posts/system-design-interview","next":"/posts/system-design-interview/part-3","parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}},{"slug":"system-design-interview/part-3","title":"Design a URL Shortener (TinyURL)","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$14","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"9 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":3,"total":6,"prev":"/posts/system-design-interview/part-2","next":"/posts/system-design-interview/part-4","parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}},{"slug":"system-design-interview/part-4","title":"Design a Chat System (WhatsApp)","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$15","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"10 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":4,"total":6,"prev":"/posts/system-design-interview/part-3","next":"/posts/system-design-interview/part-5","parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}},{"slug":"system-design-interview/part-6","title":"Design a Video Streaming Service (YouTube)","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$16","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"12 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":6,"total":6,"prev":"/posts/system-design-interview/part-5","next":null,"parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}}]}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"Design a Video Streaming Service (YouTube)\",\"description\":\"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.\",\"datePublished\":\"2024-04-01\",\"dateModified\":\"2024-04-01\",\"author\":{\"@type\":\"Person\",\"name\":\"Abstract Algorithms\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Abstract Algorithms\",\"url\":\"https://abstractalgorithms.github.io\"},\"url\":\"https://abstractalgorithms.github.io/posts/system-design-interview/part-5\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://abstractalgorithms.github.io/posts/system-design-interview/part-5\"},\"image\":{\"@type\":\"ImageObject\",\"url\":\"https://abstractalgorithms.github.io/posts/system-design-interview/assets/intro.png\"}}"}}]]}]
b:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Design a Video Streaming Service (YouTube) | Abstract Algorithms"}],["$","meta","3",{"name":"description","content":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice."}],["$","meta","4",{"name":"author","content":"Abstract Algorithms"}],["$","meta","5",{"name":"keywords","content":"algorithms,data structures,system design,software engineering,programming,computer science,performance optimization,big o notation,hash tables,database indexing"}],["$","meta","6",{"name":"creator","content":"Abstract Algorithms"}],["$","meta","7",{"name":"publisher","content":"Abstract Algorithms"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","meta","10",{"property":"og:title","content":"Design a Video Streaming Service (YouTube)"}],["$","meta","11",{"property":"og:description","content":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice."}],["$","meta","12",{"property":"og:type","content":"article"}],["$","meta","13",{"property":"article:published_time","content":"2024-04-01"}],["$","meta","14",{"property":"article:author","content":"Abstract Algorithms"}],["$","meta","15",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","16",{"name":"twitter:title","content":"Abstract Algorithms"}],["$","meta","17",{"name":"twitter:description","content":"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices"}],["$","link","18",{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link","19",{"rel":"icon","href":"/icon.svg","type":"image/svg+xml","sizes":"32x32"}],["$","link","20",{"rel":"apple-touch-icon","href":"/apple-icon.svg","type":"image/svg+xml","sizes":"180x180"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
