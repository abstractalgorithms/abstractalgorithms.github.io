export const metadata = {
  "postId": "2a8f6e4c-7b5d-4e9a-a1c3-6d8e9f0a1b2c",
  "title": "LLM Engineering Mastery: Part 3 - Production Deployment and Scaling",
  "date": "2024-02-10",
  "excerpt": "Part 3 of the LLM Engineering Mastery series: Master production deployment, scaling strategies, monitoring, and security for enterprise-grade LLM applications.",
  "author": "Abstract Algorithms",
  "tags": [
    "llm",
    "production",
    "deployment",
    "scaling",
    "monitoring",
    "security"
  ],
  "coverImage": "./assets/production-deployment-cover.png",
  "series": {
    "name": "LLM Engineering Mastery",
    "order": 3,
    "total": 3,
    "prev": "/posts/llm-engineering-mastery-part-2/",
    "overview": "/posts/llm-engineering-mastery-series/"
  }
};


# LLM Engineering Mastery: Part 3 - Production Deployment and Scaling

In this final part of the LLM Engineering Mastery series, we'll cover everything you need to deploy, scale, and maintain LLM applications in production environments. From infrastructure patterns to monitoring and security, this guide provides the practical knowledge needed for enterprise-grade deployments.

## Infrastructure Patterns for LLM Applications

### 1. Microservices Architecture for LLM Systems

```python
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional
import asyncio
import httpx
from datetime import datetime
import logging

# Data models
class ChatRequest(BaseModel):
    messages: List[dict]
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7
    max_tokens: int = 1000

class RAGRequest(BaseModel):
    query: str
    collection: str = "default"
    top_k: int = 5

class ChatResponse(BaseModel):
    response: str
    model_used: str
    tokens_used: int
    processing_time: float
    request_id: str

# LLM Service
class LLMService:
    def __init__(self):
        self.app = FastAPI(title="LLM Service", version="1.0.0")
        self.setup_routes()
        self.setup_middleware()
    
    def setup_middleware(self):
        @self.app.middleware("http")
        async def log_requests(request, call_next):
            start_time = datetime.now()
            
            response = await call_next(request)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            logging.info(
                "Request processed",
                extra={
                    "method": request.method,
                    "url": str(request.url),
                    "status_code": response.status_code,
                    "processing_time": processing_time
                }
            )
            
            return response
    
    def setup_routes(self):
        @self.app.post("/chat/completions", response_model=ChatResponse)
        async def chat_completion(request: ChatRequest):
            start_time = datetime.now()
            
            try:
                # Route to appropriate model provider
                if request.model.startswith("gpt"):
                    result = await self._call_openai(request)
                elif request.model.startswith("claude"):
                    result = await self._call_anthropic(request)
                else:
                    raise HTTPException(status_code=400, detail="Unsupported model")
                
                processing_time = (datetime.now() - start_time).total_seconds()
                
                return ChatResponse(
                    response=result["content"],
                    model_used=request.model,
                    tokens_used=result["tokens"],
                    processing_time=processing_time,
                    request_id=result["request_id"]
                )
                
            except Exception as e:
                logging.error("Chat completion failed", extra={"error": str(e)})
                raise HTTPException(status_code=500, detail="Internal server error")
        
        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy", "timestamp": datetime.now().isoformat()}
        
        @self.app.get("/models")
        async def list_models():
            return {
                "available_models": [
                    "gpt-3.5-turbo",
                    "gpt-4-turbo", 
                    "claude-3-sonnet",
                    "claude-3-haiku"
                ]
            }
    
    async def _call_openai(self, request: ChatRequest) -> dict:
        # Implementation for OpenAI API calls
        # This would include the robust client from Part 1
        pass
    
    async def _call_anthropic(self, request: ChatRequest) -> dict:
        # Implementation for Anthropic API calls
        pass

# RAG Service
class RAGService:
    def __init__(self, llm_service_url: str):
        self.app = FastAPI(title="RAG Service", version="1.0.0")
        self.llm_service_url = llm_service_url
        self.setup_routes()
    
    def setup_routes(self):
        @self.app.post("/rag/query")
        async def rag_query(request: RAGRequest):
            try:
                # Retrieve relevant documents
                relevant_docs = await self._retrieve_documents(
                    request.query, 
                    request.collection, 
                    request.top_k
                )
                
                # Build context
                context = self._build_context(relevant_docs)
                
                # Generate response using LLM service
                llm_request = ChatRequest(
                    messages=[
                        {
                            "role": "system",
                            "content": "Answer based on the provided context."
                        },
                        {
                            "role": "user", 
                            "content": "Context:\n" + context + "\n\nQuestion: " + request.query
                        }
                    ]
                )
                
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        self.llm_service_url + "/chat/completions",
                        json=llm_request.dict()
                    )
                    response.raise_for_status()
                    llm_response = response.json()
                
                return {
                    "answer": llm_response["response"],
                    "sources": relevant_docs,
                    "tokens_used": llm_response["tokens_used"]
                }
                
            except Exception as e:
                logging.error("RAG query failed", extra={"error": str(e)})
                raise HTTPException(status_code=500, detail="RAG processing failed")
    
    async def _retrieve_documents(self, query: str, collection: str, top_k: int):
        # Implementation for document retrieval
        # This would use the vector store from Part 2
        pass
    
    def _build_context(self, documents: List[dict]) -> str:
        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append("Document " + str(i) + ":")
            context_parts.append(doc["content"])
            context_parts.append("")
        return "\n".join(context_parts)

# API Gateway
class APIGateway:
    def __init__(self, llm_service_url: str, rag_service_url: str):
        self.app = FastAPI(title="LLM API Gateway", version="1.0.0")
        self.llm_service_url = llm_service_url
        self.rag_service_url = rag_service_url
        self.setup_routes()
        self.setup_middleware()
    
    def setup_middleware(self):
        # Rate limiting, authentication, etc.
        pass
    
    def setup_routes(self):
        @self.app.post("/v1/chat/completions")
        async def proxy_chat(request: ChatRequest):
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    self.llm_service_url + "/chat/completions",
                    json=request.dict(),
                    timeout=60.0
                )
                response.raise_for_status()
                return response.json()
        
        @self.app.post("/v1/rag/query")
        async def proxy_rag(request: RAGRequest):
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    self.rag_service_url + "/rag/query",
                    json=request.dict(),
                    timeout=60.0
                )
                response.raise_for_status()
                return response.json()

# Docker Compose for local development
docker_compose_content = """
version: '3.8'

services:
  llm-service:
    build: ./llm-service
    ports:
      - "8001:8000"
    environment:      - OPENAI_API_KEY=\$\{OPENAI_API_KEY\}
      - ANTHROPIC_API_KEY=\$\{ANTHROPIC_API_KEY\}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  rag-service:
    build: ./rag-service
    ports:
      - "8002:8000"
    environment:
      - LLM_SERVICE_URL=http://llm-service:8000
      - VECTOR_DB_URL=\$\{VECTOR_DB_URL\}
    depends_on:
      - llm-service
      - vector-db
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  api-gateway:
    build: ./api-gateway
    ports:
      - "8000:8000"
    environment:
      - LLM_SERVICE_URL=http://llm-service:8000
      - RAG_SERVICE_URL=http://rag-service:8000
    depends_on:
      - llm-service
      - rag-service

  vector-db:
    image: chromadb/chroma:latest
    ports:
      - "8003:8000"
    volumes:
      - vector_data:/chroma/chroma

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

volumes:
  vector_data:
"""
```

### 2. Kubernetes Deployment Configuration

```yaml
# llm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
  labels:
    app: llm-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-service
  template:
    metadata:
      labels:
        app: llm-service
    spec:
      containers:
      - name: llm-service
        image: your-registry/llm-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-api-key
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: anthropic-api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm-service
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

---
# Ingress for external access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-ingress
  annotations:
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.yourdomain.com
    secretName: llm-tls
  rules:
  - host: api.yourdomain.com
    http:
      paths:
      - path: /v1
        pathType: Prefix
        backend:
          service:
            name: api-gateway
            port:
              number: 80
```

## Monitoring and Observability

### 1. Comprehensive Monitoring System

```python
import logging
import time
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from functools import wraps
import structlog
from typing import Any, Callable
import asyncio

# Prometheus metrics
REQUEST_COUNT = Counter(
    'llm_requests_total',
    'Total number of LLM requests',
    ['model', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'llm_request_duration_seconds',
    'Time spent processing LLM requests',
    ['model', 'endpoint']
)

TOKEN_USAGE = Counter(
    'llm_tokens_total',
    'Total number of tokens processed',
    ['model', 'type']  # type: input/output
)

COST_TRACKING = Counter(
    'llm_cost_total_usd',
    'Total cost in USD',
    ['model', 'provider']
)

ACTIVE_REQUESTS = Gauge(
    'llm_active_requests',
    'Number of currently active requests',
    ['model']
)

ERROR_RATE = Counter(
    'llm_errors_total',
    'Total number of errors',
    ['model', 'error_type']
)

class MetricsCollector:
    def __init__(self):
        self.logger = structlog.get_logger()
    
    def record_request(self, model: str, endpoint: str, status: str):
        """Record a request with its status"""
        REQUEST_COUNT.labels(model=model, endpoint=endpoint, status=status).inc()
    
    def record_duration(self, model: str, endpoint: str, duration: float):
        """Record request duration"""
        REQUEST_DURATION.labels(model=model, endpoint=endpoint).observe(duration)
    
    def record_token_usage(self, model: str, input_tokens: int, output_tokens: int):
        """Record token usage"""
        TOKEN_USAGE.labels(model=model, type='input').inc(input_tokens)
        TOKEN_USAGE.labels(model=model, type='output').inc(output_tokens)
    
    def record_cost(self, model: str, provider: str, cost: float):
        """Record cost"""
        COST_TRACKING.labels(model=model, provider=provider).inc(cost)
    
    def record_error(self, model: str, error_type: str):
        """Record error"""
        ERROR_RATE.labels(model=model, error_type=error_type).inc()
    
    def track_active_request(self, model: str, increment: bool = True):
        """Track active requests"""
        if increment:
            ACTIVE_REQUESTS.labels(model=model).inc()
        else:
            ACTIVE_REQUESTS.labels(model=model).dec()

# Monitoring decorator
def monitor_llm_request(model: str, endpoint: str):
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def async_wrapper(*args, **kwargs) -> Any:
            metrics = MetricsCollector()
            start_time = time.time()
            
            metrics.track_active_request(model, increment=True)
            
            try:
                result = await func(*args, **kwargs)
                
                # Record success metrics
                duration = time.time() - start_time
                metrics.record_request(model, endpoint, 'success')
                metrics.record_duration(model, endpoint, duration)
                
                # Record token usage if available
                if hasattr(result, 'tokens_used'):
                    metrics.record_token_usage(
                        model, 
                        result.input_tokens, 
                        result.output_tokens
                    )
                
                return result
                
            except Exception as e:
                # Record error metrics
                duration = time.time() - start_time
                metrics.record_request(model, endpoint, 'error')
                metrics.record_duration(model, endpoint, duration)
                metrics.record_error(model, type(e).__name__)
                
                # Log structured error
                structlog.get_logger().error(
                    "LLM request failed",
                    model=model,
                    endpoint=endpoint,
                    error=str(e),
                    duration=duration
                )
                
                raise
            
            finally:
                metrics.track_active_request(model, increment=False)
        
        return async_wrapper
    return decorator

# Usage example
class MonitoredLLMClient:
    def __init__(self, model: str):
        self.model = model
        self.metrics = MetricsCollector()
    
    @monitor_llm_request("gpt-3.5-turbo", "chat_completion")
    async def chat_completion(self, messages: list, **kwargs):
        # Your LLM API call implementation
        pass

# Structured logging configuration
def setup_logging():
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

# Health check endpoint with detailed diagnostics
class HealthChecker:
    def __init__(self, llm_client, vector_store):
        self.llm_client = llm_client
        self.vector_store = vector_store
    
    async def comprehensive_health_check(self) -> dict:
        """Perform comprehensive health check"""
        checks = {}
        overall_healthy = True
        
        # Check LLM service connectivity
        try:
            test_response = await self.llm_client.complete([
                {"role": "user", "content": "Health check test"}
            ], max_tokens=5)
            
            checks["llm_service"] = {
                "status": "healthy",
                "response_time": 0.5,  # Calculate actual response time
                "last_check": time.time()
            }
        except Exception as e:
            checks["llm_service"] = {
                "status": "unhealthy",
                "error": str(e),
                "last_check": time.time()
            }
            overall_healthy = False
        
        # Check vector store connectivity
        try:
            # Test vector store query
            test_results = self.vector_store.search("health check", top_k=1)
            
            checks["vector_store"] = {
                "status": "healthy",
                "documents_count": len(test_results),
                "last_check": time.time()
            }
        except Exception as e:
            checks["vector_store"] = {
                "status": "unhealthy", 
                "error": str(e),
                "last_check": time.time()
            }
            overall_healthy = False
        
        # Check system resources
        import psutil
        
        checks["system_resources"] = {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent
        }
        
        # Check if resources are within acceptable limits
        if (checks["system_resources"]["cpu_percent"] > 90 or 
            checks["system_resources"]["memory_percent"] > 90):
            overall_healthy = False
        
        return {
            "status": "healthy" if overall_healthy else "unhealthy",
            "timestamp": time.time(),
            "checks": checks
        }

# Start metrics server
def start_metrics_server(port: int = 8080):
    start_http_server(port)
    print("Metrics server started on port " + str(port))
```

### 2. Custom Dashboards and Alerting

```python
# Grafana dashboard configuration (JSON)
grafana_dashboard = {
    "dashboard": {
        "title": "LLM Application Monitoring",
        "panels": [
            {
                "title": "Request Rate",
                "type": "graph",
                "targets": [
                    {
                        "expr": "rate(llm_requests_total[5m])",
                        "legendFormat": "\\{\\{model\\}\\} - \\{\\{endpoint\\}\\}"
                    }
                ]
            },
            {
                "title": "Response Time",
                "type": "graph", 
                "targets": [
                    {
                        "expr": "histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m]))",
                        "legendFormat": "95th percentile"
                    },
                    {
                        "expr": "histogram_quantile(0.50, rate(llm_request_duration_seconds_bucket[5m]))",
                        "legendFormat": "50th percentile"
                    }
                ]
            },
            {
                "title": "Error Rate",
                "type": "graph",
                "targets": [
                    {
                        "expr": "rate(llm_errors_total[5m]) / rate(llm_requests_total[5m])",
                        "legendFormat": "Error Rate"
                    }
                ]
            },
            {
                "title": "Token Usage",
                "type": "graph",
                "targets": [
                    {
                        "expr": "rate(llm_tokens_total[5m])",
                        "legendFormat": "\\{\\{type\\}\\} tokens"
                    }
                ]
            },
            {
                "title": "Cost Tracking",
                "type": "singlestat",
                "targets": [
                    {
                        "expr": "sum(llm_cost_total_usd)",
                        "legendFormat": "Total Cost (USD)"
                    }
                ]
            }
        ]
    }
}

# Alerting rules for Prometheus
alerting_rules = """
groups:
- name: llm_application_alerts
  rules:
  - alert: HighErrorRate
    expr: rate(llm_errors_total[5m]) / rate(llm_requests_total[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"
      description: "Error rate is \\{\\{ $value | humanizePercentage \\}\\} for the last 5 minutes"

  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m])) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High response time detected"
      description: "95th percentile response time is \\{\\{ $value \\}\\}s"

  - alert: ServiceDown
    expr: up{job="llm-service"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "LLM service is down"
      description: "LLM service has been down for more than 1 minute"

  - alert: HighCostBurn
    expr: increase(llm_cost_total_usd[1h]) > 50
    for: 0m
    labels:
      severity: warning
    annotations:
      summary: "High cost burn rate"
      description: "Cost increased by $\\{\\{ $value \\}\\} in the last hour"
"""

# Slack alerting integration
import requests
import json

class SlackAlerter:
    def __init__(self, webhook_url: str, channel: str = "#alerts"):
        self.webhook_url = webhook_url
        self.channel = channel
    
    def send_alert(self, title: str, message: str, severity: str = "warning"):
        """Send alert to Slack"""
        
        color_map = {
            "info": "#36a64f",     # green
            "warning": "#ffaa00",  # orange  
            "critical": "#ff0000"  # red
        }
        
        payload = {
            "channel": self.channel,
            "username": "LLM Monitor",
            "attachments": [
                {
                    "color": color_map.get(severity, "#808080"),
                    "title": title,
                    "text": message,
                    "fields": [
                        {
                            "title": "Severity",
                            "value": severity.upper(),
                            "short": True
                        },
                        {
                            "title": "Timestamp", 
                            "value": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "short": True
                        }
                    ]
                }
            ]
        }
        
        try:
            response = requests.post(
                self.webhook_url,
                data=json.dumps(payload),
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            response.raise_for_status()
        except Exception as e:
            logging.error("Failed to send Slack alert", extra={"error": str(e)})
```

## Security and Compliance

### 1. Authentication and Authorization

```python
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt
from datetime import datetime, timedelta
import hashlib
import secrets
from typing import Optional, List
import redis
import asyncio

class SecurityManager:
    def __init__(self, secret_key: str, redis_client: redis.Redis):
        self.secret_key = secret_key
        self.redis_client = redis_client
        self.security = HTTPBearer()
    
    def create_access_token(self, user_id: str, scopes: List[str]) -> str:
        """Create JWT access token with scopes"""
        to_encode = {
            "sub": user_id,
            "scopes": scopes,
            "exp": datetime.utcnow() + timedelta(hours=24),
            "iat": datetime.utcnow(),
            "type": "access"
        }
        
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm="HS256")
        return encoded_jwt
    
    def create_api_key(self, user_id: str, name: str, scopes: List[str]) -> tuple:
        """Create API key for service-to-service communication"""
        api_key = "ak_" + secrets.token_urlsafe(32)
        api_secret = secrets.token_urlsafe(64)
        
        # Hash the secret for storage
        secret_hash = hashlib.sha256(api_secret.encode()).hexdigest()
        
        # Store in Redis
        key_data = {
            "user_id": user_id,
            "name": name,
            "scopes": ",".join(scopes),
            "secret_hash": secret_hash,
            "created_at": datetime.utcnow().isoformat(),
            "last_used": None
        }
        
        self.redis_client.hset("api_keys:" + api_key, mapping=key_data)
        
        return api_key, api_secret
    
    async def verify_token(self, credentials: HTTPAuthorizationCredentials) -> dict:
        """Verify JWT token"""
        try:
            payload = jwt.decode(
                credentials.credentials, 
                self.secret_key, 
                algorithms=["HS256"]
            )
            
            user_id = payload.get("sub")
            scopes = payload.get("scopes", [])
            
            if user_id is None:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token"
                )
            
            return {"user_id": user_id, "scopes": scopes}
            
        except jwt.ExpiredSignatureError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired"
            )
        except jwt.JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
    
    async def verify_api_key(self, api_key: str, api_secret: str) -> dict:
        """Verify API key and secret"""
        key_data = self.redis_client.hgetall("api_keys:" + api_key)
        
        if not key_data:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid API key"
            )
        
        # Verify secret
        secret_hash = hashlib.sha256(api_secret.encode()).hexdigest()
        if secret_hash != key_data[b"secret_hash"].decode():
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid API secret"
            )
        
        # Update last used timestamp
        self.redis_client.hset(
            "api_keys:" + api_key, 
            "last_used", 
            datetime.utcnow().isoformat()
        )
        
        return {
            "user_id": key_data[b"user_id"].decode(),
            "scopes": key_data[b"scopes"].decode().split(",")
        }
    
    def require_scope(self, required_scope: str):
        """Decorator to require specific scope"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Extract auth info from kwargs or dependency injection
                auth_info = kwargs.get("auth_info")
                if not auth_info or required_scope not in auth_info.get("scopes", []):
                    raise HTTPException(
                        status_code=status.HTTP_403_FORBIDDEN,
                        detail="Insufficient permissions"
                    )
                return await func(*args, **kwargs)
            return wrapper
        return decorator

# Rate limiting
class RateLimiter:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
    
    async def is_allowed(
        self, 
        key: str, 
        limit: int, 
        window_seconds: int
    ) -> tuple[bool, dict]:
        """Check if request is allowed under rate limit"""
        
        current_time = int(time.time())
        window_start = current_time - window_seconds
        
        pipe = self.redis_client.pipeline()
        
        # Remove old entries
        pipe.zremrangebyscore(key, 0, window_start)
        
        # Count current requests
        pipe.zcard(key)
        
        # Add current request
        pipe.zadd(key, {str(current_time): current_time})
        
        # Set expiry
        pipe.expire(key, window_seconds)
        
        results = pipe.execute()
        current_requests = results[1]
        
        allowed = current_requests < limit
        
        return allowed, {
            "limit": limit,
            "current": current_requests,
            "remaining": max(0, limit - current_requests - 1),
            "reset_time": current_time + window_seconds
        }

# Secure FastAPI application
def create_secure_app() -> FastAPI:
    app = FastAPI(title="Secure LLM API")
    
    redis_client = redis.Redis(host='localhost', port=6379, db=0)
    security_manager = SecurityManager("your-secret-key", redis_client)
    rate_limiter = RateLimiter(redis_client)
    
    @app.middleware("http")
    async def security_middleware(request, call_next):
        # Add security headers
        response = await call_next(request)
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        return response
    
    async def get_current_user(
        credentials: HTTPAuthorizationCredentials = Depends(security_manager.security)
    ):
        return await security_manager.verify_token(credentials)
    
    @app.post("/v1/chat/completions")
    @security_manager.require_scope("llm:chat")
    async def secure_chat_completion(
        request: ChatRequest,
        auth_info: dict = Depends(get_current_user)
    ):
        user_id = auth_info["user_id"]
        
        # Apply rate limiting
        allowed, rate_info = await rate_limiter.is_allowed(
            "user:" + user_id,
            limit=100,  # 100 requests per hour
            window_seconds=3600
        )
        
        if not allowed:
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="Rate limit exceeded",
                headers={
                    "X-RateLimit-Limit": str(rate_info["limit"]),
                    "X-RateLimit-Remaining": str(rate_info["remaining"]),
                    "X-RateLimit-Reset": str(rate_info["reset_time"])
                }
            )
        
        # Process the request
        # ... your chat completion logic here
        
        return {"message": "Chat completion processed securely"}
    
    return app
```

### 2. Data Privacy and Compliance

```python
import hashlib
import hmac
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import json
import asyncio

class DataPrivacyManager:
    def __init__(self, encryption_key: str):
        self.encryption_key = encryption_key.encode()
    
    def anonymize_user_data(self, user_id: str) -> str:
        """Create anonymous user identifier"""
        return hmac.new(
            self.encryption_key,
            user_id.encode(),
            hashlib.sha256
        ).hexdigest()[:16]
    
    def sanitize_conversation(self, messages: List[dict]) -> List[dict]:
        """Remove PII from conversation data"""
        sanitized = []
        
        pii_patterns = [
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
            r'\b\d{4}\s?\d{4}\s?\d{4}\s?\d{4}\b',  # Credit card
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
            r'\b\d{3}-\d{3}-\d{4}\b',  # Phone number
        ]
        
        for message in messages:
            content = message.get("content", "")
            
            # Replace PII patterns with placeholders
            for pattern in pii_patterns:
                content = re.sub(pattern, "[REDACTED]", content)
            
            sanitized.append({
                **message,
                "content": content
            })
        
        return sanitized
    
    def log_data_access(self, user_id: str, data_type: str, purpose: str):
        """Log data access for compliance"""
        access_log = {
            "timestamp": datetime.utcnow().isoformat(),
            "user_id": self.anonymize_user_data(user_id),
            "data_type": data_type,
            "purpose": purpose,
            "access_granted": True
        }
        
        # Store in compliance log (implement your storage mechanism)
        self._store_compliance_log(access_log)
    
    def handle_data_deletion_request(self, user_id: str) -> bool:
        """Handle GDPR/CCPA deletion requests"""
        try:
            # Delete user conversations
            # Delete user preferences
            # Delete user analytics data
            # Update logs to reflect deletion
            
            deletion_log = {
                "timestamp": datetime.utcnow().isoformat(),
                "user_id": self.anonymize_user_data(user_id),
                "action": "data_deletion",
                "status": "completed"
            }
            
            self._store_compliance_log(deletion_log)
            return True
            
        except Exception as e:
            logging.error("Data deletion failed", extra={"error": str(e)})
            return False
    
    def _store_compliance_log(self, log_entry: dict):
        """Store compliance log entry"""
        # Implement your preferred storage mechanism
        # Could be database, file system, or external compliance service
        pass

# Content filtering for safety
class ContentFilter:
    def __init__(self):
        self.harmful_patterns = [
            r'\b(kill|murder|suicide)\b',
            r'\b(bomb|explosive|weapon)\b',
            r'\b(hack|exploit|vulnerability)\b',
            # Add more patterns based on your safety requirements
        ]
    
    async def filter_content(self, content: str) -> tuple[bool, List[str]]:
        """Filter content for harmful patterns"""
        violations = []
        
        for pattern in self.harmful_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                violations.append(pattern)
        
        is_safe = len(violations) == 0
        return is_safe, violations
    
    async def filter_request(self, request: ChatRequest) -> ChatRequest:
        """Filter incoming request"""
        filtered_messages = []
        
        for message in request.messages:
            content = message.get("content", "")
            is_safe, violations = await self.filter_content(content)
            
            if not is_safe:
                # Log the violation
                logging.warning(
                    "Content violation detected",
                    extra={
                        "violations": violations,
                        "content_preview": content[:100]
                    }
                )
                
                # Replace with safe content or reject
                message["content"] = "[Content filtered for safety]"
            
            filtered_messages.append(message)
        
        return ChatRequest(
            **{**request.dict(), "messages": filtered_messages}
        )
```

## Scaling Strategies and Performance Optimization

### 1. Caching Strategies

```python
import redis
import json
import hashlib
from typing import Optional, Any
import asyncio

class LLMCache:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
        self.default_ttl = 3600  # 1 hour
    
    def _generate_cache_key(self, messages: List[dict], model: str, **kwargs) -> str:
        """Generate deterministic cache key"""
        # Create a deterministic representation
        cache_data = {
            "messages": messages,
            "model": model,
            **{k: v for k, v in kwargs.items() if k in ["temperature", "max_tokens"]}
        }
        
        # Sort for deterministic ordering
        cache_string = json.dumps(cache_data, sort_keys=True)
        
        # Hash for compact key
        return "llm_cache:" + hashlib.md5(cache_string.encode()).hexdigest()
    
    async def get(self, messages: List[dict], model: str, **kwargs) -> Optional[dict]:
        """Get cached response"""
        cache_key = self._generate_cache_key(messages, model, **kwargs)
        
        try:
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
        except Exception as e:
            logging.warning("Cache retrieval failed", extra={"error": str(e)})
        
        return None
    
    async def set(
        self, 
        messages: List[dict], 
        model: str, 
        response: dict, 
        ttl: Optional[int] = None,
        **kwargs
    ):
        """Cache response"""
        cache_key = self._generate_cache_key(messages, model, **kwargs)
        ttl = ttl or self.default_ttl
        
        try:
            self.redis_client.setex(
                cache_key,
                ttl,
                json.dumps(response)
            )
        except Exception as e:
            logging.warning("Cache storage failed", extra={"error": str(e)})
    
    async def invalidate_pattern(self, pattern: str):
        """Invalidate cache entries matching pattern"""
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                self.redis_client.delete(*keys)
        except Exception as e:
            logging.warning("Cache invalidation failed", extra={"error": str(e)})

class CachedLLMClient:
    def __init__(self, llm_client, cache: LLMCache):
        self.llm_client = llm_client
        self.cache = cache
    
    async def complete(self, messages: List[dict], **kwargs) -> dict:
        """Complete with caching"""
        
        # Check cache first
        cached_response = await self.cache.get(messages, self.llm_client.model, **kwargs)
        if cached_response:
            logging.info("Cache hit", extra={"cache_key": "hit"})
            return cached_response
        
        # Call LLM API
        response = await self.llm_client.complete(messages, **kwargs)
        
        # Cache the response
        await self.cache.set(messages, self.llm_client.model, response, **kwargs)
        
        return response

# Connection pooling and load balancing
class LLMLoadBalancer:
    def __init__(self, providers: List[dict]):
        """
        providers: [
            {"name": "openai", "client": openai_client, "weight": 0.7},
            {"name": "anthropic", "client": anthropic_client, "weight": 0.3}
        ]
        """
        self.providers = providers
        self.current_loads = {p["name"]: 0 for p in providers}
    
    async def select_provider(self, request_type: str = "chat") -> dict:
        """Select provider based on load and weights"""
        
        # Calculate weighted scores based on current load
        best_provider = None
        best_score = float('in')
        
        for provider in self.providers:
            current_load = self.current_loads[provider["name"]]
            weight = provider["weight"]
            
            # Score = load / weight (lower is better)
            score = current_load / weight
            
            if score < best_score:
                best_score = score
                best_provider = provider
        
        # Update load tracking
        if best_provider:
            self.current_loads[best_provider["name"]] += 1
        
        return best_provider
    
    async def complete_with_load_balancing(self, messages: List[dict], **kwargs) -> dict:
        """Complete request with load balancing"""
        
        provider = await self.select_provider()
        
        try:
            response = await provider["client"].complete(messages, **kwargs)
            return response
        except Exception as e:
            logging.error(
                "Provider failed, attempting fallback",
                extra={"provider": provider["name"], "error": str(e)}
            )
            
            # Try other providers as fallback
            for fallback_provider in self.providers:
                if fallback_provider["name"] != provider["name"]:
                    try:
                        return await fallback_provider["client"].complete(messages, **kwargs)
                    except Exception as fe:
                        logging.error(
                            "Fallback provider failed",
                            extra={"provider": fallback_provider["name"], "error": str(fe)}
                        )
            
            # If all providers fail, raise the original exception
            raise e
        
        finally:
            # Decrease load counter
            self.current_loads[provider["name"]] -= 1

# Async request batching
class RequestBatcher:
    def __init__(self, batch_size: int = 10, max_wait_time: float = 0.1):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
        self.batch_timer = None
    
    async def add_request(self, request: dict, response_future: asyncio.Future):
        """Add request to batch"""
        self.pending_requests.append({
            "request": request,
            "future": response_future
        })
        
        # Start timer if this is the first request
        if len(self.pending_requests) == 1:
            self.batch_timer = asyncio.create_task(
                self._wait_and_process_batch()
            )
        
        # Process immediately if batch is full
        if len(self.pending_requests) >= self.batch_size:
            if self.batch_timer:
                self.batch_timer.cancel()
            await self._process_batch()
    
    async def _wait_and_process_batch(self):
        """Wait for max_wait_time then process batch"""
        try:
            await asyncio.sleep(self.max_wait_time)
            await self._process_batch()
        except asyncio.CancelledError:
            pass
    
    async def _process_batch(self):
        """Process current batch of requests"""
        if not self.pending_requests:
            return
        
        batch = self.pending_requests.copy()
        self.pending_requests.clear()
        
        # Process batch requests
        try:
            # Implement batch processing logic here
            # This could involve parallel API calls or optimized batch API endpoints
            
            responses = await self._execute_batch([req["request"] for req in batch])
            
            # Resolve futures with responses
            for i, batch_item in enumerate(batch):
                batch_item["future"].set_result(responses[i])
                
        except Exception as e:
            # Reject all futures with the error
            for batch_item in batch:
                batch_item["future"].set_exception(e)
    
    async def _execute_batch(self, requests: List[dict]) -> List[dict]:
        """Execute batch of requests"""
        # Implement parallel execution
        tasks = []
        for request in requests:
            task = asyncio.create_task(self._execute_single_request(request))
            tasks.append(task)
        
        return await asyncio.gather(*tasks)
    
    async def _execute_single_request(self, request: dict) -> dict:
        """Execute single request (implement your LLM client call here)"""
        # This is where you'.format(
            "request": request,
            "future": response_future
        )d call your actual LLM client
        pass
```

## Key Takeaways for Part 3

1. **Infrastructure Patterns**: Use microservices architecture with proper service separation
2. **Monitoring is Essential**: Implement comprehensive monitoring with metrics, logging, and alerting
3. **Security First**: Implement authentication, authorization, rate limiting, and content filtering
4. **Performance Optimization**: Use caching, load balancing, and request batching for scale
5. **Compliance Matters**: Handle data privacy, PII protection, and regulatory requirements

## Series Conclusion

Congratulations! You've completed the **LLM Engineering Mastery** series. You now have the practical knowledge to:

- Select and integrate foundation models effectively
- Build advanced RAG systems with proper evaluation
- Deploy and scale LLM applications in production
- Monitor and maintain enterprise-grade systems
- Implement security and compliance best practices

The field of LLM engineering is rapidly evolving, but these foundational patterns and practices will serve you well as you build the next generation of AI-powered applications.

### Next Steps

1. **Practice**: Implement these patterns in your own projects
2. **Stay Updated**: Follow LLM research and new model releases
3. **Community**: Join LLM engineering communities and share your experiences
4. **Experiment**: Try new techniques and optimization strategies
5. **Scale Gradually**: Start small and scale based on real usage patterns

---

*This concludes the LLM Engineering Mastery series. Keep building amazing AI applications!*

