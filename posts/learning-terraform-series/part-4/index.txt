3:I[4707,[],""]
6:I[6423,[],""]
7:I[981,["8592","static/chunks/common-1942b2e5063f4af5.js","3185","static/chunks/app/layout-f803094fc502a10d.js"],"AuthProvider"]
8:I[8931,["8592","static/chunks/common-1942b2e5063f4af5.js","3185","static/chunks/app/layout-f803094fc502a10d.js"],"default"]
9:I[917,["7601","static/chunks/app/error-1745ca505ccb7f84.js"],"default"]
a:I[5618,["9160","static/chunks/app/not-found-5aff7e7753541a4f.js"],"default"]
4:["slug","learning-terraform-series","d"]
5:["part","part-4","d"]
0:["Ph-JG0SHzMEpsRc4oi7jl",[[["",{"children":["posts",{"children":[["slug","learning-terraform-series","d"],{"children":[["part","part-4","d"],{"children":["__PAGE__?{\"slug\":\"learning-terraform-series\",\"part\":\"part-4\"}",{}]}]}]}]},"$undefined","$undefined",true],["",{"children":["posts",{"children":[["slug","learning-terraform-series","d"],{"children":[["part","part-4","d"],{"children":["__PAGE__",{},[["$L1","$L2",null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/275ed64cc4367444.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/f2c5f2458408eb15.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"Abstract Algorithms\",\"description\":\"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices\",\"url\":\"https://abstractalgorithms.github.io\",\"potentialAction\":{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https://abstractalgorithms.github.io/posts/{search_term_string}\"},\"query-input\":\"required name=search_term_string\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Abstract Algorithms\",\"url\":\"https://abstractalgorithms.github.io\"}}"}}],["$","link",null,{"rel":"manifest","href":"/manifest.json"}],["$","meta",null,{"name":"theme-color","content":"#00D885"}],["$","meta",null,{"name":"google-site-verification","content":"D5v1M3nD8oO9DNaZKujCwBLNNqf35CTJo114uv8yMNU"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-VZR168MHE2"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n            window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-VZR168MHE2');\n          "}}]]}],["$","body",null,{"className":"__className_e8ce0c","children":["$","$L7",null,{"children":["$","$L8",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$9","errorStyles":[],"errorScripts":[],"template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","$La",null,{}],"notFoundStyles":[]}]}]}]}]]}]],null],null],["$Lb",null]]]]
c:I[5878,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"Image"]
d:I[2972,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],""]
e:I[1343,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"default"]
f:I[9798,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"default"]
10:I[6883,["8592","static/chunks/common-1942b2e5063f4af5.js","7337","static/chunks/app/posts/%5Bslug%5D/%5Bpart%5D/page-4666292345f72325.js"],"default"]
11:Tfb9,<h2>Why Use Hash Tables?</h2>
<p>Hash tables are ideal for scenarios where you need to:</p>
<ul>
<li>Quickly look up values by a unique key (e.g., username â†’ user profile)</li>
<li>Count occurrences of items (e.g., word frequency in a document)</li>
<li>Implement sets, caches, or associative arrays</li>
<li>Index data for fast retrieval (e.g., database indexes, symbol tables in compilers)</li>
</ul>
<p><strong>Example Applications:</strong></p>
<ul>
<li>Caching web pages or database queries</li>
<li>Implementing sets/maps in programming languages (e.g., Python's <code>dict</code>, JavaScript's <code>Object</code>/<code>Map</code>)</li>
<li>Counting unique visitors or items</li>
<li>Storing configuration or environment variables</li>
</ul>
<h2>Anatomy of a Hash Table</h2>
<ol>
<li><strong>Hash Function</strong>: Transforms keys into array indices. A robust function minimizes collisions and distributes keys uniformly.</li>
<li><strong>Buckets / Slots</strong>: Underlying array where values reside.</li>
<li><strong>Collision Resolution</strong>: Techniques like chaining or open addressing to handle index conflicts.</li>
</ol>
<h2>How Hash Functions Work</h2>
<p>A hash function takes an input (the key) and returns an integer (the hash code), which is then mapped to an index in the underlying array. Good hash functions:</p>
<ul>
<li>Are deterministic (same input always gives same output)</li>
<li>Distribute keys uniformly to minimize clustering</li>
<li>Are fast to compute</li>
</ul>
<h3>Example: Simple Modulo Hash</h3>
<pre><code class="language-javascript">function simpleHash(key, tableSize) {
  let hash = 0;
  for (let char of key) {
    hash = (hash * 31 + char.charCodeAt(0)) % tableSize;
  }
  return hash;
}
</code></pre>
<blockquote>
<p>The choice of multiplier (e.g., 31) affects distribution; primes often yield better spreads.</p>
</blockquote>
<h3>Real-World Hash Functions</h3>
<ul>
<li><strong>MurmurHash, CityHash, FNV-1a</strong>: Used in production systems for better distribution and speed.</li>
<li><strong>Cryptographic hashes (SHA-256, MD5)</strong>: Used for security, not for hash tables (too slow).</li>
</ul>
<h2>Handling Collisions</h2>
<p>When two keys hash to the same index, a collision occurs. There are two main strategies:</p>
<h3>Chaining</h3>
<p>Each bucket holds a list of entries. Collisions are handled by appending to the list.</p>
<pre><code class="language-javascript">class HashTableChain {
  constructor(size = 42) {
    this.buckets = Array.from({ length: size }, () => []);
  }

  insert(key, value) {
    const index = simpleHash(key, this.buckets.length);
    this.buckets[index].push([key, value]);
  }

  // ...existing code...
}
</code></pre>
<h3>Open Addressing (Linear Probing)</h3>
<p>All entries are stored in the array itself. On collision, the algorithm searches for the next available slot.</p>
<p>{/* Linear probing illustration would go here */}</p>
<pre><code class="language-javascript">class HashTableProbing {
  constructor(size = 42) {
    this.table = new Array(size).fill(null);
  }

  // ...existing code...
}
</code></pre>
<h2>Example Scenario: Username Lookup</h2>
<p>Suppose you want to check if a username is taken:</p>
<ol>
<li>Hash the username to get an index.</li>
<li>Check the bucket (or slot) at that index.</li>
<li>If found, the username is taken; otherwise, it's available.</li>
</ol>
<p>This operation is extremely fast, even with thousands or millions of users.</p>
<h2>Performance Analysis</h2>
<ul>
<li><strong>Average Case</strong>: With a good hash function and low load factor, operations are nearly instantaneous.</li>
<li><strong>Worst Case</strong>: If many keys collide (poor hash function or overloaded table), performance degrades to linear time.</li>
</ul>
<h2>Conclusion</h2>
<p>Well-implemented hash tables power applications that require rapid lookups, from caching layers to in-memory databases. Selecting the right collision strategy and hash function is key to maintaining high performance.</p>
12:T29e8,<h1>System Design Interview Mastery: Complete Guide</h1>
<p>Welcome to the comprehensive System Design Mastery series! This 6-part guide will take you from understanding the fundamentals to solving the most popular system design interview questions asked at top tech companies.</p>
<h2>What You'll Learn</h2>
<p>By the end of this series, you'll master:</p>
<ul>
<li><strong>Systematic Problem-Solving Approach</strong>: A proven methodology to tackle any system design question</li>
<li><strong>Top 5 Interview Questions</strong>: Detailed solutions to the most commonly asked questions</li>
<li><strong>Scalability Patterns</strong>: How to design systems that handle millions of users</li>
<li><strong>Trade-offs Analysis</strong>: Understanding when to choose specific technologies and architectures</li>
<li><strong>Interview Techniques</strong>: How to communicate your design decisions effectively</li>
</ul>
<h2>Series Overview</h2>
<h3>Part 1: Introduction &#x26; Methodology (This Part)</h3>
<p>Learn the systematic approach to system design interviews and core concepts.</p>
<h3>Part 2: Design a URL Shortener (TinyURL)</h3>
<p>Master the fundamentals with this classic system design problem.</p>
<h3>Part 3: Design a Chat System (WhatsApp)</h3>
<p>Learn real-time communication patterns and WebSocket architecture.</p>
<h3>Part 4: Design a Social Media Feed (Twitter)</h3>
<p>Understand content delivery, caching, and timeline generation.</p>
<h3>Part 5: Design a Video Streaming Service (YouTube)</h3>
<p>Explore CDNs, video processing, and large-scale storage.</p>
<h3>Part 6: Design a Distributed Cache (Redis)</h3>
<p>Deep dive into caching strategies and data consistency.</p>
<h2>The System Design Interview Process</h2>
<p>Understanding the interview format is crucial for success. Most system design interviews follow a predictable structure that allows candidates to demonstrate their architectural thinking and problem-solving skills.</p>
<h3>Key Interview Phases</h3>
<ol>
<li>
<p><strong>Requirements Clarification (5-10 minutes)</strong></p>
<ul>
<li>Define functional requirements</li>
<li>Identify non-functional requirements</li>
<li>Establish scale and constraints</li>
</ul>
</li>
<li>
<p><strong>High-Level Design (15-20 minutes)</strong></p>
<ul>
<li>Sketch the overall architecture</li>
<li>Identify major components</li>
<li>Define data flow</li>
</ul>
</li>
<li>
<p><strong>Detailed Design (15-20 minutes)</strong></p>
<ul>
<li>Deep dive into critical components</li>
<li>Database schema design</li>
<li>API design</li>
</ul>
</li>
<li>
<p><strong>Scale and Optimize (10-15 minutes)</strong></p>
<ul>
<li>Address bottlenecks</li>
<li>Discuss caching strategies</li>
<li>Handle edge cases</li>
</ul>
</li>
</ol>
<h2>The Universal Template</h2>
<p>Every system design problem can be approached using this template:</p>
<h3>1. Functional Requirements</h3>
<p><strong>Actors</strong>: Define who will use the system</p>
<ul>
<li>Reader</li>
<li>Writer</li>
<li>Admin</li>
</ul>
<p><strong>Use Cases</strong>: Define how actors interact with the system</p>
<ul>
<li>How the Reader will use the system</li>
<li>How the Writer will use the system</li>
<li>Administrative functions</li>
</ul>
<p><strong>Features</strong>: List specific functionality</p>
<ul>
<li>What features are needed by each actor</li>
<li>What is explicitly out of scope</li>
</ul>
<h3>2. Non-Functional Requirements</h3>
<p>Define NFR expectations for all actors:</p>
<ul>
<li><strong>Scalability</strong>: How many users? Growth expectations?</li>
<li><strong>Availability</strong>: Uptime requirements (99.9%, 99.99%?)</li>
<li><strong>Performance</strong>: Latency expectations for reads/writes</li>
<li><strong>Data Consistency</strong>: Strong vs eventual consistency needs</li>
</ul>
<h3>3. Estimations</h3>
<p><strong>User Metrics</strong>:</p>
<ul>
<li>Daily Active Users (DAU)</li>
<li>Monthly Active Users (MAU)</li>
</ul>
<p><strong>Throughput</strong>:</p>
<ul>
<li>Queries Per Second (QPS) for reads</li>
<li>Queries Per Second (QPS) for writes</li>
<li>Read/Write ratio</li>
</ul>
<p><strong>Storage Estimations</strong>:</p>
<ul>
<li>Data per user/action</li>
<li>Daily storage needs</li>
<li>Annual storage needs</li>
<li>5-10 year projections</li>
</ul>
<p><strong>Memory Estimations</strong>:</p>
<ul>
<li>Cache requirements</li>
<li>RAM needs per server</li>
<li>Disk storage requirements</li>
</ul>
<p><strong>Scale Reference</strong>:</p>
<table>
<thead>
<tr>
<th>Unit</th>
<th>Decimal</th>
<th>Storage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Million</td>
<td>10^6</td>
<td>Megabytes</td>
</tr>
<tr>
<td>Billion</td>
<td>10^9</td>
<td>Gigabytes</td>
</tr>
<tr>
<td>Trillion</td>
<td>10^12</td>
<td>Terabytes</td>
</tr>
<tr>
<td>Quadrillion</td>
<td>10^15</td>
<td>Petabytes</td>
</tr>
</tbody>
</table>
<h3>4. Design Goals</h3>
<p><strong>Performance Requirements</strong>:</p>
<ul>
<li>Latency targets</li>
<li>Throughput requirements</li>
<li>Consistency vs Availability trade-offs</li>
</ul>
<p><strong>Architecture Patterns</strong>:</p>
<ul>
<li>Pipe and Filter Pattern</li>
<li>Event Driven Architecture</li>
<li>Pub/Sub Messaging</li>
<li>Streaming Processing</li>
</ul>
<p><strong>Usage Patterns</strong>:</p>
<table>
<thead>
<tr>
<th>Workload Type</th>
<th>Example</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read Heavy</td>
<td>Social Media</td>
<td>High read traffic from users browsing content</td>
</tr>
<tr>
<td>Write Heavy</td>
<td>Logging, Transactions</td>
<td>Frequent write operations for data capture</td>
</tr>
<tr>
<td>Balanced</td>
<td>E-Commerce</td>
<td>Mix of reads (browsing) and writes (orders)</td>
</tr>
<tr>
<td>Batch Processing</td>
<td>Analytics</td>
<td>Large data volumes processed in scheduled batches</td>
</tr>
<tr>
<td>Real-time</td>
<td>Trading, Monitoring</td>
<td>Immediate response to events required</td>
</tr>
</tbody>
</table>
<p><strong>Data Access Patterns</strong>:</p>
<table>
<thead>
<tr>
<th>Access Type</th>
<th>Use Case</th>
<th>Additional Information</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sequential Access</td>
<td>File Processing</td>
<td>Read/write data in order</td>
</tr>
<tr>
<td>Random Access</td>
<td>Database Lookup</td>
<td>Access specific data by key/index</td>
</tr>
<tr>
<td>Write Once, Read Many</td>
<td>Archival, Config</td>
<td>Data written once, read frequently</td>
</tr>
<tr>
<td>Pattern Matching</td>
<td>Log Analysis</td>
<td>Extract patterns using regex or similar</td>
</tr>
<tr>
<td>Range Queries</td>
<td>Time-series Data</td>
<td>Query data within specific ranges</td>
</tr>
</tbody>
</table>
<h2>Core Concepts to Master</h2>
<h3>Scalability Fundamentals</h3>
<p><strong>Horizontal Scaling (Scale Out)</strong>:</p>
<ul>
<li>Add more servers to handle increased load</li>
<li>Better fault tolerance and cost-effectiveness</li>
<li>Examples: Web servers, microservices</li>
</ul>
<p><strong>Vertical Scaling (Scale Up)</strong>:</p>
<ul>
<li>Increase power of existing machines</li>
<li>Simpler to implement but has physical limits</li>
<li>Examples: Database upgrades, CPU/RAM increases</li>
</ul>
<h3>Database Strategies</h3>
<p><strong>SQL vs NoSQL</strong>:</p>
<ul>
<li><strong>SQL</strong>: ACID properties, complex queries, structured data</li>
<li><strong>NoSQL</strong>: Horizontal scaling, flexible schema, specific use cases</li>
</ul>
<p><strong>Database Patterns</strong>:</p>
<ul>
<li><strong>Master-Slave Replication</strong>: Read scaling</li>
<li><strong>Master-Master Replication</strong>: Write scaling with conflicts</li>
<li><strong>Database Sharding</strong>: Horizontal partitioning</li>
<li><strong>Federation</strong>: Split databases by function</li>
</ul>
<h3>Caching Strategies</h3>
<p><strong>Cache Patterns</strong>:</p>
<ul>
<li><strong>Cache-Aside</strong>: Application manages cache</li>
<li><strong>Write-Through</strong>: Write to cache and database simultaneously</li>
<li><strong>Write-Behind</strong>: Write to cache first, database later</li>
<li><strong>Refresh-Ahead</strong>: Proactively refresh cache before expiration</li>
</ul>
<p><strong>Cache Levels</strong>:</p>
<ul>
<li>Browser cache</li>
<li>CDN (Content Delivery Network)</li>
<li>Load balancer cache</li>
<li>Application cache</li>
<li>Database cache</li>
</ul>
<h3>Communication Patterns</h3>
<p><strong>Synchronous Communication</strong>:</p>
<ul>
<li>HTTP/HTTPS requests</li>
<li>RPC (Remote Procedure Calls)</li>
<li>GraphQL</li>
</ul>
<p><strong>Asynchronous Communication</strong>:</p>
<ul>
<li>Message queues (RabbitMQ, Apache Kafka)</li>
<li>Pub/Sub systems</li>
<li>Event streaming</li>
</ul>
<h2>Common Design Patterns</h2>
<h3>Microservices Architecture</h3>
<ul>
<li>Service decomposition</li>
<li>API Gateway pattern</li>
<li>Service discovery</li>
<li>Circuit breaker pattern</li>
</ul>
<h3>Event-Driven Architecture</h3>
<ul>
<li>Event sourcing</li>
<li>CQRS (Command Query Responsibility Segregation)</li>
<li>Saga pattern for distributed transactions</li>
</ul>
<h3>Data Management Patterns</h3>
<ul>
<li>Database per service</li>
<li>Shared database anti-pattern</li>
<li>Event-driven data synchronization</li>
</ul>
<h2>Preparation Tips</h2>
<h3>Study Strategy</h3>
<ol>
<li><strong>Understand fundamentals</strong>: Master basic concepts before diving into complex problems</li>
<li><strong>Practice systematically</strong>: Use the template for every problem</li>
<li><strong>Learn from real systems</strong>: Study how actual systems like Google, Facebook, and Amazon work</li>
<li><strong>Think about trade-offs</strong>: Every design decision has pros and cons</li>
<li><strong>Practice communication</strong>: Explain your thinking process clearly</li>
</ol>
<h3>Common Mistakes to Avoid</h3>
<ul>
<li>Jumping to solution without understanding requirements</li>
<li>Over-engineering the initial design</li>
<li>Ignoring non-functional requirements</li>
<li>Not considering scalability from the start</li>
<li>Poor time management during the interview</li>
</ul>
<h2>System Design Fundamentals Quiz</h2>
<p>Before diving into specific use cases, test your understanding of the core system design concepts. The interactive quiz will appear at the end of this series introduction.</p>
<h2>What's Next?</h2>
<p>In the next part, we'll apply this methodology to design a URL shortener service like TinyURL. This classic problem will help you practice the systematic approach and understand how to break down complex requirements into manageable components.</p>
<p>Each subsequent part will tackle increasingly complex problems, building your confidence and expertise in system design interviews.</p>
<p><strong>Ready to start?</strong> Let's dive into Part 2 and design our first system!</p>
13:T3117,<h1>Design a URL Shortener (TinyURL)</h1>
<p>In this part, we'll apply our systematic methodology to design a URL shortener service like TinyURL or bit.ly. This is one of the most popular system design interview questions because it covers fundamental concepts while being simple enough to design in 45 minutes.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>URL Creator</strong>: Users who want to shorten long URLs</li>
<li><strong>URL Consumer</strong>: Users who click on shortened URLs</li>
<li><strong>System Administrator</strong>: Manages the service</li>
</ul>
<h3>Use Cases</h3>
<p><strong>URL Creator</strong>:</p>
<ul>
<li>Create shortened URLs from long URLs</li>
<li>Set custom aliases (optional)</li>
<li>Set expiration dates for URLs</li>
<li>View analytics (click count, geographic data)</li>
</ul>
<p><strong>URL Consumer</strong>:</p>
<ul>
<li>Access original URLs via shortened links</li>
<li>Experience fast redirection (&#x3C;100ms)</li>
</ul>
<p><strong>System Administrator</strong>:</p>
<ul>
<li>Monitor system health and performance</li>
<li>Manage expired URLs and cleanup</li>
<li>Handle abuse and spam detection</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>Shorten long URLs to ~7 character format</li>
<li>Redirect shortened URLs to original URLs</li>
<li>Custom aliases for URLs</li>
<li>URL expiration functionality</li>
<li>Basic analytics (click count)</li>
<li>High availability for redirections</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>User authentication/accounts</li>
<li>Advanced analytics dashboard</li>
<li>Real-time collaboration features</li>
<li>API rate limiting (assume handled by infrastructure)</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support 100 million URLs shortened per month</li>
<li>Handle 10 billion redirections per month</li>
<li>Scale to serve global users</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.9% uptime for URL creation</li>
<li>99.99% uptime for URL redirection</li>
<li>Graceful degradation during failures</li>
</ul>
<h3>Performance</h3>
<ul>
<li>URL creation: &#x3C;200ms response time</li>
<li>URL redirection: &#x3C;100ms response time</li>
<li>Handle traffic spikes during viral content</li>
</ul>
<h3>Data Consistency</h3>
<ul>
<li>Strong consistency for URL creation</li>
<li>Eventual consistency acceptable for analytics</li>
<li>No duplicate shortened URLs</li>
</ul>
<h2>3. Estimations</h2>
<h3>User Metrics</h3>
<ul>
<li><strong>Daily Active Users</strong>: 10 million</li>
<li><strong>URLs created per day</strong>: 3.3 million (100M/month)</li>
<li><strong>Redirections per day</strong>: 333 million (10B/month)</li>
</ul>
<h3>Throughput</h3>
<ul>
<li><strong>URL Creation QPS</strong>: 38 queries/second (3.3M/24/3600)</li>
<li><strong>URL Redirection QPS</strong>: 3,858 queries/second (333M/24/3600)</li>
<li><strong>Peak QPS</strong>: 5x average = 19,290 QPS</li>
<li><strong>Read/Write Ratio</strong>: 100:1 (heavy read workload)</li>
</ul>
<h3>Storage Estimations</h3>
<p><strong>Per URL Storage</strong>:</p>
<ul>
<li>Shortened URL: 7 bytes</li>
<li>Original URL: 500 bytes (average)</li>
<li>Metadata (creation date, expiration, etc.): 100 bytes</li>
<li><strong>Total per URL</strong>: ~600 bytes</li>
</ul>
<p><strong>Storage Growth</strong>:</p>
<ul>
<li><strong>Per Day</strong>: 3.3M × 600 bytes = 2 GB/day</li>
<li><strong>Per Year</strong>: 2 GB × 365 = 730 GB/year</li>
<li><strong>Per 5 Years</strong>: 730 GB × 5 = 3.65 TB</li>
</ul>
<h3>Memory Estimations</h3>
<p><strong>Cache Requirements</strong> (80/20 rule):</p>
<ul>
<li>20% of URLs generate 80% of traffic</li>
<li>Daily hot URLs: 333M × 20% = 66.6M URLs</li>
<li>Cache size: 66.6M × 600 bytes = ~40 GB</li>
<li>With replication: 40 GB × 3 = 120 GB total cache</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Latency</strong>: &#x3C;100ms for redirections, &#x3C;200ms for creation</li>
<li><strong>Throughput</strong>: 20K QPS peak capacity</li>
<li><strong>Consistency</strong>: Strong for writes, eventual for analytics</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Read-Heavy Workload</strong>: Implement aggressive caching</li>
<li><strong>Event-Driven</strong>: Use async processing for analytics</li>
<li><strong>Stateless Services</strong>: Enable horizontal scaling</li>
</ul>
<h3>Data Access Patterns</h3>
<ul>
<li><strong>Random Access</strong>: Database lookups by shortened URL key</li>
<li><strong>Write Once, Read Many</strong>: URLs rarely modified after creation</li>
<li><strong>Cache-Friendly</strong>: High cache hit ratios expected</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Client] → [Load Balancer] → [Web Servers] → [Cache] → [Database]
                                    ↓
                            [Analytics Service] → [Analytics DB]
                                    ↓
                              [Message Queue]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>Load Balancer</strong>: Distributes traffic across web servers</li>
<li><strong>Web Servers</strong>: Handle URL creation and redirection logic</li>
<li><strong>Cache Layer</strong>: Redis cluster for hot URL lookups</li>
<li><strong>Database</strong>: Primary storage for URL mappings</li>
<li><strong>Analytics Service</strong>: Processes click events asynchronously</li>
<li><strong>Message Queue</strong>: Decouples analytics from main flow</li>
</ol>
<h3>API Design</h3>
<p><strong>Create Short URL</strong>:</p>
<pre><code class="language-http">POST /api/v1/shorten
Content-Type: application/json

{
  "long_url": "https://example.com/very/long/path",
  "custom_alias": "mylink", // optional
  "expiration_date": "2024-12-31" // optional
}

Response:
{
  "short_url": "https://short.ly/abc123",
  "long_url": "https://example.com/very/long/path",
  "created_at": "2024-06-17T10:00:00Z",
  "expires_at": "2024-12-31T23:59:59Z"
}
</code></pre>
<p><strong>Redirect URL</strong>:</p>
<pre><code class="language-http">GET /{short_code}

Response: 301 Redirect
Location: https://example.com/very/long/path
</code></pre>
<p><strong>Get Analytics</strong>:</p>
<pre><code class="language-http">GET /api/v1/analytics/{short_code}

Response:
{
  "short_code": "abc123",
  "click_count": 1542,
  "created_at": "2024-06-17T10:00:00Z",
  "last_accessed": "2024-06-17T15:30:00Z"
}
</code></pre>
<h3>Data Schema</h3>
<p><strong>URLs Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE urls (
    short_code VARCHAR(7) PRIMARY KEY,
    long_url TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    click_count BIGINT DEFAULT 0,
    created_by_ip VARCHAR(45)
);

CREATE INDEX idx_expires_at ON urls(expires_at);
</code></pre>
<p><strong>Analytics Events Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE click_events (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    short_code VARCHAR(7) NOT NULL,
    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    user_ip VARCHAR(45),
    user_agent TEXT,
    referer TEXT
);

CREATE INDEX idx_short_code_time ON click_events(short_code, clicked_at);
</code></pre>
<h2>URL Encoding Algorithm</h2>
<h3>Base62 Encoding</h3>
<p>We'll use Base62 encoding (a-z, A-Z, 0-9) to generate short codes:</p>
<pre><code class="language-python">def base62_encode(num):
    base = 62
    alphabet = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    encoded = ""
    
    while num > 0:
        encoded = alphabet[num % base] + encoded
        num //= base
    
    return encoded or alphabet[0]

def base62_decode(encoded):
    base = 62
    alphabet = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    num = 0
    
    for char in encoded:
        num = num * base + alphabet.index(char)
    
    return num
</code></pre>
<h3>Counter-Based Approach</h3>
<ol>
<li>Use auto-incrementing database counter</li>
<li>Encode counter value to Base62</li>
<li>With 7 characters: 62^7 = 3.5 trillion possible URLs</li>
</ol>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No collisions</li>
<li>Predictable, sequential generation</li>
<li>Simple implementation</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Sequential patterns might be guessable</li>
<li>Single point of failure for counter</li>
</ul>
<h3>Alternative: Hash-Based Approach</h3>
<pre><code class="language-python">import hashlib

def generate_short_code(long_url, timestamp):
    data = f"{long_url}{timestamp}"
    hash_value = hashlib.md5(data.encode()).hexdigest()
    
    # Take first 7 characters and convert to Base62
    return hash_value[:7]
</code></pre>
<h2>Detailed Design Deep Dive</h2>
<h3>Caching Strategy</h3>
<p><strong>Multi-Layer Caching</strong>:</p>
<ol>
<li><strong>Browser Cache</strong>: Cache 301 redirects for 1 hour</li>
<li><strong>CDN Cache</strong>: Cache popular URLs at edge locations</li>
<li><strong>Application Cache</strong>: Redis cluster with:
<ul>
<li>TTL: 24 hours for hot URLs</li>
<li>LRU eviction policy</li>
<li>99% hit ratio target</li>
</ul>
</li>
</ol>
<p><strong>Cache Key Strategy</strong>:</p>
<pre><code>Key: "url:{short_code}"
Value: {
  "long_url": "https://example.com/path",
  "expires_at": "2024-12-31T23:59:59Z"
}
</code></pre>
<h3>Database Sharding</h3>
<p><strong>Shard by Short Code</strong>:</p>
<ul>
<li>Consistent hashing on short_code</li>
<li>4 shards initially, plan for 16 shards</li>
<li>Each shard handles ~25% of traffic</li>
</ul>
<p><strong>Shard Key</strong>: First 2 characters of short_code</p>
<ul>
<li>Shard 1: aa-pz</li>
<li>Shard 2: qa-9z</li>
<li>Shard 3: Aa-Pz</li>
<li>Shard 4: Qa-9z</li>
</ul>
<h3>Analytics Processing</h3>
<p><strong>Async Event Processing</strong>:</p>
<ol>
<li>URL click triggers event</li>
<li>Event published to message queue</li>
<li>Analytics service processes events in batches</li>
<li>Updates click counts every 5 minutes</li>
</ol>
<p><strong>Analytics Pipeline</strong>:</p>
<pre><code>[Click Event] → [Kafka Queue] → [Analytics Worker] → [Analytics DB]
                                        ↓
                                [Real-time Dashboard]
</code></pre>
<h2>Scaling Considerations</h2>
<h3>Handling Traffic Spikes</h3>
<p><strong>Auto-Scaling Strategy</strong>:</p>
<ul>
<li>Monitor QPS and response time</li>
<li>Scale web servers horizontally</li>
<li>Pre-warm cache for viral content</li>
<li>Circuit breakers for graceful degradation</li>
</ul>
<h3>Geographic Distribution</h3>
<p><strong>Multi-Region Deployment</strong>:</p>
<ul>
<li>Primary region: US-East (main database)</li>
<li>Secondary regions: EU-West, Asia-Pacific</li>
<li>Read replicas in each region</li>
<li>Global load balancer routes to nearest region</li>
</ul>
<h3>Performance Optimizations</h3>
<ol>
<li><strong>Connection Pooling</strong>: Reuse database connections</li>
<li><strong>Async Processing</strong>: Non-blocking I/O for analytics</li>
<li><strong>Batch Operations</strong>: Group database writes</li>
<li><strong>CDN Integration</strong>: Cache static assets and popular URLs</li>
</ol>
<h2>Security Considerations</h2>
<h3>Spam Prevention</h3>
<ul>
<li>Rate limiting per IP address</li>
<li>URL validation and sanitization</li>
<li>Malicious URL detection</li>
<li>CAPTCHA for suspicious traffic</li>
</ul>
<h3>Data Protection</h3>
<ul>
<li>HTTPS enforcement</li>
<li>SQL injection prevention</li>
<li>Input validation for custom aliases</li>
<li>Access logs for audit trails</li>
</ul>
<h2>URL Shortener Design Quiz</h2>
<p>Test your understanding of URL shortener system design with the interactive quiz that appears after each part of this series.</p>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Read-Heavy Optimization</strong>: Aggressive caching is crucial for URL shorteners</li>
<li><strong>Simple but Scalable</strong>: Start simple, add complexity as needed</li>
<li><strong>Analytics Separation</strong>: Decouple analytics from core functionality</li>
<li><strong>Global Distribution</strong>: CDNs and regional deployments improve performance</li>
<li><strong>Failure Planning</strong>: Design for graceful degradation during traffic spikes</li>
</ol>
<h2>What's Next?</h2>
<p>In Part 3, we'll design a real-time chat system like WhatsApp, which introduces new challenges around WebSocket connections, message delivery guarantees, and online presence management.</p>
14:T46f5,<h1>Design a Chat System (WhatsApp)</h1>
<p>In this part, we'll design a real-time chat system similar to WhatsApp or Slack. This problem introduces complex challenges around real-time communication, message delivery, and online presence management.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>Chat User</strong>: Sends and receives messages</li>
<li><strong>Group Admin</strong>: Manages group chats</li>
<li><strong>System</strong>: Handles presence and delivery</li>
</ul>
<h3>Use Cases</h3>
<p><strong>Chat User</strong>:</p>
<ul>
<li>Send one-on-one messages</li>
<li>Participate in group chats (up to 500 members)</li>
<li>See online/offline status of contacts</li>
<li>Receive messages in real-time</li>
<li>View message delivery status (sent, delivered, read)</li>
<li>Share media files (images, videos, documents)</li>
</ul>
<p><strong>Group Admin</strong>:</p>
<ul>
<li>Create and manage group chats</li>
<li>Add/remove participants</li>
<li>Set group permissions</li>
</ul>
<p><strong>System Functions</strong>:</p>
<ul>
<li>Deliver messages reliably</li>
<li>Maintain message ordering</li>
<li>Handle offline message delivery</li>
<li>Manage user presence status</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>One-on-one messaging</li>
<li>Group messaging (up to 500 members)</li>
<li>Real-time message delivery</li>
<li>Message delivery status</li>
<li>Online presence indicators</li>
<li>Media file sharing</li>
<li>Message history storage</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>Voice/video calling</li>
<li>Message encryption (assume handled by client)</li>
<li>Advanced group features (channels, threads)</li>
<li>Message search functionality</li>
<li>Push notifications (assume external service)</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support 1 billion users globally</li>
<li>Handle 50 billion messages per day</li>
<li>Support 10 million concurrent users</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.9% uptime for message delivery</li>
<li>Graceful degradation during failures</li>
<li>Message ordering must be preserved</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Message delivery: &#x3C;100ms in same region</li>
<li>Cross-region delivery: &#x3C;300ms</li>
<li>Group message fanout: &#x3C;500ms</li>
<li>Support real-time presence updates</li>
</ul>
<h3>Data Consistency</h3>
<ul>
<li>Strong consistency for message ordering</li>
<li>Eventual consistency for presence status</li>
<li>At-least-once message delivery guarantee</li>
</ul>
<h2>3. Estimations</h2>
<h3>User Metrics</h3>
<ul>
<li><strong>Total Users</strong>: 1 billion</li>
<li><strong>Daily Active Users</strong>: 500 million</li>
<li><strong>Concurrent Users</strong>: 10 million peak</li>
<li><strong>Average sessions per user</strong>: 4 per day</li>
</ul>
<h3>Message Volume</h3>
<ul>
<li><strong>Messages per day</strong>: 50 billion</li>
<li><strong>Messages per second</strong>: 578K average</li>
<li><strong>Peak QPS</strong>: 1.2 million</li>
<li><strong>Group messages</strong>: 20% of total volume</li>
</ul>
<h3>Storage Estimations</h3>
<p><strong>Per Message Storage</strong>:</p>
<ul>
<li>Message ID: 8 bytes</li>
<li>Sender ID: 8 bytes</li>
<li>Receiver/Group ID: 8 bytes</li>
<li>Message content: 100 bytes average</li>
<li>Metadata: 50 bytes</li>
<li><strong>Total per message</strong>: ~200 bytes</li>
</ul>
<p><strong>Storage Growth</strong>:</p>
<ul>
<li><strong>Per Day</strong>: 50B × 200 bytes = 10 TB/day</li>
<li><strong>Per Year</strong>: 10 TB × 365 = 3.65 PB/year</li>
<li><strong>Per 5 Years</strong>: 18.25 PB (with compression ~9 PB)</li>
</ul>
<h3>Connection Estimations</h3>
<ul>
<li><strong>WebSocket connections</strong>: 10 million concurrent</li>
<li><strong>Memory per connection</strong>: 10KB</li>
<li><strong>Total connection memory</strong>: 100 GB</li>
<li><strong>Servers needed</strong>: 200 servers (500MB per server)</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Latency</strong>: &#x3C;100ms same region, &#x3C;300ms cross-region</li>
<li><strong>Throughput</strong>: 1.2M messages/second peak</li>
<li><strong>Availability</strong>: 99.9% uptime</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Event-Driven</strong>: Message routing and delivery</li>
<li><strong>Pub/Sub</strong>: Real-time message distribution</li>
<li><strong>Microservices</strong>: Decomposed by functionality</li>
</ul>
<h3>Usage Patterns</h3>
<ul>
<li><strong>Real-time Processing</strong>: Immediate message delivery</li>
<li><strong>Write Heavy</strong>: High message ingestion rate</li>
<li><strong>Connection Heavy</strong>: Millions of persistent connections</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Mobile/Web Client] ↔ [WebSocket Gateway] → [Message Service] → [Message Queue]
                                ↓                    ↓              ↓
                        [Presence Service] → [User Service] → [Database Cluster]
                                ↓                    ↓              ↓
                        [Notification Service] → [Analytics] → [Message Storage]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>WebSocket Gateway</strong>: Manages persistent connections</li>
<li><strong>Message Service</strong>: Core message processing logic</li>
<li><strong>Presence Service</strong>: Tracks user online status</li>
<li><strong>User Service</strong>: User profiles and friend lists</li>
<li><strong>Message Queue</strong>: Reliable message delivery</li>
<li><strong>Database Cluster</strong>: Distributed message storage</li>
</ol>
<h3>API Design</h3>
<p><strong>WebSocket Events</strong>:</p>
<p><strong>Send Message</strong>:</p>
<pre><code class="language-json">{
  "type": "send_message",
  "data": {
    "message_id": "msg_123456",
    "chat_id": "chat_789",
    "content": "Hello World!",
    "message_type": "text",
    "timestamp": "2024-06-17T10:00:00Z"
  }
}
</code></pre>
<p><strong>Receive Message</strong>:</p>
<pre><code class="language-json">{
  "type": "new_message",
  "data": {
    "message_id": "msg_123456",
    "chat_id": "chat_789",
    "sender_id": "user_456",
    "content": "Hello World!",
    "timestamp": "2024-06-17T10:00:00Z",
    "delivery_status": "delivered"
  }
}
</code></pre>
<p><strong>Presence Update</strong>:</p>
<pre><code class="language-json">{
  "type": "presence_update",
  "data": {
    "user_id": "user_456",
    "status": "online",
    "last_seen": "2024-06-17T10:00:00Z"
  }
}
</code></pre>
<p><strong>REST APIs</strong>:</p>
<p><strong>Create Chat</strong>:</p>
<pre><code class="language-http">POST /api/v1/chats
{
  "type": "group",
  "name": "Project Team",
  "participants": ["user_123", "user_456", "user_789"]
}
</code></pre>
<p><strong>Get Chat History</strong>:</p>
<pre><code class="language-http">GET /api/v1/chats/{chat_id}/messages?limit=50&#x26;before=msg_123
</code></pre>
<h3>Database Schema</h3>
<p><strong>Users Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_seen TIMESTAMP,
    status ENUM('online', 'offline', 'away') DEFAULT 'offline'
);
</code></pre>
<p><strong>Chats Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE chats (
    chat_id BIGINT PRIMARY KEY,
    chat_type ENUM('direct', 'group') NOT NULL,
    name VARCHAR(255),
    created_by BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
</code></pre>
<p><strong>Messages Table</strong> (Partitioned by chat_id):</p>
<pre><code class="language-sql">CREATE TABLE messages (
    message_id BIGINT PRIMARY KEY,
    chat_id BIGINT NOT NULL,
    sender_id BIGINT NOT NULL,
    content TEXT NOT NULL,
    message_type ENUM('text', 'image', 'file') DEFAULT 'text',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_chat_time (chat_id, created_at),
    INDEX idx_sender (sender_id)
) PARTITION BY HASH(chat_id) PARTITIONS 100;
</code></pre>
<p><strong>Chat Participants Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE chat_participants (
    chat_id BIGINT,
    user_id BIGINT,
    joined_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    role ENUM('member', 'admin') DEFAULT 'member',
    
    PRIMARY KEY (chat_id, user_id),
    INDEX idx_user_chats (user_id)
);
</code></pre>
<h2>Detailed Design Deep Dive</h2>
<h3>WebSocket Connection Management</h3>
<p><strong>Connection Gateway</strong>:</p>
<pre><code class="language-python">class ConnectionGateway:
    def __init__(self):
        self.connections = {}  # user_id -> connection
        self.user_servers = {}  # user_id -> server_id
    
    def handle_connection(self, user_id, websocket):
        # Store connection mapping
        self.connections[user_id] = websocket
        self.user_servers[user_id] = self.server_id
        
        # Update presence service
        self.presence_service.set_online(user_id, self.server_id)
        
        # Subscribe to user's message queue
        self.message_queue.subscribe(f"user_{user_id}", self.deliver_message)
    
    def deliver_message(self, message):
        user_id = message['recipient_id']
        if user_id in self.connections:
            self.connections[user_id].send(message)
        else:
            # User offline, store for later delivery
            self.offline_storage.store(user_id, message)
</code></pre>
<p><strong>Load Balancing Connections</strong>:</p>
<ul>
<li>Consistent hashing by user_id</li>
<li>Session affinity for WebSocket connections</li>
<li>Health checks and failover</li>
</ul>
<h3>Message Processing Pipeline</h3>
<p><strong>Message Flow</strong>:</p>
<ol>
<li>Client sends message via WebSocket</li>
<li>Gateway validates and adds metadata</li>
<li>Message service processes and stores</li>
<li>Fanout service delivers to recipients</li>
<li>Delivery confirmation sent back</li>
</ol>
<p><strong>Message Service</strong>:</p>
<pre><code class="language-python">class MessageService:
    def process_message(self, message):
        # 1. Validate message
        if not self.validate_message(message):
            return {"error": "Invalid message"}
        
        # 2. Generate unique message ID
        message['message_id'] = self.generate_id()
        message['timestamp'] = datetime.utcnow()
        
        # 3. Store message
        self.store_message(message)
        
        # 4. Fanout to recipients
        recipients = self.get_chat_participants(message['chat_id'])
        for recipient_id in recipients:
            if recipient_id != message['sender_id']:
                self.message_queue.publish(f"user_{recipient_id}", message)
        
        # 5. Return acknowledgment
        return {"status": "sent", "message_id": message['message_id']}
</code></pre>
<h3>Group Message Fanout</h3>
<p><strong>Fanout Strategies</strong>:</p>
<p><strong>Pull Model</strong> (Recommended for large groups):</p>
<pre><code class="language-python">def fanout_pull_model(message, chat_id):
    # Store message once
    message_storage.store(message)
    
    # Notify online participants
    online_users = presence_service.get_online_users(chat_id)
    for user_id in online_users:
        notification_queue.publish(f"user_{user_id}", {
            "type": "new_message_notification",
            "chat_id": chat_id,
            "message_id": message['message_id']
        })
</code></pre>
<p><strong>Push Model</strong> (For small groups &#x3C;50 members):</p>
<pre><code class="language-python">def fanout_push_model(message, chat_id):
    participants = chat_service.get_participants(chat_id)
    
    for user_id in participants:
        if user_id != message['sender_id']:
            # Send full message to each participant
            message_queue.publish(f"user_{user_id}", message)
</code></pre>
<h3>Presence Service</h3>
<p><strong>Real-time Presence Updates</strong>:</p>
<pre><code class="language-python">class PresenceService:
    def __init__(self):
        self.redis_client = redis.Redis()
        self.heartbeat_interval = 30  # seconds
    
    def set_online(self, user_id, server_id):
        self.redis_client.hset("user_presence", user_id, json.dumps({
            "status": "online",
            "server_id": server_id,
            "last_seen": time.time()
        }))
        
        # Notify contacts about status change
        contacts = self.get_user_contacts(user_id)
        for contact_id in contacts:
            self.notify_presence_change(contact_id, user_id, "online")
    
    def heartbeat(self, user_id):
        # Update last seen timestamp
        presence = self.get_presence(user_id)
        if presence:
            presence['last_seen'] = time.time()
            self.redis_client.hset("user_presence", user_id, json.dumps(presence))
    
    def cleanup_offline_users(self):
        # Background job to mark users offline after timeout
        current_time = time.time()
        for user_id, presence_data in self.redis_client.hgetall("user_presence").items():
            presence = json.loads(presence_data)
            if current_time - presence['last_seen'] > 60:  # 1 minute timeout
                self.set_offline(user_id)
</code></pre>
<h3>Message Ordering and Delivery</h3>
<p><strong>Message Ordering</strong>:</p>
<ul>
<li>Use logical timestamps (Lamport clocks)</li>
<li>Sequence numbers per chat</li>
<li>Vector clocks for concurrent updates</li>
</ul>
<p><strong>Delivery Guarantees</strong>:</p>
<pre><code class="language-python">class MessageDelivery:
    def deliver_with_retry(self, user_id, message, max_retries=3):
        for attempt in range(max_retries):
            try:
                if self.is_user_online(user_id):
                    self.send_via_websocket(user_id, message)
                else:
                    self.store_for_offline_delivery(user_id, message)
                
                # Wait for acknowledgment
                if self.wait_for_ack(message['message_id'], timeout=5):
                    return True
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    # Final failure - store in dead letter queue
                    self.dead_letter_queue.store(user_id, message)
                    return False
                
                # Exponential backoff
                time.sleep(2 ** attempt)
        
        return False
</code></pre>
<h2>Scaling Considerations</h2>
<h3>Database Sharding</h3>
<p><strong>Shard by Chat ID</strong>:</p>
<pre><code class="language-python">def get_shard(chat_id):
    return chat_id % NUM_SHARDS

def route_message(message):
    shard = get_shard(message['chat_id'])
    return message_databases[shard]
</code></pre>
<p><strong>Hot Partition Problem</strong>:</p>
<ul>
<li>Very active group chats can overwhelm a single shard</li>
<li>Solution: Further partition by time ranges</li>
<li>Move viral chats to dedicated high-performance shards</li>
</ul>
<h3>Caching Strategy</h3>
<p><strong>Multi-Level Caching</strong>:</p>
<ol>
<li><strong>L1 Cache</strong>: Recent messages in application memory</li>
<li><strong>L2 Cache</strong>: Redis cluster for chat metadata</li>
<li><strong>L3 Cache</strong>: Chat participant lists</li>
</ol>
<pre><code class="language-python">class MessageCache:
    def get_recent_messages(self, chat_id, limit=50):
        # Try L1 cache first
        cache_key = f"recent_messages:{chat_id}"
        messages = self.memory_cache.get(cache_key)
        
        if not messages:
            # Try L2 cache (Redis)
            messages = self.redis_cache.get(cache_key)
            
            if not messages:
                # Fetch from database
                messages = self.database.get_messages(chat_id, limit)
                
                # Cache in both levels
                self.redis_cache.set(cache_key, messages, ttl=300)
            
            self.memory_cache.set(cache_key, messages, ttl=60)
        
        return messages
</code></pre>
<h3>Geographic Distribution</h3>
<p><strong>Multi-Region Architecture</strong>:</p>
<ul>
<li>WebSocket gateways in each region</li>
<li>Message routing based on user location</li>
<li>Cross-region message replication</li>
<li>Regional presence services with global sync</li>
</ul>
<h2>Chat System Design Quiz</h2>
<p>Test your understanding of real-time chat system design with the interactive quiz that appears after each part of this series.</p>
<h2>Security and Privacy</h2>
<h3>Message Security</h3>
<ul>
<li>End-to-end encryption (client-side)</li>
<li>Message integrity verification</li>
<li>Forward secrecy for key rotation</li>
</ul>
<h3>Privacy Protection</h3>
<ul>
<li>Message retention policies</li>
<li>User data anonymization</li>
<li>GDPR compliance for data deletion</li>
</ul>
<h3>Abuse Prevention</h3>
<ul>
<li>Rate limiting for spam prevention</li>
<li>Content moderation pipelines</li>
<li>User reporting mechanisms</li>
</ul>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Real-time Architecture</strong>: WebSockets enable bidirectional communication</li>
<li><strong>Message Ordering</strong>: Critical for user experience, requires careful design</li>
<li><strong>Presence Management</strong>: Efficient tracking reduces system overhead</li>
<li><strong>Fanout Strategies</strong>: Choose between push/pull based on group size</li>
<li><strong>Graceful Degradation</strong>: System should handle failures without data loss</li>
</ol>
<h2>What's Next?</h2>
<p>In Part 4, we'll design a social media feed system like Twitter, which introduces challenges around content ranking, timeline generation, and handling viral content.</p>
15:T5446,<h1>Design a Social Media Feed (Twitter)</h1>
<p>In this part, we'll design a social media feed system like Twitter. This problem introduces complex challenges around content ranking, timeline generation, viral content handling, and personalized content delivery.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>User</strong>: Posts and consumes content</li>
<li><strong>Content Creator</strong>: Influential users with many followers</li>
<li><strong>Content Moderator</strong>: Reviews flagged content</li>
<li><strong>System</strong>: Manages recommendations and trending</li>
</ul>
<h3>Use Cases</h3>
<p><strong>User</strong>:</p>
<ul>
<li>Post tweets (text, images, videos)</li>
<li>Follow/unfollow other users</li>
<li>View personalized timeline</li>
<li>Like, retweet, and comment on posts</li>
<li>Search for tweets and users</li>
<li>View trending topics</li>
</ul>
<p><strong>Content Creator</strong>:</p>
<ul>
<li>Publish content to large audiences</li>
<li>View analytics and engagement metrics</li>
<li>Promote content</li>
</ul>
<p><strong>Content Moderator</strong>:</p>
<ul>
<li>Review reported content</li>
<li>Take action on policy violations</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>Post tweets (280 characters, media support)</li>
<li>Follow/unfollow users</li>
<li>Home timeline (personalized feed)</li>
<li>User timeline (user's own tweets)</li>
<li>Like, retweet, reply functionality</li>
<li>Trending topics and hashtags</li>
<li>Search functionality</li>
<li>Basic analytics</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>Direct messaging (covered in Part 3)</li>
<li>Live streaming</li>
<li>Advanced recommendation algorithms</li>
<li>Advertisement system</li>
<li>Advanced analytics dashboard</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support 500 million users</li>
<li>Handle 300 million tweets per day</li>
<li>Support 100 million daily active users</li>
<li>Handle traffic spikes during viral events</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.9% uptime for timeline generation</li>
<li>99.99% uptime for tweet reading</li>
<li>Graceful degradation during peak traffic</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Timeline generation: &#x3C;200ms</li>
<li>Tweet posting: &#x3C;100ms</li>
<li>Search results: &#x3C;300ms</li>
<li>Handle 300K tweets/second during peak</li>
</ul>
<h3>Data Consistency</h3>
<ul>
<li>Eventual consistency for timeline updates</li>
<li>Strong consistency for user actions (follow/unfollow)</li>
<li>Tweet immutability after posting</li>
</ul>
<h2>3. Estimations</h2>
<h3>User Metrics</h3>
<ul>
<li><strong>Total Users</strong>: 500 million</li>
<li><strong>Daily Active Users</strong>: 100 million</li>
<li><strong>Average tweets per user per day</strong>: 3</li>
<li><strong>Average follows per user</strong>: 200</li>
<li><strong>Heavy users (celebrities)</strong>: 1% with 1M+ followers</li>
</ul>
<h3>Tweet Volume</h3>
<ul>
<li><strong>Tweets per day</strong>: 300 million</li>
<li><strong>Tweets per second</strong>: 3,472 average</li>
<li><strong>Peak TPS</strong>: 17,360 (5x average)</li>
<li><strong>Tweet fanout ratio</strong>: 1:200 (average followers)</li>
</ul>
<h3>Storage Estimations</h3>
<p><strong>Per Tweet Storage</strong>:</p>
<ul>
<li>Tweet ID: 8 bytes</li>
<li>User ID: 8 bytes</li>
<li>Content: 300 bytes (average with metadata)</li>
<li>Media URLs: 100 bytes</li>
<li>Timestamps: 16 bytes</li>
<li><strong>Total per tweet</strong>: ~450 bytes</li>
</ul>
<p><strong>Storage Growth</strong>:</p>
<ul>
<li><strong>Per Day</strong>: 300M × 450 bytes = 135 GB/day</li>
<li><strong>Per Year</strong>: 135 GB × 365 = 49 TB/year</li>
<li><strong>Per 5 Years</strong>: 245 TB</li>
</ul>
<p><strong>Timeline Cache Storage</strong>:</p>
<ul>
<li>Cache top 1000 tweets per user</li>
<li>100M users × 1000 tweets × 450 bytes = 45 TB cache</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Timeline Generation</strong>: &#x3C;200ms for cached timelines</li>
<li><strong>Tweet Publishing</strong>: &#x3C;100ms response time</li>
<li><strong>Search</strong>: &#x3C;300ms for result delivery</li>
<li><strong>Viral Content</strong>: Handle 100K retweets/minute</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Event-Driven</strong>: Tweet fanout and timeline updates</li>
<li><strong>CQRS</strong>: Separate read and write models</li>
<li><strong>Cache-Heavy</strong>: Aggressive caching for read performance</li>
</ul>
<h3>Usage Patterns</h3>
<ul>
<li><strong>Read Heavy</strong>: 300:1 read to write ratio</li>
<li><strong>Real-time</strong>: Immediate timeline updates</li>
<li><strong>Spike Traffic</strong>: Viral content creates traffic spikes</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Client] → [Load Balancer] → [API Gateway] → [Tweet Service]
                                    ↓           ↓
                            [Timeline Service] [User Service]
                                    ↓           ↓
                            [Fanout Service] → [Cache Layer]
                                    ↓           ↓
                            [Message Queue] → [Database Cluster]
                                    ↓           ↓
                            [Search Service] [Media Service]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>API Gateway</strong>: Routes requests and handles authentication</li>
<li><strong>Tweet Service</strong>: Handles tweet creation and retrieval</li>
<li><strong>Timeline Service</strong>: Generates and serves user timelines</li>
<li><strong>Fanout Service</strong>: Distributes tweets to followers</li>
<li><strong>User Service</strong>: Manages user profiles and relationships</li>
<li><strong>Search Service</strong>: Provides tweet and user search</li>
<li><strong>Cache Layer</strong>: Multi-tier caching for performance</li>
</ol>
<h3>API Design</h3>
<p><strong>Post Tweet</strong>:</p>
<pre><code class="language-http">POST /api/v1/tweets
Authorization: Bearer {token}
{
  "content": "Hello world! #myFirstTweet",
  "media_urls": ["https://cdn.example.com/image1.jpg"],
  "reply_to": null
}

Response:
{
  "tweet_id": "1234567890",
  "user_id": "user_123",
  "content": "Hello world! #myFirstTweet",
  "created_at": "2024-06-17T10:00:00Z",
  "engagement": {
    "likes": 0,
    "retweets": 0,
    "replies": 0
  }
}
</code></pre>
<p><strong>Get Timeline</strong>:</p>
<pre><code class="language-http">GET /api/v1/timeline?type=home&#x26;limit=20&#x26;cursor=tweet_123

Response:
{
  "tweets": [
    {
      "tweet_id": "1234567890",
      "user": {
        "user_id": "user_456",
        "username": "@johndoe",
        "display_name": "John Doe",
        "avatar_url": "https://cdn.example.com/avatar.jpg"
      },
      "content": "Great weather today!",
      "created_at": "2024-06-17T10:00:00Z",
      "engagement": {
        "likes": 42,
        "retweets": 15,
        "replies": 8
      },
      "media": []
    }
  ],
  "next_cursor": "tweet_456",
  "has_more": true
}
</code></pre>
<p><strong>Follow User</strong>:</p>
<pre><code class="language-http">POST /api/v1/users/{user_id}/follow

Response:
{
  "following": true,
  "follower_count": 1543,
  "following_count": 287
}
</code></pre>
<h3>Database Schema</h3>
<p><strong>Users Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    display_name VARCHAR(100),
    bio TEXT,
    avatar_url VARCHAR(500),
    verified BOOLEAN DEFAULT FALSE,
    follower_count INT DEFAULT 0,
    following_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
</code></pre>
<p><strong>Tweets Table</strong> (Partitioned by created_at):</p>
<pre><code class="language-sql">CREATE TABLE tweets (
    tweet_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    content TEXT NOT NULL,
    reply_to BIGINT,
    retweet_of BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    like_count INT DEFAULT 0,
    retweet_count INT DEFAULT 0,
    reply_count INT DEFAULT 0,
    
    INDEX idx_user_time (user_id, created_at),
    INDEX idx_reply_to (reply_to),
    FOREIGN KEY (user_id) REFERENCES users(user_id)
) PARTITION BY RANGE (UNIX_TIMESTAMP(created_at));
</code></pre>
<p><strong>Follows Table</strong> (Sharded by follower_id):</p>
<pre><code class="language-sql">CREATE TABLE follows (
    follower_id BIGINT,
    following_id BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (follower_id, following_id),
    INDEX idx_following (following_id, follower_id)
);
</code></pre>
<p><strong>Timeline Cache Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE user_timelines (
    user_id BIGINT,
    tweet_id BIGINT,
    score DECIMAL(10,2), -- for ranking
    created_at TIMESTAMP,
    
    PRIMARY KEY (user_id, score, tweet_id),
    INDEX idx_user_time (user_id, created_at)
);
</code></pre>
<h2>Timeline Generation Strategies</h2>
<h3>Push Model (Write-Heavy)</h3>
<p><strong>Tweet Fanout on Write</strong>:</p>
<pre><code class="language-python">class PushTimelineService:
    def fanout_tweet(self, tweet, user_id):
        # Get all followers
        followers = self.user_service.get_followers(user_id)
        
        # Add tweet to each follower's timeline
        for follower_id in followers:
            self.timeline_cache.add_to_timeline(follower_id, tweet)
            
            # Limit timeline size (keep only latest 1000 tweets)
            self.timeline_cache.trim_timeline(follower_id, max_size=1000)
    
    def get_timeline(self, user_id, limit=20):
        # Timeline is pre-computed, just read from cache
        return self.timeline_cache.get_timeline(user_id, limit)
</code></pre>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Fast timeline reads (pre-computed)</li>
<li>Real-time timeline updates</li>
<li>Simple implementation</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Expensive for users with many followers</li>
<li>Storage overhead (duplicate tweets)</li>
<li>Celebrity problem (1M+ followers)</li>
</ul>
<h3>Pull Model (Read-Heavy)</h3>
<p><strong>Timeline Generation on Read</strong>:</p>
<pre><code class="language-python">class PullTimelineService:
    def get_timeline(self, user_id, limit=20):
        # Get users that this user follows
        following = self.user_service.get_following(user_id)
        
        # Get recent tweets from each followed user
        all_tweets = []
        for followed_user_id in following:
            tweets = self.tweet_service.get_user_tweets(
                followed_user_id, 
                limit=100
            )
            all_tweets.extend(tweets)
        
        # Sort by timestamp and return top tweets
        sorted_tweets = sorted(all_tweets, key=lambda x: x.created_at, reverse=True)
        return sorted_tweets[:limit]
</code></pre>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No fanout cost for popular users</li>
<li>No storage duplication</li>
<li>Consistent view of latest data</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Slow timeline generation</li>
<li>Database load on read</li>
<li>Difficult to rank by engagement</li>
</ul>
<h3>Hybrid Model (Recommended)</h3>
<p><strong>Smart Fanout Strategy</strong>:</p>
<pre><code class="language-python">class HybridTimelineService:
    def __init__(self):
        self.celebrity_threshold = 1000000  # 1M followers
        
    def fanout_tweet(self, tweet, user_id):
        follower_count = self.user_service.get_follower_count(user_id)
        
        if follower_count > self.celebrity_threshold:
            # Celebrity: don't fanout, use pull on read
            self.celebrity_tweets_cache.add(user_id, tweet)
        else:
            # Regular user: fanout to all followers
            followers = self.user_service.get_followers(user_id)
            for follower_id in followers:
                self.timeline_cache.add_to_timeline(follower_id, tweet)
    
    def get_timeline(self, user_id, limit=20):
        # Get pre-computed timeline
        timeline_tweets = self.timeline_cache.get_timeline(user_id, limit)
        
        # Get tweets from celebrities this user follows
        celebrity_following = self.user_service.get_celebrity_following(user_id)
        celebrity_tweets = []
        
        for celebrity_id in celebrity_following:
            tweets = self.celebrity_tweets_cache.get_recent_tweets(celebrity_id, 10)
            celebrity_tweets.extend(tweets)
        
        # Merge and sort all tweets
        all_tweets = timeline_tweets + celebrity_tweets
        sorted_tweets = sorted(all_tweets, key=lambda x: x.created_at, reverse=True)
        
        return sorted_tweets[:limit]
</code></pre>
<h2>Detailed Design Deep Dive</h2>
<h3>Fanout Service Architecture</h3>
<p><strong>Async Fanout Processing</strong>:</p>
<pre><code class="language-python">class FanoutService:
    def __init__(self):
        self.message_queue = MessageQueue()
        self.batch_size = 1000
        
    def queue_fanout(self, tweet):
        # Queue fanout job for async processing
        fanout_job = {
            "tweet_id": tweet.id,
            "user_id": tweet.user_id,
            "timestamp": tweet.created_at
        }
        self.message_queue.publish("fanout_queue", fanout_job)
    
    def process_fanout_batch(self, jobs):
        # Process multiple fanout jobs in batch
        for job in jobs:
            followers = self.get_followers_batch(job.user_id)
            
            # Batch insert into timeline cache
            timeline_entries = []
            for follower_id in followers:
                timeline_entries.append({
                    "user_id": follower_id,
                    "tweet_id": job.tweet_id,
                    "score": self.calculate_score(job),
                    "created_at": job.timestamp
                })
            
            self.timeline_cache.batch_insert(timeline_entries)
</code></pre>
<h3>Caching Strategy</h3>
<p><strong>Multi-Layer Cache Architecture</strong>:</p>
<ol>
<li><strong>L1 Cache</strong>: Application-level cache (Recent timelines)</li>
<li><strong>L2 Cache</strong>: Redis cluster (User timelines, tweet data)</li>
<li><strong>L3 Cache</strong>: CDN (Media files, static content)</li>
</ol>
<pre><code class="language-python">class CacheManager:
    def __init__(self):
        self.l1_cache = LRUCache(max_size=10000)  # In-memory
        self.l2_cache = RedisCluster()
        self.l3_cache = CDN()
    
    def get_timeline(self, user_id, limit=20):
        cache_key = f"timeline:{user_id}:{limit}"
        
        # Try L1 cache first
        timeline = self.l1_cache.get(cache_key)
        if timeline:
            return timeline
            
        # Try L2 cache (Redis)
        timeline = self.l2_cache.get(cache_key)
        if timeline:
            self.l1_cache.set(cache_key, timeline, ttl=60)
            return timeline
            
        # Generate timeline and cache
        timeline = self.timeline_service.generate_timeline(user_id, limit)
        
        self.l2_cache.set(cache_key, timeline, ttl=300)
        self.l1_cache.set(cache_key, timeline, ttl=60)
        
        return timeline
</code></pre>
<h3>Search Service</h3>
<p><strong>Elasticsearch Integration</strong>:</p>
<pre><code class="language-python">class SearchService:
    def __init__(self):
        self.elasticsearch = Elasticsearch()
        
    def index_tweet(self, tweet):
        doc = {
            "tweet_id": tweet.id,
            "user_id": tweet.user_id,
            "username": tweet.user.username,
            "content": tweet.content,
            "hashtags": self.extract_hashtags(tweet.content),
            "mentions": self.extract_mentions(tweet.content),
            "created_at": tweet.created_at,
            "engagement_score": self.calculate_engagement_score(tweet)
        }
        
        self.elasticsearch.index(
            index="tweets",
            id=tweet.id,
            body=doc
        )
    
    def search_tweets(self, query, limit=20, offset=0):
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        {"match": {"content": {"query": query, "boost": 2}}},
                        {"match": {"hashtags": {"query": query, "boost": 3}}},
                        {"match": {"username": {"query": query, "boost": 1.5}}}
                    ]
                }
            },
            "sort": [
                {"engagement_score": {"order": "desc"}},
                {"created_at": {"order": "desc"}}
            ],
            "size": limit,
            "from": offset
        }
        
        return self.elasticsearch.search(index="tweets", body=search_body)
</code></pre>
<h3>Trending Topics</h3>
<p><strong>Real-time Trend Detection</strong>:</p>
<pre><code class="language-python">class TrendingService:
    def __init__(self):
        self.redis = Redis()
        self.trend_window = 3600  # 1 hour window
        
    def update_hashtag_count(self, hashtag):
        current_hour = int(time.time() // self.trend_window)
        key = f"hashtag_count:{current_hour}:{hashtag}"
        
        # Increment count for current hour
        self.redis.incr(key)
        self.redis.expire(key, self.trend_window * 2)  # Keep 2 hours
        
        # Update global trending scores
        self.update_trending_score(hashtag)
    
    def get_trending_topics(self, limit=10):
        # Get top hashtags by score
        return self.redis.zrevrange("trending_hashtags", 0, limit-1, withscores=True)
    
    def calculate_trend_score(self, hashtag, current_count, historical_avg):
        # Simple trending algorithm
        if historical_avg == 0:
            return current_count
        
        trend_ratio = current_count / historical_avg
        velocity_score = trend_ratio * math.log(current_count + 1)
        
        return velocity_score
</code></pre>
<h2>Scaling Considerations</h2>
<h3>Database Sharding</h3>
<p><strong>Tweets Sharding Strategy</strong>:</p>
<pre><code class="language-python">def get_tweet_shard(tweet_id):
    # Shard by tweet_id for even distribution
    return tweet_id % NUM_TWEET_SHARDS

def get_user_shard(user_id):
    # Shard by user_id for user-related data
    return user_id % NUM_USER_SHARDS
</code></pre>
<p><strong>Timeline Sharding</strong>:</p>
<pre><code class="language-python">def get_timeline_shard(user_id):
    # Shard user timelines by user_id
    return user_id % NUM_TIMELINE_SHARDS
</code></pre>
<h3>Handling Viral Content</h3>
<p><strong>Circuit Breaker for Fanout</strong>:</p>
<pre><code class="language-python">class ViralContentHandler:
    def __init__(self):
        self.fanout_threshold = 100000  # 100K followers
        self.circuit_breaker = CircuitBreaker()
        
    def handle_viral_tweet(self, tweet, user_id):
        follower_count = self.user_service.get_follower_count(user_id)
        
        if follower_count > self.fanout_threshold:
            # Skip immediate fanout for viral content
            self.queue_delayed_fanout(tweet, delay=60)  # 1 minute delay
            
            # Use pull model for immediate reads
            self.celebrity_cache.add_hot_tweet(user_id, tweet)
        else:
            # Normal fanout
            self.fanout_service.fanout_tweet(tweet, user_id)
</code></pre>
<h3>Media Handling</h3>
<p><strong>CDN Strategy for Media</strong>:</p>
<pre><code class="language-python">class MediaService:
    def __init__(self):
        self.cdn = CloudFrontCDN()
        self.storage = S3Storage()
        
    def upload_media(self, media_file, user_id):
        # Generate unique filename
        filename = f"{user_id}/{uuid.uuid4()}.{media_file.extension}"
        
        # Upload to S3
        s3_url = self.storage.upload(filename, media_file)
        
        # Generate CDN URL
        cdn_url = self.cdn.get_url(filename)
        
        return {
            "media_id": str(uuid.uuid4()),
            "original_url": s3_url,
            "cdn_url": cdn_url,
            "thumbnail_url": self.generate_thumbnail(cdn_url)
        }
</code></pre>
<h2>Social Media Feed Design Quiz</h2>
<p>Test your understanding of social media feed system design with the interactive quiz that appears after each part of this series.</p>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Hybrid Approach</strong>: Combine push and pull models based on user characteristics</li>
<li><strong>Aggressive Caching</strong>: Multi-layer caching is essential for read performance</li>
<li><strong>Async Processing</strong>: Use message queues for fanout and background processing</li>
<li><strong>Viral Content</strong>: Design circuit breakers and fallback mechanisms</li>
<li><strong>Search Integration</strong>: Elasticsearch enables fast, relevant search results</li>
</ol>
<h2>What's Next?</h2>
<p>In Part 5, we'll design a video streaming service like YouTube, which introduces challenges around large file storage, content delivery networks, and video processing pipelines.</p>
16:T6a24,<h1>Design a Video Streaming Service (YouTube)</h1>
<p>In this part, we'll design a video streaming service like YouTube or Netflix. This introduces unique challenges around large file storage, content delivery networks, video processing, and global content distribution.</p>
<h2>1. Functional Requirements</h2>
<h3>Actors</h3>
<ul>
<li><strong>Content Creator</strong>: Uploads and manages videos</li>
<li><strong>Viewer</strong>: Watches and interacts with videos</li>
<li><strong>Content Moderator</strong>: Reviews flagged content</li>
<li><strong>System</strong>: Handles video processing and recommendations</li>
</ul>
<h3>Use Cases</h3>
<p><strong>Content Creator</strong>:</p>
<ul>
<li>Upload videos (various formats, up to 4K resolution)</li>
<li>Add metadata (title, description, thumbnails, tags)</li>
<li>View analytics (views, engagement, revenue)</li>
<li>Manage video settings (privacy, monetization)</li>
<li>Live streaming capability</li>
</ul>
<p><strong>Viewer</strong>:</p>
<ul>
<li>Search and browse videos</li>
<li>Watch videos with adaptive quality</li>
<li>Like, comment, share videos</li>
<li>Subscribe to channels</li>
<li>Create and manage playlists</li>
<li>View personalized recommendations</li>
</ul>
<p><strong>Content Moderator</strong>:</p>
<ul>
<li>Review flagged content</li>
<li>Apply community guidelines</li>
<li>Manage copyright claims</li>
</ul>
<h3>Functional Requirements</h3>
<p>✅ <strong>In Scope</strong>:</p>
<ul>
<li>Video upload and storage</li>
<li>Video transcoding (multiple resolutions)</li>
<li>Video playback with adaptive streaming</li>
<li>Search and discovery</li>
<li>User engagement (likes, comments, subscriptions)</li>
<li>Basic recommendation system</li>
<li>Analytics and metrics</li>
</ul>
<p>❌ <strong>Out of Scope</strong>:</p>
<ul>
<li>Advanced recommendation algorithms (ML-based)</li>
<li>Monetization and ad serving</li>
<li>Live streaming infrastructure</li>
<li>Advanced content moderation</li>
<li>Content creator studio tools</li>
</ul>
<h2>2. Non-Functional Requirements</h2>
<h3>Scalability</h3>
<ul>
<li>Support 2 billion users globally</li>
<li>Handle 500 hours of video uploaded per minute</li>
<li>Support 1 billion hours watched per day</li>
<li>Handle traffic spikes during viral events</li>
</ul>
<h3>Availability</h3>
<ul>
<li>99.9% uptime for video playback</li>
<li>99.5% uptime for video uploads</li>
<li>Global content distribution</li>
<li>Graceful degradation during failures</li>
</ul>
<h3>Performance</h3>
<ul>
<li>Video start time: &#x3C;2 seconds globally</li>
<li>Upload processing: &#x3C;30 minutes for 1-hour video</li>
<li>Search results: &#x3C;300ms</li>
<li>Support 4K streaming with &#x3C;1% rebuffering</li>
</ul>
<h3>Storage &#x26; Bandwidth</h3>
<ul>
<li>Petabyte-scale storage requirements</li>
<li>Multi-region content replication</li>
<li>Intelligent content placement</li>
<li>Bandwidth optimization</li>
</ul>
<h2>3. Estimations</h2>
<h3>User Metrics</h3>
<ul>
<li><strong>Total Users</strong>: 2 billion</li>
<li><strong>Daily Active Users</strong>: 500 million</li>
<li><strong>Average watch time per user</strong>: 40 minutes/day</li>
<li><strong>Concurrent viewers</strong>: 50 million peak</li>
</ul>
<h3>Video Metrics</h3>
<ul>
<li><strong>Videos uploaded per day</strong>: 720,000 (500 hours/min × 60 min/hour × 24 hours)</li>
<li><strong>Video views per day</strong>: 5 billion</li>
<li><strong>Average video length</strong>: 10 minutes</li>
<li><strong>Video upload formats</strong>: 90% mobile (1080p), 10% professional (4K)</li>
</ul>
<h3>Storage Estimations</h3>
<p><strong>Per Video Storage</strong> (Multiple Resolutions):</p>
<ul>
<li>Original file: 1 GB (10 min @ 4K)</li>
<li>1080p: 400 MB</li>
<li>720p: 200 MB</li>
<li>480p: 100 MB</li>
<li>360p: 50 MB</li>
<li>Thumbnails: 1 MB</li>
<li><strong>Total per video</strong>: ~1.75 GB</li>
</ul>
<p><strong>Storage Growth</strong>:</p>
<ul>
<li><strong>Per Day</strong>: 720K videos × 1.75 GB = 1.26 PB/day</li>
<li><strong>Per Year</strong>: 1.26 PB × 365 = 460 PB/year</li>
<li><strong>With Replication (3x)</strong>: 1.38 EB/year</li>
</ul>
<h3>Bandwidth Estimations</h3>
<ul>
<li><strong>Average bitrate</strong>: 2 Mbps (adaptive streaming)</li>
<li><strong>Concurrent viewers</strong>: 50M × 2 Mbps = 100 Tbps</li>
<li><strong>Daily bandwidth</strong>: 50M × 2 Mbps × 40 min = 400 TB/day</li>
</ul>
<h2>4. Design Goals</h2>
<h3>Performance Requirements</h3>
<ul>
<li><strong>Video Start Time</strong>: &#x3C;2 seconds globally</li>
<li><strong>Buffering</strong>: &#x3C;1% rebuffering ratio</li>
<li><strong>Upload Speed</strong>: Support simultaneous uploads</li>
<li><strong>Search Latency</strong>: &#x3C;300ms</li>
</ul>
<h3>Architecture Patterns</h3>
<ul>
<li><strong>Microservices</strong>: Decomposed by functionality</li>
<li><strong>Event-Driven</strong>: Video processing workflows</li>
<li><strong>CQRS</strong>: Separate read/write models for metadata</li>
</ul>
<h3>Usage Patterns</h3>
<ul>
<li><strong>Read Heavy</strong>: 100:1 read to write ratio</li>
<li><strong>Large Files</strong>: Multi-GB video files</li>
<li><strong>Global Distribution</strong>: Viewers worldwide</li>
<li><strong>Batch Processing</strong>: Video encoding workflows</li>
</ul>
<h2>5. High-Level Design</h2>
<h3>Building Blocks</h3>
<pre><code>[Client] → [CDN] → [Load Balancer] → [API Gateway]
                           ↓              ↓
                   [Video Service] → [Upload Service]
                           ↓              ↓
                   [Metadata DB] ← [Video Processing]
                           ↓              ↓
                   [Search Service] → [Blob Storage]
                           ↓              ↓
                   [Analytics] ← [Recommendation Service]
</code></pre>
<h3>Core Components</h3>
<ol>
<li><strong>CDN</strong>: Global content delivery network</li>
<li><strong>Upload Service</strong>: Handles video file uploads</li>
<li><strong>Video Processing</strong>: Transcoding and optimization</li>
<li><strong>Metadata Service</strong>: Video information and user data</li>
<li><strong>Search Service</strong>: Video discovery and search</li>
<li><strong>Streaming Service</strong>: Adaptive video delivery</li>
<li><strong>Analytics Service</strong>: View tracking and metrics</li>
</ol>
<h3>API Design</h3>
<p><strong>Upload Video</strong>:</p>
<pre><code class="language-http">POST /api/v1/videos/upload
Authorization: Bearer {token}
Content-Type: multipart/form-data

{
  "title": "Amazing Travel Video",
  "description": "My trip to Iceland",
  "tags": ["travel", "iceland", "nature"],
  "category": "travel",
  "privacy": "public",
  "thumbnail": {file},
  "video_file": {file}
}

Response:
{
  "video_id": "abc123def456",
  "upload_url": "https://upload.example.com/abc123def456",
  "status": "processing",
  "estimated_completion": "2024-06-17T10:30:00Z"
}
</code></pre>
<p><strong>Get Video</strong>:</p>
<pre><code class="language-http">GET /api/v1/videos/{video_id}

Response:
{
  "video_id": "abc123def456",
  "title": "Amazing Travel Video",
  "description": "My trip to Iceland",
  "channel": {
    "channel_id": "channel_789",
    "name": "Travel Enthusiast",
    "subscriber_count": 15420
  },
  "duration": 600,
  "views": 12543,
  "likes": 892,
  "upload_date": "2024-06-17T10:00:00Z",
  "streaming_urls": {
    "4k": "https://cdn.example.com/abc123def456/4k.m3u8",
    "1080p": "https://cdn.example.com/abc123def456/1080p.m3u8",
    "720p": "https://cdn.example.com/abc123def456/720p.m3u8",
    "480p": "https://cdn.example.com/abc123def456/480p.m3u8"
  },
  "thumbnails": {
    "default": "https://cdn.example.com/abc123def456/thumb.jpg",
    "medium": "https://cdn.example.com/abc123def456/thumb_medium.jpg"
  }
}
</code></pre>
<p><strong>Search Videos</strong>:</p>
<pre><code class="language-http">GET /api/v1/search?q=travel iceland&#x26;limit=20&#x26;offset=0&#x26;sort=relevance

Response:
{
  "results": [
    {
      "video_id": "abc123def456",
      "title": "Amazing Travel Video",
      "thumbnail": "https://cdn.example.com/abc123def456/thumb.jpg",
      "duration": 600,
      "views": 12543,
      "channel_name": "Travel Enthusiast",
      "upload_date": "2024-06-17T10:00:00Z"
    }
  ],
  "total_results": 15420,
  "next_page_token": "eyJvZmZzZXQiOjIwfQ=="
}
</code></pre>
<h3>Database Schema</h3>
<p><strong>Videos Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE videos (
    video_id VARCHAR(20) PRIMARY KEY,
    channel_id BIGINT NOT NULL,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    duration INT NOT NULL,
    category VARCHAR(50),
    privacy ENUM('public', 'unlisted', 'private') DEFAULT 'public',
    status ENUM('processing', 'ready', 'failed') DEFAULT 'processing',
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    view_count BIGINT DEFAULT 0,
    like_count INT DEFAULT 0,
    dislike_count INT DEFAULT 0,
    comment_count INT DEFAULT 0,
    
    INDEX idx_channel_date (channel_id, upload_date),
    INDEX idx_category_views (category, view_count),
    FULLTEXT idx_search (title, description)
);
</code></pre>
<p><strong>Video Files Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE video_files (
    video_id VARCHAR(20),
    resolution ENUM('4k', '1080p', '720p', '480p', '360p'),
    file_url VARCHAR(500) NOT NULL,
    file_size BIGINT NOT NULL,
    bitrate INT NOT NULL,
    codec VARCHAR(20) NOT NULL,
    
    PRIMARY KEY (video_id, resolution),
    FOREIGN KEY (video_id) REFERENCES videos(video_id)
);
</code></pre>
<p><strong>Channels Table</strong>:</p>
<pre><code class="language-sql">CREATE TABLE channels (
    channel_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    subscriber_count BIGINT DEFAULT 0,
    video_count INT DEFAULT 0,
    total_views BIGINT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_subscribers (subscriber_count),
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);
</code></pre>
<p><strong>View Events Table</strong> (Time-series data):</p>
<pre><code class="language-sql">CREATE TABLE view_events (
    event_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    video_id VARCHAR(20) NOT NULL,
    user_id BIGINT,
    session_id VARCHAR(50),
    watched_duration INT NOT NULL,
    total_duration INT NOT NULL,
    quality VARCHAR(10),
    device_type VARCHAR(20),
    geo_location VARCHAR(10),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_video_time (video_id, timestamp),
    INDEX idx_user_time (user_id, timestamp)
) PARTITION BY RANGE (UNIX_TIMESTAMP(timestamp));
</code></pre>
<h2>Video Processing Pipeline</h2>
<h3>Upload and Processing Workflow</h3>
<pre><code class="language-python">class VideoProcessingPipeline:
    def __init__(self):
        self.upload_service = UploadService()
        self.transcoding_service = TranscodingService()
        self.storage_service = StorageService()
        self.metadata_service = MetadataService()
        
    def process_upload(self, video_file, metadata):
        # 1. Generate unique video ID
        video_id = self.generate_video_id()
        
        # 2. Upload original file to staging storage
        staging_url = self.upload_service.upload_to_staging(video_file, video_id)
        
        # 3. Extract video metadata
        video_info = self.extract_video_metadata(staging_url)
        
        # 4. Create database record
        self.metadata_service.create_video_record(video_id, metadata, video_info)
        
        # 5. Queue transcoding jobs
        self.queue_transcoding_jobs(video_id, staging_url, video_info)
        
        return {"video_id": video_id, "status": "processing"}
    
    def queue_transcoding_jobs(self, video_id, source_url, video_info):
        resolutions = self.determine_target_resolutions(video_info)
        
        for resolution in resolutions:
            job = {
                "video_id": video_id,
                "source_url": source_url,
                "target_resolution": resolution,
                "output_format": "mp4",
                "codec": "h264"
            }
            
            self.transcoding_queue.publish(job)
</code></pre>
<h3>Transcoding Service</h3>
<pre><code class="language-python">class TranscodingService:
    def __init__(self):
        self.ffmpeg = FFMpegWrapper()
        self.storage = StorageService()
        
    def transcode_video(self, job):
        video_id = job['video_id']
        source_url = job['source_url']
        resolution = job['target_resolution']
        
        try:
            # 1. Download source file
            local_source = self.download_source_file(source_url)
            
            # 2. Transcode to target resolution
            output_file = self.ffmpeg.transcode(
                input_file=local_source,
                resolution=resolution,
                codec=job['codec'],
                bitrate=self.get_target_bitrate(resolution)
            )
            
            # 3. Upload transcoded file to CDN
            cdn_url = self.storage.upload_to_cdn(output_file, video_id, resolution)
            
            # 4. Generate thumbnails
            thumbnails = self.generate_thumbnails(local_source, video_id)
            
            # 5. Update metadata with file URLs
            self.metadata_service.update_video_files(video_id, {
                "resolution": resolution,
                "file_url": cdn_url,
                "file_size": os.path.getsize(output_file),
                "bitrate": self.get_target_bitrate(resolution)
            })
            
            # 6. Cleanup temporary files
            self.cleanup_temp_files([local_source, output_file])
            
        except Exception as e:
            self.handle_transcoding_error(video_id, resolution, str(e))
    
    def get_target_bitrate(self, resolution):
        bitrates = {
            "4k": 20000,      # 20 Mbps
            "1080p": 8000,    # 8 Mbps
            "720p": 4000,     # 4 Mbps
            "480p": 2000,     # 2 Mbps
            "360p": 1000      # 1 Mbps
        }
        return bitrates.get(resolution, 2000)
</code></pre>
<h2>Content Delivery and Streaming</h2>
<h3>CDN Architecture</h3>
<pre><code class="language-python">class CDNManager:
    def __init__(self):
        self.primary_regions = ["us-east", "us-west", "eu-west", "asia-pacific"]
        self.edge_locations = self.load_edge_locations()
        
    def upload_to_cdn(self, video_file, video_id, resolution):
        # 1. Upload to primary storage
        primary_url = self.upload_to_primary_storage(video_file, video_id, resolution)
        
        # 2. Replicate to major regions
        replication_jobs = []
        for region in self.primary_regions:
            job = {
                "source_url": primary_url,
                "target_region": region,
                "video_id": video_id,
                "resolution": resolution
            }
            replication_jobs.append(job)
        
        self.queue_replication_jobs(replication_jobs)
        
        return primary_url
    
    def get_optimal_cdn_url(self, video_id, resolution, user_location):
        # Find nearest CDN edge location
        nearest_edge = self.find_nearest_edge(user_location)
        
        # Check if content is available at edge
        edge_url = f"https://{nearest_edge}/videos/{video_id}/{resolution}.mp4"
        
        if self.check_content_availability(edge_url):
            return edge_url
        else:
            # Fallback to regional CDN
            regional_url = self.get_regional_url(video_id, resolution, user_location)
            
            # Trigger cache warming for future requests
            self.trigger_cache_warming(edge_url, regional_url)
            
            return regional_url
</code></pre>
<h3>Adaptive Streaming</h3>
<pre><code class="language-python">class AdaptiveStreamingService:
    def generate_hls_manifest(self, video_id, available_resolutions):
        """Generate HLS master playlist for adaptive streaming"""
        
        manifest = "#EXTM3U\n#EXT-X-VERSION:3\n\n"
        
        for resolution in available_resolutions:
            bandwidth = self.get_bandwidth_for_resolution(resolution)
            resolution_str = self.get_resolution_string(resolution)
            
            manifest += f"#EXT-X-STREAM-INF:BANDWIDTH={bandwidth},RESOLUTION={resolution_str}\n"
            manifest += f"{resolution}.m3u8\n"
        
        return manifest
    
    def generate_resolution_playlist(self, video_id, resolution):
        """Generate playlist for specific resolution"""
        
        # Get video segments for this resolution
        segments = self.get_video_segments(video_id, resolution)
        
        playlist = "#EXTM3U\n#EXT-X-VERSION:3\n#EXT-X-TARGETDURATION:10\n\n"
        
        for segment in segments:
            playlist += f"#EXTINF:{segment.duration},\n"
            playlist += f"{segment.url}\n"
        
        playlist += "#EXT-X-ENDLIST\n"
        
        return playlist
    
    def get_bandwidth_for_resolution(self, resolution):
        bandwidths = {
            "4k": 20000000,     # 20 Mbps
            "1080p": 8000000,   # 8 Mbps
            "720p": 4000000,    # 4 Mbps
            "480p": 2000000,    # 2 Mbps
            "360p": 1000000     # 1 Mbps
        }
        return bandwidths.get(resolution, 2000000)
</code></pre>
<h2>Search and Discovery</h2>
<h3>Video Search Service</h3>
<pre><code class="language-python">class VideoSearchService:
    def __init__(self):
        self.elasticsearch = Elasticsearch()
        self.search_analytics = SearchAnalytics()
        
    def index_video(self, video):
        """Index video for search"""
        
        doc = {
            "video_id": video.video_id,
            "title": video.title,
            "description": video.description,
            "tags": video.tags,
            "category": video.category,
            "channel_name": video.channel.name,
            "upload_date": video.upload_date,
            "duration": video.duration,
            "view_count": video.view_count,
            "like_count": video.like_count,
            "engagement_score": self.calculate_engagement_score(video)
        }
        
        self.elasticsearch.index(
            index="videos",
            id=video.video_id,
            body=doc
        )
    
    def search_videos(self, query, filters=None, limit=20, offset=0):
        """Search videos with ranking"""
        
        search_body = {
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": [
                                    "title^3",
                                    "description^2", 
                                    "tags^2",
                                    "channel_name^1.5"
                                ]
                            }
                        }
                    ],
                    "filter": self.build_filters(filters)
                }
            },
            "sort": [
                {"_score": {"order": "desc"}},
                {"engagement_score": {"order": "desc"}},
                {"upload_date": {"order": "desc"}}
            ],
            "size": limit,
            "from": offset
        }
        
        results = self.elasticsearch.search(index="videos", body=search_body)
        
        # Log search for analytics
        self.search_analytics.log_search(query, results['hits']['total']['value'])
        
        return self.format_search_results(results)
    
    def calculate_engagement_score(self, video):
        """Calculate video engagement score for ranking"""
        
        age_in_days = (datetime.now() - video.upload_date).days
        age_factor = 1.0 / (age_in_days + 1)
        
        view_score = math.log(video.view_count + 1)
        like_ratio = video.like_count / max(video.view_count, 1)
        
        engagement_score = (view_score * 0.7 + like_ratio * 100 * 0.3) * age_factor
        
        return engagement_score
</code></pre>
<h2>Analytics and Monitoring</h2>
<h3>Video Analytics</h3>
<pre><code class="language-python">class VideoAnalyticsService:
    def __init__(self):
        self.time_series_db = InfluxDB()
        self.cache = Redis()
        
    def track_video_view(self, video_id, user_id, watch_data):
        """Track video view event"""
        
        event = {
            "video_id": video_id,
            "user_id": user_id,
            "watched_duration": watch_data['watched_duration'],
            "total_duration": watch_data['total_duration'],
            "completion_rate": watch_data['watched_duration'] / watch_data['total_duration'],
            "quality": watch_data['quality'],
            "device_type": watch_data['device_type'],
            "geo_location": watch_data['geo_location'],
            "timestamp": datetime.utcnow()
        }
        
        # Store in time-series database
        self.time_series_db.write_point("video_views", event)
        
        # Update real-time counters
        self.update_realtime_metrics(video_id, event)
    
    def update_realtime_metrics(self, video_id, event):
        """Update real-time view counters"""
        
        # Increment view count
        self.cache.incr(f"video_views:{video_id}")
        
        # Update hourly view count
        hour_key = f"video_views_hourly:{video_id}:{datetime.utcnow().strftime('%Y%m%d%H')}"
        self.cache.incr(hour_key)
        self.cache.expire(hour_key, 86400)  # 24 hours
        
        # Track completion rate
        if event['completion_rate'] > 0.8:  # 80% completion
            self.cache.incr(f"video_completions:{video_id}")
    
    def get_video_analytics(self, video_id, time_range="24h"):
        """Get analytics for a specific video"""
        
        metrics = {
            "total_views": self.cache.get(f"video_views:{video_id}") or 0,
            "hourly_views": self.get_hourly_views(video_id, time_range),
            "completion_rate": self.calculate_completion_rate(video_id),
            "geographic_distribution": self.get_geographic_stats(video_id, time_range),
            "device_breakdown": self.get_device_stats(video_id, time_range),
            "quality_distribution": self.get_quality_stats(video_id, time_range)
        }
        
        return metrics
</code></pre>
<h2>Scaling Considerations</h2>
<h3>Storage Optimization</h3>
<p><strong>Intelligent Storage Tiering</strong>:</p>
<pre><code class="language-python">class StorageTierManager:
    def __init__(self):
        self.hot_storage = "SSD_TIER"      # Recent, popular content
        self.warm_storage = "HDD_TIER"     # Older, moderate popularity
        self.cold_storage = "GLACIER_TIER" # Archived content
        
    def determine_storage_tier(self, video_id):
        video_stats = self.get_video_stats(video_id)
        
        # Recent videos (&#x3C; 30 days) → Hot storage
        if video_stats['age_days'] &#x3C; 30:
            return self.hot_storage
            
        # Popular videos (> 1000 views/day) → Hot storage
        if video_stats['daily_views'] > 1000:
            return self.hot_storage
            
        # Moderate popularity → Warm storage
        if video_stats['daily_views'] > 10:
            return self.warm_storage
            
        # Low popularity → Cold storage
        return self.cold_storage
    
    def migrate_storage_tier(self, video_id, target_tier):
        current_urls = self.get_video_file_urls(video_id)
        
        for resolution, url in current_urls.items():
            # Copy to new storage tier
            new_url = self.copy_to_tier(url, target_tier)
            
            # Update database with new URL
            self.update_video_file_url(video_id, resolution, new_url)
            
            # Delete from old tier (after verification)
            self.schedule_deletion(url, delay="24h")
</code></pre>
<h3>Global Distribution</h3>
<p><strong>Edge Cache Management</strong>:</p>
<pre><code class="language-python">class EdgeCacheManager:
    def __init__(self):
        self.popularity_threshold = 1000  # views per hour
        self.cache_regions = ["NA", "EU", "ASIA", "SA", "AFRICA"]
        
    def should_cache_at_edge(self, video_id):
        recent_views = self.get_recent_views(video_id, hours=1)
        return recent_views > self.popularity_threshold
    
    def predict_viral_content(self, video_id):
        """Predict if content will go viral based on early metrics"""
        
        # Get first hour metrics
        first_hour_views = self.get_views_in_timeframe(video_id, "1h")
        first_hour_engagement = self.get_engagement_rate(video_id, "1h")
        
        # Simple viral prediction
        viral_score = first_hour_views * first_hour_engagement
        
        if viral_score > 10000:  # Threshold for viral prediction
            # Pre-cache in all regions
            self.precache_in_all_regions(video_id)
            return True
            
        return False
</code></pre>
<h2>Video Streaming Design Quiz</h2>
<p>Test your understanding of video streaming system design with the interactive quiz that appears after each part of this series.</p>
<h2>Security and Content Protection</h2>
<h3>Copyright Protection</h3>
<ul>
<li>Content ID system for automatic detection</li>
<li>DMCA takedown process automation</li>
<li>Watermarking and fingerprinting</li>
</ul>
<h3>Access Control</h3>
<ul>
<li>JWT-based authentication</li>
<li>CDN token authentication</li>
<li>Geographic content restrictions</li>
</ul>
<h3>DRM Implementation</h3>
<ul>
<li>Encrypted video streams</li>
<li>License server integration</li>
<li>Device-specific decryption keys</li>
</ul>
<h2>Key Takeaways</h2>
<ol>
<li><strong>Multi-Resolution Strategy</strong>: Store videos in multiple qualities for adaptive streaming</li>
<li><strong>Global CDN</strong>: Essential for low latency worldwide video delivery</li>
<li><strong>Intelligent Caching</strong>: Predict and pre-cache viral content</li>
<li><strong>Storage Tiering</strong>: Optimize costs with hot/warm/cold storage</li>
<li><strong>Async Processing</strong>: Use message queues for video transcoding workflows</li>
</ol>
<h2>What's Next?</h2>
<p>In Part 6, we'll design a distributed cache system like Redis, which covers fundamental concepts of caching, data consistency, and distributed system coordination.</p>
2:["$","article",null,{"className":"min-h-screen bg-white","children":[["$","header",null,{"className":"bg-white border-b border-gray-200","children":["$","div",null,{"className":"medium-container py-12","children":[["$","div",null,{"className":"max-w-3xl mx-auto text-center","children":[["$","div",null,{"className":"mb-6","children":[["$","div",null,{"className":"flex flex-wrap justify-center gap-2 mb-6","children":[["$","span","terraform",{"className":"tag","children":"terraform"}],["$","span","infrastructure",{"className":"tag","children":"infrastructure"}],["$","span","devops",{"className":"tag","children":"devops"}],["$","span","cloud",{"className":"tag","children":"cloud"}],["$","span","iac",{"className":"tag","children":"iac"}]]}],["$","h1",null,{"className":"text-4xl md:text-5xl font-bold text-gray-900 mb-6 leading-tight","children":"Advanced Features"}],["$","p",null,{"className":"text-xl text-gray-600 leading-relaxed","children":"A comprehensive guide covering Terraform from basics to advanced concepts, with hands-on examples and best practices."}]]}],["$","div",null,{"className":"flex items-center justify-center space-x-6 text-gray-500","children":[["$","div",null,{"className":"flex items-center space-x-2","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-user w-5 h-5","children":[["$","path","975kel",{"d":"M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2"}],["$","circle","17ys0d",{"cx":"12","cy":"7","r":"4"}],"$undefined"]}],["$","span",null,{"className":"font-medium","children":"Abstract Algorithms"}]]}],["$","div",null,{"className":"flex items-center space-x-2","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar w-5 h-5","children":[["$","rect","eu3xkr",{"width":"18","height":"18","x":"3","y":"4","rx":"2","ry":"2"}],["$","line","m3sa8f",{"x1":"16","x2":"16","y1":"2","y2":"6"}],["$","line","18kwsl",{"x1":"8","x2":"8","y1":"2","y2":"6"}],["$","line","xt86sb",{"x1":"3","x2":"21","y1":"10","y2":"10"}],"$undefined"]}],["$","span",null,{"children":"over 1 year ago"}]]}],["$","div",null,{"className":"flex items-center space-x-2","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-clock w-5 h-5","children":[["$","circle","1mglay",{"cx":"12","cy":"12","r":"10"}],["$","polyline","68esgv",{"points":"12 6 12 12 16 14"}],"$undefined"]}],["$","span",null,{"children":"5 min read"}]]}]]}],["$","div",null,{"className":"flex items-center justify-center space-x-4 mt-8","children":[["$","button",null,{"className":"flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-share2 w-4 h-4","children":[["$","circle","gq8acd",{"cx":"18","cy":"5","r":"3"}],["$","circle","w7nqdw",{"cx":"6","cy":"12","r":"3"}],["$","circle","1xt0gg",{"cx":"18","cy":"19","r":"3"}],["$","line","47mynk",{"x1":"8.59","x2":"15.42","y1":"13.51","y2":"17.49"}],["$","line","1n3mei",{"x1":"15.41","x2":"8.59","y1":"6.51","y2":"10.49"}],"$undefined"]}],["$","span",null,{"children":"Share"}]]}],["$","button",null,{"className":"flex items-center space-x-2 px-4 py-2 text-gray-600 hover:text-gray-900 bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-bookmark w-4 h-4","children":[["$","path","1fy3hk",{"d":"m19 21-7-4-7 4V5a2 2 0 0 1 2-2h10a2 2 0 0 1 2 2v16z"}],"$undefined"]}],["$","span",null,{"children":"Save"}]]}]]}]]}],["$","div",null,{"className":"max-w-4xl mx-auto mt-12","children":["$","div",null,{"className":"relative aspect-video rounded-xl overflow-hidden shadow-lg","children":["$","$Lc",null,{"src":"/posts/learning-terraform-series/assets/overview.png","alt":"Advanced Features","fill":true,"className":"object-cover","priority":true}]}]}]]}]}],["$","nav",null,{"className":"flex flex-col items-center my-8","children":[["$","div",null,{"className":"text-sm text-gray-500 mb-2","children":["Learning Terraform"," (Part ",4," of ",5,")"]}],["$","div",null,{"className":"flex gap-4","children":[["$","$Ld",null,{"href":"/posts/learning-terraform-series/part-3","className":"px-4 py-2 bg-gray-100 rounded hover:bg-gray-200 text-gray-700 font-medium transition-colors","children":"← Previous"}],["$","span",null,{"className":"px-4 py-2 text-green-700 font-semibold","children":"Current"}],["$","$Ld",null,{"href":"/posts/learning-terraform-series/part-5","className":"px-4 py-2 bg-green-100 rounded hover:bg-green-200 text-green-700 font-medium transition-colors","children":"Next →"}]]}]]}],["$","div",null,{"className":"medium-container py-8","children":["$","div",null,{"className":"max-w-3xl mx-auto","children":[["$","$Le",null,{"slug":"learning-terraform-series/part-4"}],["$","$Lf",null,{}]]}]}],["$","$L10",null,{"posts":[{"slug":"hash-tables-ultimate-guide","title":"Hash Tables: The Ultimate Guide","date":"2024-04-05","excerpt":"Comprehensive exploration of hash tables, from core concepts to advanced techniques, enhanced with illustrative graphics.","content":"$11","author":"Abstract Algorithms","tags":["data-structures","hash-tables","algorithms","performance"],"readingTime":"4 min read","coverImage":"/posts/hash-tables-ultimate-guide/assets/overview.png","fixedUrl":"$undefined","series":"$undefined"},{"slug":"system-design-interview","title":"System Design Mastery: Complete Guide","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$12","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"6 min read","coverImage":"/posts/system-design-interview/assets/intro.png","fixedUrl":"$undefined","series":{"name":"System Design Mastery","order":1,"total":6,"prev":null,"next":"/posts/system-design-interview/part-2"}},{"slug":"system-design-interview/part-2","title":"Introduction & Methodology","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$13","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"7 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":2,"total":6,"prev":"/posts/system-design-interview","next":"/posts/system-design-interview/part-3","parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}},{"slug":"system-design-interview/part-3","title":"Design a URL Shortener (TinyURL)","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$14","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"9 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":3,"total":6,"prev":"/posts/system-design-interview/part-2","next":"/posts/system-design-interview/part-4","parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}},{"slug":"system-design-interview/part-4","title":"Design a Chat System (WhatsApp)","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$15","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"10 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":4,"total":6,"prev":"/posts/system-design-interview/part-3","next":"/posts/system-design-interview/part-5","parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}},{"slug":"system-design-interview/part-5","title":"Design a Social Media Feed (Twitter)","date":"2024-04-01","excerpt":"Master system design interviews with this comprehensive 6-part series covering methodology, top interview questions, and hands-on practice.","content":"$16","author":"Abstract Algorithms","tags":["system-design","interview","scalability","architecture"],"readingTime":"11 min read","coverImage":"/posts/system-design-interview/assets/intro.png","series":{"name":"System Design Mastery","order":5,"total":6,"prev":"/posts/system-design-interview/part-4","next":"/posts/system-design-interview/part-6","parts":[{"order":1,"slug":"system-design-interview","title":"Introduction & Methodology"},{"order":2,"slug":"system-design-interview/part-2","title":"Design a URL Shortener (TinyURL)"},{"order":3,"slug":"system-design-interview/part-3","title":"Design a Chat System (WhatsApp)"},{"order":4,"slug":"system-design-interview/part-4","title":"Design a Social Media Feed (Twitter)"},{"order":5,"slug":"system-design-interview/part-5","title":"Design a Video Streaming Service (YouTube)"},{"order":6,"slug":"system-design-interview/part-6","title":"Design a Distributed Cache (Redis)"}]}}]}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"Advanced Features\",\"description\":\"A comprehensive guide covering Terraform from basics to advanced concepts, with hands-on examples and best practices.\",\"datePublished\":\"2024-02-20\",\"dateModified\":\"2024-02-20\",\"author\":{\"@type\":\"Person\",\"name\":\"Abstract Algorithms\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Abstract Algorithms\",\"url\":\"https://abstractalgorithms.github.io\"},\"url\":\"https://abstractalgorithms.github.io/posts/learning-terraform-series/part-4\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://abstractalgorithms.github.io/posts/learning-terraform-series/part-4\"},\"image\":{\"@type\":\"ImageObject\",\"url\":\"https://abstractalgorithms.github.io/posts/learning-terraform-series/assets/overview.png\"}}"}}]]}]
b:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Advanced Features | Abstract Algorithms"}],["$","meta","3",{"name":"description","content":"A comprehensive guide covering Terraform from basics to advanced concepts, with hands-on examples and best practices."}],["$","meta","4",{"name":"author","content":"Abstract Algorithms"}],["$","meta","5",{"name":"keywords","content":"algorithms,data structures,system design,software engineering,programming,computer science,performance optimization,big o notation,hash tables,database indexing"}],["$","meta","6",{"name":"creator","content":"Abstract Algorithms"}],["$","meta","7",{"name":"publisher","content":"Abstract Algorithms"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","meta","10",{"property":"og:title","content":"Advanced Features"}],["$","meta","11",{"property":"og:description","content":"A comprehensive guide covering Terraform from basics to advanced concepts, with hands-on examples and best practices."}],["$","meta","12",{"property":"og:type","content":"article"}],["$","meta","13",{"property":"article:published_time","content":"2024-02-20"}],["$","meta","14",{"property":"article:author","content":"Abstract Algorithms"}],["$","meta","15",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","16",{"name":"twitter:title","content":"Abstract Algorithms"}],["$","meta","17",{"name":"twitter:description","content":"A comprehensive blog about algorithms, data structures, system design, and software engineering best practices"}],["$","link","18",{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link","19",{"rel":"icon","href":"/icon.svg","type":"image/svg+xml","sizes":"32x32"}],["$","link","20",{"rel":"apple-touch-icon","href":"/apple-icon.svg","type":"image/svg+xml","sizes":"180x180"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
