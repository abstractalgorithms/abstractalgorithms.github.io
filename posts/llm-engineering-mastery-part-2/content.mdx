export const metadata = {
  "postId": "8e7d5b2c-9f3a-4e1b-8c6d-1a2b3c4d5e6f",
  "title": "LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems",
  "date": "2024-02-03",
  "excerpt": "Part 2 of the LLM Engineering Mastery series: Master advanced prompt engineering techniques and build production-ready RAG systems for enhanced LLM applications.",
  "author": "Abstract Algorithms",
  "tags": [
    "llm",
    "prompt-engineering",
    "rag",
    "vector-databases",
    "retrieval"
  ],
  "coverImage": "./assets/prompt-engineering-rag-cover.png",
  "series": {
    "name": "LLM Engineering Mastery",
    "order": 2,
    "total": 3,
    "prev": "/posts/llm-engineering-mastery-part-1/",
    "next": "/posts/llm-engineering-mastery-part-3/",
    "overview": "/posts/llm-engineering-mastery-series/"
  }
};


# LLM Engineering Mastery: Part 2 - Advanced Prompt Engineering and RAG Systems

Building on the foundation model integration from Part 1, we now dive deep into advanced prompt engineering techniques and Retrieval-Augmented Generation (RAG) systems that can dramatically enhance your LLM applications' capabilities and reliability.

## Advanced Prompt Engineering Techniques

### 1. Few-Shot Learning Patterns

Few-shot prompting provides examples to guide the model's behavior and output format.

```python
class FewShotPromptBuilder:
    def __init__(self):
        self.examples = {}
    
    def add_example(self, category: str, input_text: str, output_text: str):
        """Add an example for few-shot learning"""
        if category not in self.examples:
            self.examples[category] = []
        
        self.examples[category].append({
            "input": input_text,
            "output": output_text
        })
    
    def build_prompt(self, category: str, query: str, max_examples: int = 3) -> str:
        """Build a few-shot prompt with examples"""
        if category not in self.examples:
            return query
        
        examples = self.examples[category][:max_examples]
        
        prompt_parts = [
            "Here are some examples of the expected format:",
            ""
        ]
        
        for i, example in enumerate(examples, 1):
            prompt_parts.extend([
                "Example " + str(i) + ":",
                "Input: " + example["input"],
                "Output: " + example["output"],
                ""
            ])
        
        prompt_parts.extend([
            "Now, please process this input:",
            "Input: " + query,
            "Output:"
        ])
        
        return "\n".join(prompt_parts)

# Usage for code generation
prompt_builder = FewShotPromptBuilder()

# Add examples for Python function generation
prompt_builder.add_example(
    "python_function",
    "Create a function to calculate factorial",
    """def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)"""
)

prompt_builder.add_example(
    "python_function", 
    "Create a function to check if a string is palindrome",
    """def is_palindrome(s):
    s = s.lower().replace(' ', '')
    return s == s[::-1]"""
)

# Generate prompt for new task
prompt = prompt_builder.build_prompt(
    "python_function",
    "Create a function to find the maximum element in a list"
)
```

### 2. Chain-of-Thought (CoT) Reasoning

Chain-of-thought prompting encourages step-by-step reasoning for complex problems.

```python
class ChainOfThoughtPrompt:
    def __init__(self):
        self.reasoning_templates = {
            "problem_solving": """Let's solve this step by step:

1. First, I need to understand what the problem is asking
2. Then, I'll identify the key information given
3. Next, I'll determine what approach to use
4. Finally, I'll work through the solution step by step

Problem: {problem}

Step-by-step solution:""",
            
            "code_debugging": """Let me debug this code systematically:

1. First, I'll read through the code to understand its purpose
2. Then, I'll identify potential issues or errors
3. Next, I'll analyze the logic flow
4. Finally, I'll provide the corrected version with explanations

Code to debug: {code}

Debugging analysis:""",
            
            "data_analysis": """Let me analyze this data step by step:

1. First, I'll examine the data structure and format
2. Then, I'll identify patterns and key metrics
3. Next, I'll consider what insights can be drawn
4. Finally, I'll provide conclusions and recommendations

Data: {data}

Analysis:"""
        }
    
    def generate_cot_prompt(self, template_type: str, **kwargs) -> str:
        """Generate a chain-of-thought prompt"""
        if template_type not in self.reasoning_templates:
            raise ValueError("Unknown template type: " + template_type)
        
        return self.reasoning_templates[template_type].format(**kwargs)
    
    def create_custom_cot(self, problem_description: str, steps: list) -> str:
        """Create a custom chain-of-thought prompt"""
        prompt_parts = [
            "Let's approach this systematically:",
            ""
        ]
        
        for i, step in enumerate(steps, 1):
            prompt_parts.append(str(i) + ". " + step)
        
        prompt_parts.extend([
            "",
            "Problem: " + problem_description,
            "",
            "Step-by-step solution:"
        ])
        
        return "\n".join(prompt_parts)

# Usage example
cot = ChainOfThoughtPrompt()

# For complex problem solving
math_prompt = cot.generate_cot_prompt(
    "problem_solving",
    problem="A company's revenue increased by 25% in Q1, decreased by 15% in Q2, and increased by 30% in Q3. If the Q3 revenue was $169,000, what was the initial revenue?"
)

# For code debugging
debug_prompt = cot.generate_cot_prompt(
    "code_debugging",
    code="""def find_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)

result = find_average([])"""
)
```

### 3. Tree-of-Thought for Complex Decision Making

Tree-of-thought explores multiple reasoning paths and evaluates them.

```python
class TreeOfThoughtPrompt:
    def __init__(self, llm_client):
        self.client = llm_client
    
    async def generate_thoughts(self, problem: str, num_thoughts: int = 3) -> list:
        """Generate multiple initial thought paths"""
        prompt = """Problem: {problem}

Generate {num_thoughts} different approaches or initial thoughts for solving this problem. 
Format each as:
Thought X: [brief approach description]

Thoughts:""".format(problem=problem, num_thoughts=num_thoughts)
        
        response = await self.client.complete([
            {"role": "user", "content": prompt}
        ], temperature=0.8)
        
        # Parse thoughts from response
        content = response["choices"][0]["message"]["content"]
        thoughts = []
        
        for line in content.split('\n'):
            if line.strip().startswith('Thought'):
                thought = line.split(':', 1)[1].strip() if ':' in line else line.strip()
                thoughts.append(thought)
        
        return thoughts[:num_thoughts]
    
    async def evaluate_thought(self, problem: str, thought: str) -> float:
        """Evaluate the quality/feasibility of a thought"""
        eval_prompt = """Problem: {problem}

Proposed approach: {thought}

Evaluate this approach on a scale of 1-10 considering:
- Feasibility (can it actually work?)
- Efficiency (is it a good use of resources?)
- Completeness (does it address the full problem?)

Provide only a numeric score (1-10):""".format(problem=problem, thought=thought)
        
        response = await self.client.complete([
            {"role": "user", "content": eval_prompt}
        ], temperature=0.1, max_tokens=10)
        
        try:
            score = float(response["choices"][0]["message"]["content"].strip())
            return min(max(score, 1), 10)  # Clamp between 1-10
        except ValueError:
            return 5.0  # Default score if parsing fails
    
    async def expand_thought(self, problem: str, thought: str) -> str:
        """Expand a thought into detailed steps"""
        expand_prompt = """Problem: {problem}

Approach: {thought}

Expand this approach into detailed, actionable steps. Be specific and practical:

Detailed steps:""".format(problem=problem, thought=thought)
        
        response = await self.client.complete([
            {"role": "user", "content": expand_prompt}
        ], temperature=0.3)
        
        return response["choices"][0]["message"]["content"]
    
    async def solve_with_tot(self, problem: str) -> dict:
        """Solve a problem using tree-of-thought approach"""
        # Generate initial thoughts
        thoughts = await self.generate_thoughts(problem)
        
        # Evaluate each thought
        evaluations = []
        for thought in thoughts:
            score = await self.evaluate_thought(problem, thought)
            evaluations.append((thought, score))
        
        # Sort by score and select best thoughts
        evaluations.sort(key=lambda x: x[1], reverse=True)
        best_thoughts = evaluations[:2]  # Top 2 thoughts
        
        # Expand the best thoughts
        expanded_solutions = []
        for thought, score in best_thoughts:
            expanded = await self.expand_thought(problem, thought)
            expanded_solutions.append({
                "approach": thought,
                "score": score,
                "detailed_solution": expanded
            })
        
        return {
            "problem": problem,
            "all_thoughts": evaluations,
            "best_solutions": expanded_solutions
        }

# Usage example
async def main():
    # Assuming you have an LLM client
    tot = TreeOfThoughtPrompt(llm_client)
    
    result = await tot.solve_with_tot(
        "Design a system to handle 1 million concurrent users for a social media platform"
    )
    
    print("Best Solutions:")
    for i, solution in enumerate(result["best_solutions"], 1):
        print("Solution " + str(i) + " (Score: " + str(solution["score"]) + "):")
        print(solution["approach"])
        print(solution["detailed_solution"])
        print("-" * 50)
```

## Building Production-Ready RAG Systems

### 1. RAG Architecture and Components

```python
import numpy as np
from typing import List, Dict, Any, Optional
import chromadb
from sentence_transformers import SentenceTransformer
import asyncio

class DocumentChunker:
    def __init__(self, chunk_size: int = 1000, overlap: int = 200):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_text(self, text: str, metadata: dict = None) -> List[dict]:
        """Split text into overlapping chunks"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), self.chunk_size - self.overlap):
            chunk_words = words[i:i + self.chunk_size]
            chunk_text = ' '.join(chunk_words)
            
            chunk_metadata = {
                "chunk_index": len(chunks),
                "start_word": i,
                "end_word": i + len(chunk_words),
                **(metadata or {})
            }
            
            chunks.append({
                "content": chunk_text,
                "metadata": chunk_metadata
            })
        
        return chunks
    
    def semantic_chunking(self, text: str, encoder, similarity_threshold: float = 0.8) -> List[dict]:
        """Chunk text based on semantic similarity"""
        sentences = text.split('. ')
        if len(sentences) < 2:
            return [{"content": text, "metadata": {"chunk_index": 0}}]
        
        # Encode sentences
        embeddings = encoder.encode(sentences)
        
        chunks = []
        current_chunk = [sentences[0]]
        
        for i in range(1, len(sentences)):
            # Calculate similarity with current chunk
            current_embedding = np.mean([embeddings[j] for j in range(len(current_chunk))], axis=0)
            similarity = np.dot(current_embedding, embeddings[i]) / (
                np.linalg.norm(current_embedding) * np.linalg.norm(embeddings[i])
            )
            
            if similarity > similarity_threshold and len(' '.join(current_chunk)) < self.chunk_size:
                current_chunk.append(sentences[i])
            else:
                # Finalize current chunk and start new one
                chunks.append({
                    "content": '. '.join(current_chunk),
                    "metadata": {"chunk_index": len(chunks)}
                })
                current_chunk = [sentences[i]]
        
        # Add final chunk
        if current_chunk:
            chunks.append({
                "content": '. '.join(current_chunk),
                "metadata": {"chunk_index": len(chunks)}
            })
        
        return chunks

class VectorStore:
    def __init__(self, collection_name: str = "documents"):
        self.client = chromadb.Client()
        self.collection = self.client.create_collection(collection_name)
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def add_documents(self, documents: List[dict]):
        """Add documents to the vector store"""
        contents = [doc["content"] for doc in documents]
        metadatas = [doc["metadata"] for doc in documents]
        ids = [str(i) for i in range(len(documents))]
        
        # Generate embeddings
        embeddings = self.encoder.encode(contents).tolist()
        
        self.collection.add(
            embeddings=embeddings,
            documents=contents,
            metadatas=metadatas,
            ids=ids
        )
    
    def search(self, query: str, top_k: int = 5) -> List[dict]:
        """Search for relevant documents"""
        query_embedding = self.encoder.encode([query]).tolist()
        
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )
        
        documents = []
        for i in range(len(results["documents"][0])):
            documents.append({
                "content": results["documents"][0][i],
                "metadata": results["metadatas"][0][i],
                "distance": results["distances"][0][i]
            })
        
        return documents

class RAGSystem:
    def __init__(self, llm_client, vector_store: VectorStore):
        self.llm_client = llm_client
        self.vector_store = vector_store
        self.chunker = DocumentChunker()
    
    def ingest_document(self, content: str, metadata: dict = None):
        """Ingest a document into the RAG system"""
        chunks = self.chunker.chunk_text(content, metadata)
        self.vector_store.add_documents(chunks)
    
    async def retrieve_and_generate(
        self, 
        query: str, 
        top_k: int = 5,
        system_prompt: str = None
    ) -> dict:
        """Retrieve relevant documents and generate response"""
        
        # Retrieve relevant documents
        relevant_docs = self.vector_store.search(query, top_k=top_k)
        
        # Build context from retrieved documents
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            context_parts.append("Document " + str(i) + ":")
            context_parts.append(doc["content"])
            context_parts.append("")
        
        context = "\n".join(context_parts)
        
        # Build RAG prompt
        default_system = """You are a helpful assistant that answers questions based on the provided context. 
Use only the information from the context to answer questions. If the answer cannot be found in the context, say so clearly."""
        
        system_message = system_prompt or default_system
        
        user_prompt = """Context:
{context}

Question: {query}

Please provide a detailed answer based on the context above:""".format(
            context=context,
            query=query
        )
        
        # Generate response
        response = await self.llm_client.complete([
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_prompt}
        ])
        
        return {
            "query": query,
            "answer": response["choices"][0]["message"]["content"],
            "sources": relevant_docs,
            "context_used": context
        }
    
    async def conversational_rag(
        self, 
        query: str, 
        conversation_history: List[dict],
        top_k: int = 5
    ) -> dict:
        """RAG with conversation history"""
        
        # Create a comprehensive query including conversation context
        history_context = ""
        if conversation_history:
            recent_history = conversation_history[-3:]  # Last 3 exchanges
            history_parts = []
            for exchange in recent_history:
                if exchange["role"] == "user":
                    history_parts.append("User: " + exchange["content"])
                elif exchange["role"] == "assistant":
                    history_parts.append("Assistant: " + exchange["content"])
            
            history_context = "\n".join(history_parts)
        
        # Enhanced query for better retrieval
        enhanced_query = query
        if history_context:
            enhanced_query = "Previous conversation:\n" + history_context + "\n\nCurrent question: " + query
        
        # Use the enhanced query for retrieval
        relevant_docs = self.vector_store.search(enhanced_query, top_k=top_k)
        
        # Build context
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            context_parts.append("Document " + str(i) + ":")
            context_parts.append(doc["content"])
            context_parts.append("")
        
        context = "\n".join(context_parts)
        
        # Build conversational RAG prompt
        messages = [
            {
                "role": "system", 
                "content": """You are a helpful assistant that answers questions based on provided context and conversation history. 
Use the context and previous conversation to provide coherent, contextual responses."""
            }
        ]
        
        # Add conversation history
        messages.extend(conversation_history[-5:])  # Last 5 messages
        
        # Add current query with context
        current_prompt = """Context:
{context}

Question: {query}

Answer:""".format(context=context, query=query)
        
        messages.append({"role": "user", "content": current_prompt})
        
        response = await self.llm_client.complete(messages)
        
        return {
            "query": query,
            "answer": response["choices"][0]["message"]["content"],
            "sources": relevant_docs,
            "enhanced_query": enhanced_query
        }

# Usage example
async def main():
    # Initialize components
    vector_store = VectorStore("technical_docs")
    rag_system = RAGSystem(llm_client, vector_store)
    
    # Ingest documents
    documents = [
        "Python is a high-level programming language known for its simplicity and readability...",
        "Machine learning is a subset of artificial intelligence that enables computers to learn...",
        "REST APIs are architectural style for designing networked applications..."
    ]
    
    for doc in documents:
        rag_system.ingest_document(doc, {"source": "technical_guide"})
    
    # Query the system
    result = await rag_system.retrieve_and_generate(
        "What are the benefits of Python programming?",
        top_k=3
    )
    
    print("Answer:", result["answer"])
    print("Sources used:", len(result["sources"]))
```

### 2. Advanced RAG Techniques

#### Hybrid Search (Keyword + Semantic)

```python
from elasticsearch import Elasticsearch
import numpy as np

class HybridSearchRAG:
    def __init__(self, llm_client, es_host: str = "localhost:9200"):
        self.llm_client = llm_client
        self.es_client = Elasticsearch([es_host])
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.index_name = "hybrid_docs"
    
    def create_index(self):
        """Create Elasticsearch index with dense vector support"""
        mapping = {
            "mappings": {
                "properties": {
                    "content": {"type": "text"},
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 384  # all-MiniLM-L6-v2 dimension
                    },
                    "metadata": {"type": "object"}
                }
            }
        }
        
        if self.es_client.indices.exists(index=self.index_name):
            self.es_client.indices.delete(index=self.index_name)
        
        self.es_client.indices.create(index=self.index_name, body=mapping)
    
    def add_document(self, content: str, metadata: dict = None):
        """Add document with both text and vector representation"""
        embedding = self.encoder.encode(content).tolist()
        
        doc = {
            "content": content,
            "embedding": embedding,
            "metadata": metadata or {}
        }
        
        self.es_client.index(index=self.index_name, body=doc)
    
    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[dict]:
        """
        Perform hybrid search combining keyword and semantic search
        alpha: weight for semantic search (1-alpha for keyword search)
        """
        
        # Keyword search
        keyword_query = {
            "query": {
                "match": {
                    "content": query
                }
            },
            "size": top_k * 2  # Get more results for reranking
        }
        
        keyword_results = self.es_client.search(index=self.index_name, body=keyword_query)
        
        # Semantic search
        query_embedding = self.encoder.encode(query).tolist()
        semantic_query = {
            "query": {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": query_embedding}
                    }
                }
            },
            "size": top_k * 2
        }
        
        semantic_results = self.es_client.search(index=self.index_name, body=semantic_query)
        
        # Combine and rerank results
        combined_scores = {}
        
        # Add keyword scores
        for hit in keyword_results["hits"]["hits"]:
            doc_id = hit["_id"]
            keyword_score = hit["_score"]
            combined_scores[doc_id] = {
                "keyword_score": keyword_score,
                "semantic_score": 0,
                "doc": hit["_source"]
            }
        
        # Add semantic scores
        for hit in semantic_results["hits"]["hits"]:
            doc_id = hit["_id"]
            semantic_score = hit["_score"]
            
            if doc_id in combined_scores:
                combined_scores[doc_id]["semantic_score"] = semantic_score
            else:
                combined_scores[doc_id] = {
                    "keyword_score": 0,
                    "semantic_score": semantic_score,
                    "doc": hit["_source"]
                }
        
        # Calculate final scores and rank
        final_results = []
        for doc_id, scores in combined_scores.items():
            # Normalize scores (simple min-max normalization)
            keyword_normalized = scores["keyword_score"] / 10.0  # Adjust based on your data
            semantic_normalized = (scores["semantic_score"] - 1.0) / 1.0  # Cosine similarity range
            
            final_score = alpha * semantic_normalized + (1 - alpha) * keyword_normalized
            
            final_results.append({
                "content": scores["doc"]["content"],
                "metadata": scores["doc"]["metadata"],
                "final_score": final_score,
                "keyword_score": scores["keyword_score"],
                "semantic_score": scores["semantic_score"]
            })
        
        # Sort by final score and return top k
        final_results.sort(key=lambda x: x["final_score"], reverse=True)
        return final_results[:top_k]
    
    async def query_with_hybrid_search(self, query: str, top_k: int = 5) -> dict:
        """Query using hybrid search and generate response"""
        relevant_docs = self.hybrid_search(query, top_k)
        
        # Build context
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            context_parts.append("Document " + str(i) + " (Score: " + str(round(doc["final_score"], 3)) + "):")
            context_parts.append(doc["content"])
            context_parts.append("")
        
        context = "\n".join(context_parts)
        
        # Generate response
        prompt = """Context:
{context}

Question: {query}

Based on the context above, provide a comprehensive answer:""".format(
            context=context,
            query=query
        )
        
        response = await self.llm_client.complete([
            {"role": "user", "content": prompt}
        ])
        
        return {
            "query": query,
            "answer": response["choices"][0]["message"]["content"],
            "sources": relevant_docs
        }
```

#### Multi-Query RAG

```python
class MultiQueryRAG:
    def __init__(self, llm_client, vector_store: VectorStore):
        self.llm_client = llm_client
        self.vector_store = vector_store
    
    async def generate_query_variations(self, original_query: str, num_variations: int = 3) -> List[str]:
        """Generate variations of the original query for better retrieval"""
        prompt = """Given the following question, generate {num_variations} different ways to ask the same question. 
These variations should help retrieve more comprehensive information.

Original question: {query}

Generate {num_variations} question variations (one per line):""".format(
            query=original_query,
            num_variations=num_variations
        )
        
        response = await self.llm_client.complete([
            {"role": "user", "content": prompt}
        ], temperature=0.7)
        
        variations = []
        lines = response["choices"][0]["message"]["content"].strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('Original'):
                # Remove numbering if present
                if line[0].isdigit() and '.' in line[:3]:
                    line = line.split('.', 1)[1].strip()
                variations.append(line)
        
        return variations[:num_variations]
    
    async def multi_query_retrieve(
        self, 
        query: str, 
        num_variations: int = 3,
        docs_per_query: int = 3
    ) -> List[dict]:
        """Retrieve documents using multiple query variations"""
        
        # Generate query variations
        query_variations = await self.generate_query_variations(query, num_variations)
        all_queries = [query] + query_variations
        
        # Retrieve documents for each query
        all_docs = []
        seen_content = set()
        
        for q in all_queries:
            docs = self.vector_store.search(q, top_k=docs_per_query)
            
            for doc in docs:
                # Avoid duplicates based on content
                content_hash = hash(doc["content"])
                if content_hash not in seen_content:
                    doc["retrieved_by_query"] = q
                    all_docs.append(doc)
                    seen_content.add(content_hash)
        
        # Sort by relevance score and return top documents
        all_docs.sort(key=lambda x: x["distance"])
        return all_docs[:docs_per_query * len(all_queries)]
    
    async def answer_with_multi_query(self, query: str) -> dict:
        """Answer using multi-query RAG approach"""
        
        # Retrieve using multiple queries
        relevant_docs = await self.multi_query_retrieve(query)
        
        # Build enhanced context
        context_parts = []
        context_parts.append("Retrieved information from multiple search perspectives:")
        context_parts.append("")
        
        for i, doc in enumerate(relevant_docs, 1):
            context_parts.append("Source " + str(i) + " (found via: '" + doc["retrieved_by_query"] + "'):")
            context_parts.append(doc["content"])
            context_parts.append("")
        
        context = "\n".join(context_parts)
        
        # Generate comprehensive response
        prompt = """You have been provided with information retrieved using multiple search approaches for better coverage.

{context}

Original question: {query}

Provide a comprehensive answer that synthesizes information from all the sources:""".format(
            context=context,
            query=query
        )
        
        response = await self.llm_client.complete([
            {"role": "user", "content": prompt}
        ])
        
        return {
            "query": query,
            "answer": response["choices"][0]["message"]["content"],
            "sources": relevant_docs,
            "num_sources": len(relevant_docs)
        }
```

## Evaluation and Quality Assurance

### RAG Evaluation Framework

```python
class RAGEvaluator:
    def __init__(self, llm_client):
        self.llm_client = llm_client
    
    async def evaluate_relevance(self, query: str, retrieved_docs: List[dict]) -> List[float]:
        """Evaluate relevance of retrieved documents to the query"""
        relevance_scores = []
        
        for doc in retrieved_docs:
            prompt = """Evaluate how relevant this document is to the given query on a scale of 1-10.

Query: {query}

Document: {document}

Consider:
- Does the document contain information that helps answer the query?
- How directly related is the content to the query?
- Would this document be useful for someone trying to answer the query?

Provide only a numeric score (1-10):""".format(
                query=query,
                document=doc["content"]
            )
            
            response = await self.llm_client.complete([
                {"role": "user", "content": prompt}
            ], temperature=0.1, max_tokens=5)
            
            try:
                score = float(response["choices"][0]["message"]["content"].strip())
                relevance_scores.append(min(max(score, 1), 10))
            except ValueError:
                relevance_scores.append(5.0)  # Default score
        
        return relevance_scores
    
    async def evaluate_answer_quality(
        self, 
        query: str, 
        generated_answer: str, 
        ground_truth: str = None
    ) -> dict:
        """Evaluate the quality of the generated answer"""
        
        evaluation_criteria = [
            "Accuracy: Is the information factually correct?",
            "Completeness: Does it fully address the query?", 
            "Clarity: Is it easy to understand?",
            "Relevance: Does it stay focused on the query?"
        ]
        
        evaluation_results = {}
        
        for criterion in evaluation_criteria:
            prompt = """Evaluate the following answer based on this criterion: {criterion}

Query: {query}
Answer: {answer}

Rate on a scale of 1-10 and provide a brief explanation.

Format: Score: X/10
Explanation: [brief explanation]""".format(
                criterion=criterion,
                query=query,
                answer=generated_answer
            )
            
            response = await self.llm_client.complete([
                {"role": "user", "content": prompt}
            ], temperature=0.2)
            
            content = response["choices"][0]["message"]["content"]
            
            # Parse score and explanation
            score = 5.0  # default
            explanation = content
            
            if "Score:" in content:
                try:
                    score_line = [line for line in content.split('\n') if 'Score:' in line][0]
                    score = float(score_line.split('Score:')[1].split('/')[0].strip())
                except:
                    pass
            
            criterion_name = criterion.split(':')[0].lower()
            evaluation_results[criterion_name] = {
                "score": score,
                "explanation": explanation
            }
        
        # Calculate overall score
        overall_score = sum(result["score"] for result in evaluation_results.values()) / len(evaluation_results)
        evaluation_results["overall"] = {"score": overall_score}
        
        return evaluation_results
    
    async def evaluate_rag_system(
        self, 
        test_queries: List[dict],  # [{"query": "...", "expected_answer": "..."}]
        rag_system
    ) -> dict:
        """Comprehensive evaluation of RAG system"""
        
        results = {
            "total_queries": len(test_queries),
            "average_relevance": 0,
            "average_quality": 0,
            "detailed_results": []
        }
        
        total_relevance = 0
        total_quality = 0
        
        for test_case in test_queries:
            query = test_case["query"]
            expected = test_case.get("expected_answer", "")
            
            # Get RAG response
            rag_response = await rag_system.retrieve_and_generate(query)
            
            # Evaluate retrieval relevance
            relevance_scores = await self.evaluate_relevance(query, rag_response["sources"])
            avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
            
            # Evaluate answer quality
            quality_eval = await self.evaluate_answer_quality(
                query, 
                rag_response["answer"], 
                expected
            )
            
            result = {
                "query": query,
                "answer": rag_response["answer"],
                "relevance_score": avg_relevance,
                "quality_score": quality_eval["overall"]["score"],
                "sources_count": len(rag_response["sources"]),
                "detailed_quality": quality_eval
            }
            
            results["detailed_results"].append(result)
            total_relevance += avg_relevance
            total_quality += quality_eval["overall"]["score"]
        
        results["average_relevance"] = total_relevance / len(test_queries)
        results["average_quality"] = total_quality / len(test_queries)
        
        return results

# Usage example
async def main():
    evaluator = RAGEvaluator(llm_client)
    
    test_queries = [
        {
            "query": "What are the benefits of using Python for data science?",
            "expected_answer": "Python offers libraries like pandas, numpy, excellent community support..."
        },
        {
            "query": "How do you implement a REST API?",
            "expected_answer": "REST APIs can be implemented using frameworks like Flask, FastAPI..."
        }
    ]
    
    evaluation_results = await evaluator.evaluate_rag_system(test_queries, rag_system)
    
    print("Average Relevance Score:", evaluation_results["average_relevance"])
    print("Average Quality Score:", evaluation_results["average_quality"])
```

## Key Takeaways for Part 2

1. **Advanced Prompting**: Use few-shot, chain-of-thought, and tree-of-thought techniques for better results
2. **RAG Architecture**: Build robust retrieval systems with proper chunking and vector storage
3. **Hybrid Search**: Combine keyword and semantic search for better retrieval
4. **Multi-Query Approach**: Use query variations to capture more relevant information
5. **Evaluation is Critical**: Implement systematic evaluation for both retrieval and generation quality

## What's Next?

In **Part 3**, we'll focus on production deployment and scaling of LLM applications, covering infrastructure patterns, monitoring, security, and performance optimization strategies.

We'll cover:
- Infrastructure and deployment patterns
- Monitoring and observability for LLM applications
- Security, safety, and compliance considerations
- Scaling strategies and performance optimization
- Cost optimization and resource management

---

*This series provides practical, implementation-focused guidance for engineers building production LLM applications.*

