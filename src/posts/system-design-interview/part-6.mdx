# Design a Distributed Cache (Redis)

In this final part, we'll design a distributed cache system like Redis or Memcached. This introduces fundamental concepts of distributed systems including data consistency, partitioning, replication, and coordination.

## 1. Functional Requirements

### Actors
- **Application Client**: Reads and writes cache data
- **Cache Administrator**: Monitors and manages cache cluster
- **System**: Handles replication and failover

### Use Cases

**Application Client**:
- Store key-value pairs with TTL
- Retrieve values by key
- Delete specific keys
- Perform atomic operations (increment, append)
- Execute batch operations
- Subscribe to key events

**Cache Administrator**:
- Monitor cluster health and performance
- Add/remove nodes from cluster
- Configure replication settings
- Manage memory usage and eviction policies

**System Functions**:
- Automatic failover and recovery
- Data replication across nodes
- Load balancing and sharding
- Memory management and eviction

### Functional Requirements
✅ **In Scope**:
- Basic operations (GET, SET, DELETE)
- TTL (Time To Live) support
- Data partitioning across nodes
- Replication for high availability
- Atomic operations and transactions
- Pub/Sub messaging
- Memory management and eviction
- Cluster management

❌ **Out of Scope**:
- Complex data structures (sorted sets, streams)
- Persistence to disk
- Advanced scripting (Lua scripts)
- Advanced security features
- Cross-datacenter replication

## 2. Non-Functional Requirements

### Scalability
- Support thousands of nodes in a cluster
- Handle millions of operations per second
- Linear scaling with node addition
- Support for multiple data centers

### Availability
- 99.99% uptime
- Automatic failover <30 seconds
- No single point of failure
- Graceful degradation during failures

### Performance
- Sub-millisecond latency for cache hits
- Support 100K+ ops/sec per node
- Efficient memory utilization (>90%)
- Minimal network overhead

### Consistency
- Strong consistency within partition
- Eventual consistency across replicas
- Configurable consistency levels
- Conflict resolution mechanisms

## 3. Estimations

### Usage Metrics
- **Cache Cluster Size**: 100 nodes
- **Memory per Node**: 64 GB
- **Total Cache Capacity**: 6.4 TB
- **Operations per Second**: 10 million

### Performance Metrics
- **Average Key Size**: 100 bytes
- **Average Value Size**: 1 KB
- **Cache Hit Ratio**: 95%
- **Network Bandwidth**: 10 Gbps per node

### Memory Estimations

**Per Node Storage**:
- Available Memory: 64 GB
- OS and Overhead: 4 GB
- Cache Data: 60 GB
- **Effective Storage**: ~50 million key-value pairs per node

**Cluster Totals**:
- **Total Effective Storage**: 5 billion key-value pairs
- **Memory Efficiency**: 90% (accounting for fragmentation)
- **Replication Factor**: 3x for high availability

## 4. Design Goals

### Performance Requirements
- **Latency**: <1ms for local operations
- **Throughput**: 100K ops/sec per node
- **Memory Efficiency**: >90% utilization
- **Network Efficiency**: Minimal cross-node communication

### Architecture Patterns
- **Consistent Hashing**: For data partitioning
- **Master-Slave Replication**: For data consistency
- **Gossip Protocol**: For cluster coordination

### Usage Patterns
- **Read Heavy**: 80% reads, 20% writes
- **Hot Keys**: Power-law distribution of key access
- **TTL Patterns**: Mix of short and long-lived data

## 5. High-Level Design

### Building Blocks

```
[Client] → [Smart Client/Proxy] → [Cache Node 1] ← [Replica 1A]
                    ↓                     ↓              ↓
                [Cache Node 2] ← [Replica 2A] ← [Coordinator]
                    ↓                     ↓              ↓
                [Cache Node 3] ← [Replica 3A] ← [Gossip Network]
```

### Core Components

1. **Cache Nodes**: Store actual key-value data
2. **Cluster Coordinator**: Manages cluster membership
3. **Smart Client**: Routes requests to correct nodes
4. **Replication Manager**: Handles data replication
5. **Gossip Protocol**: Disseminates cluster state
6. **Memory Manager**: Handles eviction and garbage collection

### Data Distribution Strategy

**Consistent Hashing**:
```python
class ConsistentHashing:
    def __init__(self, nodes, virtual_nodes=150):
        self.virtual_nodes = virtual_nodes
        self.ring = {}
        self.sorted_keys = []
        
        for node in nodes:
            self.add_node(node)
    
    def hash(self, key):
        return hashlib.md5(key.encode()).hexdigest()
    
    def add_node(self, node):
        for i in range(self.virtual_nodes):
            virtual_key = self.hash(f"{node}:{i}")
            self.ring[virtual_key] = node
        
        self.sorted_keys = sorted(self.ring.keys())
    
    def get_node(self, key):
        if not self.ring:
            return None
            
        hash_key = self.hash(key)
        
        # Find the first node clockwise
        for ring_key in self.sorted_keys:
            if hash_key <= ring_key:
                return self.ring[ring_key]
        
        # Wrap around to the first node
        return self.ring[self.sorted_keys[0]]
    
    def remove_node(self, node):
        for i in range(self.virtual_nodes):
            virtual_key = self.hash(f"{node}:{i}")
            if virtual_key in self.ring:
                del self.ring[virtual_key]
        
        self.sorted_keys = sorted(self.ring.keys())
```

## Cache Node Implementation

### Core Cache Operations

```python
class CacheNode:
    def __init__(self, node_id, max_memory=64*1024*1024*1024):  # 64GB
        self.node_id = node_id
        self.max_memory = max_memory
        self.data = {}
        self.ttl_data = {}
        self.access_times = {}
        self.memory_usage = 0
        self.eviction_policy = LRUEvictionPolicy()
        
    def get(self, key):
        # Check if key exists and not expired
        if key not in self.data:
            return None
            
        if self.is_expired(key):
            self.delete(key)
            return None
        
        # Update access time for LRU
        self.access_times[key] = time.time()
        
        return self.data[key]
    
    def set(self, key, value, ttl=None):
        # Check memory constraints
        value_size = self.calculate_size(value)
        
        if key in self.data:
            # Update existing key
            old_size = self.calculate_size(self.data[key])
            self.memory_usage += (value_size - old_size)
        else:
            # New key
            self.memory_usage += value_size + self.calculate_size(key)
        
        # Evict if necessary
        while self.memory_usage > self.max_memory:
            evicted_key = self.eviction_policy.evict(self.data, self.access_times)
            if evicted_key:
                self.delete(evicted_key)
            else:
                break  # No more keys to evict
        
        # Store the data
        self.data[key] = value
        self.access_times[key] = time.time()
        
        # Set TTL if provided
        if ttl:
            self.ttl_data[key] = time.time() + ttl
    
    def delete(self, key):
        if key in self.data:
            value_size = self.calculate_size(self.data[key])
            key_size = self.calculate_size(key)
            
            del self.data[key]
            del self.access_times[key]
            
            if key in self.ttl_data:
                del self.ttl_data[key]
            
            self.memory_usage -= (value_size + key_size)
            return True
        
        return False
    
    def is_expired(self, key):
        if key not in self.ttl_data:
            return False
        
        return time.time() > self.ttl_data[key]
    
    def cleanup_expired_keys(self):
        """Background task to clean up expired keys"""
        current_time = time.time()
        expired_keys = []
        
        for key, expiry_time in self.ttl_data.items():
            if current_time > expiry_time:
                expired_keys.append(key)
        
        for key in expired_keys:
            self.delete(key)
```

### Eviction Policies

```python
class LRUEvictionPolicy:
    def evict(self, data, access_times):
        if not access_times:
            return None
        
        # Find least recently used key
        lru_key = min(access_times.keys(), key=lambda k: access_times[k])
        return lru_key

class LFUEvictionPolicy:
    def __init__(self):
        self.access_counts = {}
    
    def evict(self, data, access_times):
        if not self.access_counts:
            return None
        
        # Find least frequently used key
        lfu_key = min(self.access_counts.keys(), key=lambda k: self.access_counts[k])
        return lfu_key
    
    def on_access(self, key):
        self.access_counts[key] = self.access_counts.get(key, 0) + 1

class TTLEvictionPolicy:
    def evict(self, data, access_times, ttl_data):
        # Prioritize expired keys
        current_time = time.time()
        
        for key, expiry_time in ttl_data.items():
            if current_time > expiry_time:
                return key
        
        # If no expired keys, fall back to LRU
        return LRUEvictionPolicy().evict(data, access_times)
```

## Replication and Consistency

### Master-Slave Replication

```python
class ReplicationManager:
    def __init__(self, node_id, replication_factor=3):
        self.node_id = node_id
        self.replication_factor = replication_factor
        self.replicas = set()
        self.masters = set()
        
    def add_replica(self, replica_node):
        self.replicas.add(replica_node)
    
    def replicate_write(self, key, value, ttl=None):
        """Replicate write operation to all replicas"""
        operation = {
            "type": "SET",
            "key": key,
            "value": value,
            "ttl": ttl,
            "timestamp": time.time(),
            "node_id": self.node_id
        }
        
        # Synchronous replication to ensure consistency
        successful_replications = 0
        
        for replica in self.replicas:
            try:
                result = replica.apply_operation(operation)
                if result:
                    successful_replications += 1
            except Exception as e:
                # Log replication failure
                self.log_replication_error(replica, operation, str(e))
        
        # Require majority for success (quorum)
        required_replications = (self.replication_factor + 1) // 2
        
        if successful_replications >= required_replications:
            return True
        else:
            # Rollback operation if quorum not reached
            self.rollback_operation(operation)
            return False
    
    def handle_failover(self, failed_node):
        """Handle node failure and promote replica"""
        if failed_node in self.masters:
            # Promote a replica to master
            replica_to_promote = self.select_replica_for_promotion(failed_node)
            if replica_to_promote:
                self.promote_replica_to_master(replica_to_promote, failed_node)
        
        # Update routing tables
        self.update_cluster_topology(failed_node, "FAILED")
```

### Conflict Resolution

```python
class ConflictResolver:
    def resolve_write_conflict(self, operations):
        """Resolve conflicts using last-write-wins with vector clocks"""
        
        if len(operations) == 1:
            return operations[0]
        
        # Sort by timestamp (last write wins)
        sorted_ops = sorted(operations, key=lambda op: op['timestamp'])
        latest_operation = sorted_ops[-1]
        
        # For concurrent writes (same timestamp), use node_id as tiebreaker
        concurrent_ops = [op for op in sorted_ops if op['timestamp'] == latest_operation['timestamp']]
        
        if len(concurrent_ops) > 1:
            # Use lexicographic ordering of node_id
            latest_operation = min(concurrent_ops, key=lambda op: op['node_id'])
        
        return latest_operation
    
    def detect_concurrent_writes(self, operation1, operation2):
        """Detect if two operations are concurrent using vector clocks"""
        
        # Simple timestamp-based detection
        time_diff = abs(operation1['timestamp'] - operation2['timestamp'])
        
        # Consider operations concurrent if within 100ms
        return time_diff < 0.1
```

## Cluster Management

### Gossip Protocol for Cluster Coordination

```python
class GossipProtocol:
    def __init__(self, node_id, initial_nodes):
        self.node_id = node_id
        self.cluster_state = {}
        self.heartbeat_interval = 1  # 1 second
        self.failure_detection_timeout = 5  # 5 seconds
        
        # Initialize cluster state
        for node in initial_nodes:
            self.cluster_state[node] = {
                "status": "ALIVE",
                "last_seen": time.time(),
                "metadata": {}
            }
    
    def start_gossip(self):
        """Start gossip protocol background tasks"""
        threading.Thread(target=self.gossip_loop, daemon=True).start()
        threading.Thread(target=self.failure_detection_loop, daemon=True).start()
    
    def gossip_loop(self):
        """Periodically gossip cluster state with random nodes"""
        while True:
            try:
                # Select random subset of nodes to gossip with
                alive_nodes = [node for node, state in self.cluster_state.items() 
                              if state["status"] == "ALIVE" and node != self.node_id]
                
                if alive_nodes:
                    random_nodes = random.sample(alive_nodes, min(3, len(alive_nodes)))
                    
                    for node in random_nodes:
                        self.send_gossip_message(node)
                
                time.sleep(self.heartbeat_interval)
                
            except Exception as e:
                self.log_error(f"Gossip loop error: {e}")
    
    def send_gossip_message(self, target_node):
        """Send gossip message to target node"""
        message = {
            "type": "GOSSIP",
            "sender": self.node_id,
            "cluster_state": self.cluster_state,
            "timestamp": time.time()
        }
        
        try:
            response = self.send_message(target_node, message)
            if response:
                self.merge_cluster_state(response["cluster_state"])
        except Exception as e:
            # Mark node as potentially failed
            self.mark_node_suspect(target_node)
    
    def merge_cluster_state(self, remote_state):
        """Merge remote cluster state with local state"""
        for node, remote_info in remote_state.items():
            if node not in self.cluster_state:
                # New node discovered
                self.cluster_state[node] = remote_info
            else:
                # Update if remote info is newer
                local_info = self.cluster_state[node]
                if remote_info["last_seen"] > local_info["last_seen"]:
                    self.cluster_state[node] = remote_info
    
    def failure_detection_loop(self):
        """Detect failed nodes based on heartbeat timeouts"""
        while True:
            current_time = time.time()
            
            for node, state in self.cluster_state.items():
                if node == self.node_id:
                    continue
                
                time_since_seen = current_time - state["last_seen"]
                
                if (time_since_seen > self.failure_detection_timeout and 
                    state["status"] == "ALIVE"):
                    
                    self.mark_node_failed(node)
            
            time.sleep(self.heartbeat_interval)
    
    def mark_node_failed(self, node):
        """Mark node as failed and trigger failover"""
        self.cluster_state[node]["status"] = "FAILED"
        self.cluster_state[node]["last_seen"] = time.time()
        
        # Notify cluster about node failure
        self.broadcast_node_failure(node)
        
        # Trigger rebalancing if necessary
        self.trigger_rebalancing(node)
```

## Client Implementation

### Smart Client with Connection Pooling

```python
class CacheClient:
    def __init__(self, cluster_nodes, pool_size=10):
        self.cluster_nodes = cluster_nodes
        self.consistent_hash = ConsistentHashing(cluster_nodes)
        self.connection_pools = {}
        
        # Create connection pools for each node
        for node in cluster_nodes:
            self.connection_pools[node] = ConnectionPool(node, pool_size)
    
    def get(self, key):
        """Get value for key with automatic retry and failover"""
        target_node = self.consistent_hash.get_node(key)
        replica_nodes = self.get_replica_nodes(key)
        
        # Try primary node first
        try:
            return self.execute_on_node(target_node, "GET", key)
        except NodeUnavailableException:
            # Try replica nodes
            for replica in replica_nodes:
                try:
                    return self.execute_on_node(replica, "GET", key)
                except NodeUnavailableException:
                    continue
            
            raise CacheUnavailableException(f"All nodes unavailable for key: {key}")
    
    def set(self, key, value, ttl=None):
        """Set key-value with replication"""
        target_node = self.consistent_hash.get_node(key)
        replica_nodes = self.get_replica_nodes(key)
        
        # Write to primary node
        success = self.execute_on_node(target_node, "SET", key, value, ttl)
        
        if success:
            # Asynchronously replicate to replicas
            self.async_replicate(replica_nodes, "SET", key, value, ttl)
        
        return success
    
    def execute_on_node(self, node, operation, *args):
        """Execute operation on specific node"""
        connection = self.connection_pools[node].get_connection()
        
        try:
            if operation == "GET":
                return connection.get(args[0])
            elif operation == "SET":
                return connection.set(args[0], args[1], args[2] if len(args) > 2 else None)
            elif operation == "DELETE":
                return connection.delete(args[0])
        finally:
            self.connection_pools[node].return_connection(connection)
    
    def get_replica_nodes(self, key):
        """Get replica nodes for a given key"""
        primary_node = self.consistent_hash.get_node(key)
        
        # Get next N nodes in the ring as replicas
        replicas = []
        nodes = list(self.cluster_nodes)
        primary_index = nodes.index(primary_node)
        
        for i in range(1, 4):  # 3 replicas
            replica_index = (primary_index + i) % len(nodes)
            replicas.append(nodes[replica_index])
        
        return replicas
```

## Performance Monitoring

### Metrics Collection

```python
class CacheMetrics:
    def __init__(self):
        self.hit_count = 0
        self.miss_count = 0
        self.operation_latencies = []
        self.memory_usage = 0
        self.eviction_count = 0
        
    def record_hit(self):
        self.hit_count += 1
    
    def record_miss(self):
        self.miss_count += 1
    
    def record_latency(self, operation, latency_ms):
        self.operation_latencies.append({
            "operation": operation,
            "latency": latency_ms,
            "timestamp": time.time()
        })
        
        # Keep only last 1000 measurements
        if len(self.operation_latencies) > 1000:
            self.operation_latencies = self.operation_latencies[-1000:]
    
    def get_hit_ratio(self):
        total_requests = self.hit_count + self.miss_count
        if total_requests == 0:
            return 0
        return self.hit_count / total_requests
    
    def get_average_latency(self, operation=None):
        if operation:
            latencies = [l["latency"] for l in self.operation_latencies if l["operation"] == operation]
        else:
            latencies = [l["latency"] for l in self.operation_latencies]
        
        if not latencies:
            return 0
        
        return sum(latencies) / len(latencies)
    
    def get_p99_latency(self, operation=None):
        if operation:
            latencies = [l["latency"] for l in self.operation_latencies if l["operation"] == operation]
        else:
            latencies = [l["latency"] for l in self.operation_latencies]
        
        if not latencies:
            return 0
        
        sorted_latencies = sorted(latencies)
        p99_index = int(0.99 * len(sorted_latencies))
        return sorted_latencies[p99_index]
```

## Distributed Cache Design Quiz

Test your understanding of distributed cache system design with the interactive quiz that appears after each part of this series.

## Advanced Features

### Pub/Sub Implementation

```python
class PubSubManager:
    def __init__(self):
        self.subscriptions = {}  # channel -> set of subscribers
        self.pattern_subscriptions = {}  # pattern -> set of subscribers
        
    def subscribe(self, client_id, channel):
        if channel not in self.subscriptions:
            self.subscriptions[channel] = set()
        self.subscriptions[channel].add(client_id)
    
    def unsubscribe(self, client_id, channel):
        if channel in self.subscriptions:
            self.subscriptions[channel].discard(client_id)
    
    def publish(self, channel, message):
        # Direct channel subscribers
        if channel in self.subscriptions:
            for subscriber in self.subscriptions[channel]:
                self.send_message_to_client(subscriber, channel, message)
        
        # Pattern subscribers
        for pattern, subscribers in self.pattern_subscriptions.items():
            if self.matches_pattern(channel, pattern):
                for subscriber in subscribers:
                    self.send_message_to_client(subscriber, channel, message)
```

### Memory Management

```python
class MemoryManager:
    def __init__(self, max_memory):
        self.max_memory = max_memory
        self.current_usage = 0
        self.fragmentation_threshold = 0.1
        
    def should_evict(self):
        return self.current_usage > self.max_memory * 0.9
    
    def calculate_fragmentation(self):
        # Simplified fragmentation calculation
        allocated_memory = sum(sys.getsizeof(obj) for obj in self.data.values())
        return 1 - (allocated_memory / self.current_usage)
    
    def defragment_memory(self):
        if self.calculate_fragmentation() > self.fragmentation_threshold:
            # Trigger garbage collection
            gc.collect()
            
            # Reorganize data structure if needed
            self.reorganize_data_structures()
```

## Key Takeaways

1. **Consistent Hashing**: Essential for distributed data partitioning
2. **Replication Strategy**: Balance consistency, availability, and performance
3. **Failure Detection**: Use gossip protocols for robust cluster management
4. **Smart Clients**: Implement client-side logic for routing and failover
5. **Memory Management**: Efficient eviction policies and memory monitoring

## Series Conclusion

Congratulations! You've completed the System Design Mastery series. You've learned to design:

1. **URL Shortener**: Read-heavy systems with caching
2. **Chat System**: Real-time communication and WebSockets
3. **Social Media Feed**: Content ranking and viral traffic handling
4. **Video Streaming**: Large file storage and global CDN
5. **Distributed Cache**: Consistency and distributed coordination

### Final Interview Tips

1. **Practice Regularly**: Work through problems weekly
2. **Think Out Loud**: Communicate your reasoning clearly
3. **Start Simple**: Begin with basic design, then add complexity
4. **Consider Trade-offs**: Discuss pros and cons of each decision
5. **Learn from Real Systems**: Study how companies like Google, Facebook, and Netflix solve similar problems

### Continue Learning

- Study real-world system architectures
- Read engineering blogs from top tech companies
- Practice with system design interview platforms
- Build distributed systems to gain hands-on experience
- Stay current with emerging technologies and patterns

**You're now ready to tackle any system design interview with confidence!**

---

**Previous:** [Part 5: Design a Video Streaming Service (YouTube)](/posts/system-design-interview/part-5)  
**Series Complete!** Return to [Part 1: Introduction & Methodology](/posts/system-design-interview) to review concepts.
