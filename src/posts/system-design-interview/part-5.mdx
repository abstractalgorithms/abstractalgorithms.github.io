# Design a Video Streaming Service (YouTube)

In this part, we'll design a video streaming service like YouTube or Netflix. This introduces unique challenges around large file storage, content delivery networks, video processing, and global content distribution.

## 1. Functional Requirements

### Actors
- **Content Creator**: Uploads and manages videos
- **Viewer**: Watches and interacts with videos
- **Content Moderator**: Reviews flagged content
- **System**: Handles video processing and recommendations

### Use Cases

**Content Creator**:
- Upload videos (various formats, up to 4K resolution)
- Add metadata (title, description, thumbnails, tags)
- View analytics (views, engagement, revenue)
- Manage video settings (privacy, monetization)
- Live streaming capability

**Viewer**:
- Search and browse videos
- Watch videos with adaptive quality
- Like, comment, share videos
- Subscribe to channels
- Create and manage playlists
- View personalized recommendations

**Content Moderator**:
- Review flagged content
- Apply community guidelines
- Manage copyright claims

### Functional Requirements
✅ **In Scope**:
- Video upload and storage
- Video transcoding (multiple resolutions)
- Video playback with adaptive streaming
- Search and discovery
- User engagement (likes, comments, subscriptions)
- Basic recommendation system
- Analytics and metrics

❌ **Out of Scope**:
- Advanced recommendation algorithms (ML-based)
- Monetization and ad serving
- Live streaming infrastructure
- Advanced content moderation
- Content creator studio tools

## 2. Non-Functional Requirements

### Scalability
- Support 2 billion users globally
- Handle 500 hours of video uploaded per minute
- Support 1 billion hours watched per day
- Handle traffic spikes during viral events

### Availability
- 99.9% uptime for video playback
- 99.5% uptime for video uploads
- Global content distribution
- Graceful degradation during failures

### Performance
- Video start time: &lt;2 seconds globally
- Upload processing: &lt;30 minutes for 1-hour video
- Search results: &lt;300ms
- Support 4K streaming with &lt;1% rebuffering

### Storage & Bandwidth
- Petabyte-scale storage requirements
- Multi-region content replication
- Intelligent content placement
- Bandwidth optimization

## 3. Estimations

### User Metrics
- **Total Users**: 2 billion
- **Daily Active Users**: 500 million
- **Average watch time per user**: 40 minutes/day
- **Concurrent viewers**: 50 million peak

### Video Metrics
- **Videos uploaded per day**: 720,000 (500 hours/min × 60 min/hour × 24 hours)
- **Video views per day**: 5 billion
- **Average video length**: 10 minutes
- **Video upload formats**: 90% mobile (1080p), 10% professional (4K)

### Storage Estimations

**Per Video Storage** (Multiple Resolutions):
- Original file: 1 GB (10 min @ 4K)
- 1080p: 400 MB
- 720p: 200 MB  
- 480p: 100 MB
- 360p: 50 MB
- Thumbnails: 1 MB
- **Total per video**: ~1.75 GB

**Storage Growth**:
- **Per Day**: 720K videos × 1.75 GB = 1.26 PB/day
- **Per Year**: 1.26 PB × 365 = 460 PB/year
- **With Replication (3x)**: 1.38 EB/year

### Bandwidth Estimations
- **Average bitrate**: 2 Mbps (adaptive streaming)
- **Concurrent viewers**: 50M × 2 Mbps = 100 Tbps
- **Daily bandwidth**: 50M × 2 Mbps × 40 min = 400 TB/day

## 4. Design Goals

### Performance Requirements
- **Video Start Time**: &lt;2 seconds globally
- **Buffering**: &lt;1% rebuffering ratio
- **Upload Speed**: Support simultaneous uploads
- **Search Latency**: &lt;300ms

### Architecture Patterns
- **Microservices**: Decomposed by functionality
- **Event-Driven**: Video processing workflows
- **CQRS**: Separate read/write models for metadata

### Usage Patterns
- **Read Heavy**: 100:1 read to write ratio
- **Large Files**: Multi-GB video files
- **Global Distribution**: Viewers worldwide
- **Batch Processing**: Video encoding workflows

## 5. High-Level Design

### Building Blocks

```
[Client] → [CDN] → [Load Balancer] → [API Gateway]
                           ↓              ↓
                   [Video Service] → [Upload Service]
                           ↓              ↓
                   [Metadata DB] ← [Video Processing]
                           ↓              ↓
                   [Search Service] → [Blob Storage]
                           ↓              ↓
                   [Analytics] ← [Recommendation Service]
```

### Core Components

1. **CDN**: Global content delivery network
2. **Upload Service**: Handles video file uploads
3. **Video Processing**: Transcoding and optimization
4. **Metadata Service**: Video information and user data
5. **Search Service**: Video discovery and search
6. **Streaming Service**: Adaptive video delivery
7. **Analytics Service**: View tracking and metrics

### API Design

**Upload Video**:
```http
POST /api/v1/videos/upload
Authorization: Bearer {token}
Content-Type: multipart/form-data

{
  "title": "Amazing Travel Video",
  "description": "My trip to Iceland",
  "tags": ["travel", "iceland", "nature"],
  "category": "travel",
  "privacy": "public",
  "thumbnail": {file},
  "video_file": {file}
}

Response:
{
  "video_id": "abc123def456",
  "upload_url": "https://upload.example.com/abc123def456",
  "status": "processing",
  "estimated_completion": "2024-06-17T10:30:00Z"
}
```

**Get Video**:
```http
GET /api/v1/videos/{video_id}

Response:
{
  "video_id": "abc123def456",
  "title": "Amazing Travel Video",
  "description": "My trip to Iceland",
  "channel": {
    "channel_id": "channel_789",
    "name": "Travel Enthusiast",
    "subscriber_count": 15420
  },
  "duration": 600,
  "views": 12543,
  "likes": 892,
  "upload_date": "2024-06-17T10:00:00Z",
  "streaming_urls": {
    "4k": "https://cdn.example.com/abc123def456/4k.m3u8",
    "1080p": "https://cdn.example.com/abc123def456/1080p.m3u8",
    "720p": "https://cdn.example.com/abc123def456/720p.m3u8",
    "480p": "https://cdn.example.com/abc123def456/480p.m3u8"
  },
  "thumbnails": {
    "default": "https://cdn.example.com/abc123def456/thumb.jpg",
    "medium": "https://cdn.example.com/abc123def456/thumb_medium.jpg"
  }
}
```

**Search Videos**:
```http
GET /api/v1/search?q=travel iceland&limit=20&offset=0&sort=relevance

Response:
{
  "results": [
    {
      "video_id": "abc123def456",
      "title": "Amazing Travel Video",
      "thumbnail": "https://cdn.example.com/abc123def456/thumb.jpg",
      "duration": 600,
      "views": 12543,
      "channel_name": "Travel Enthusiast",
      "upload_date": "2024-06-17T10:00:00Z"
    }
  ],
  "total_results": 15420,
  "next_page_token": "eyJvZmZzZXQiOjIwfQ=="
}
```

### Database Schema

**Videos Table**:
```sql
CREATE TABLE videos (
    video_id VARCHAR(20) PRIMARY KEY,
    channel_id BIGINT NOT NULL,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    duration INT NOT NULL,
    category VARCHAR(50),
    privacy ENUM('public', 'unlisted', 'private') DEFAULT 'public',
    status ENUM('processing', 'ready', 'failed') DEFAULT 'processing',
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    view_count BIGINT DEFAULT 0,
    like_count INT DEFAULT 0,
    dislike_count INT DEFAULT 0,
    comment_count INT DEFAULT 0,
    
    INDEX idx_channel_date (channel_id, upload_date),
    INDEX idx_category_views (category, view_count),
    FULLTEXT idx_search (title, description)
);
```

**Video Files Table**:
```sql
CREATE TABLE video_files (
    video_id VARCHAR(20),
    resolution ENUM('4k', '1080p', '720p', '480p', '360p'),
    file_url VARCHAR(500) NOT NULL,
    file_size BIGINT NOT NULL,
    bitrate INT NOT NULL,
    codec VARCHAR(20) NOT NULL,
    
    PRIMARY KEY (video_id, resolution),
    FOREIGN KEY (video_id) REFERENCES videos(video_id)
);
```

**Channels Table**:
```sql
CREATE TABLE channels (
    channel_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    subscriber_count BIGINT DEFAULT 0,
    video_count INT DEFAULT 0,
    total_views BIGINT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_subscribers (subscriber_count),
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);
```

**View Events Table** (Time-series data):
```sql
CREATE TABLE view_events (
    event_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    video_id VARCHAR(20) NOT NULL,
    user_id BIGINT,
    session_id VARCHAR(50),
    watched_duration INT NOT NULL,
    total_duration INT NOT NULL,
    quality VARCHAR(10),
    device_type VARCHAR(20),
    geo_location VARCHAR(10),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_video_time (video_id, timestamp),
    INDEX idx_user_time (user_id, timestamp)
) PARTITION BY RANGE (UNIX_TIMESTAMP(timestamp));
```

## Video Processing Pipeline

### Upload and Processing Workflow

```python
class VideoProcessingPipeline:
    def __init__(self):
        self.upload_service = UploadService()
        self.transcoding_service = TranscodingService()
        self.storage_service = StorageService()
        self.metadata_service = MetadataService()
        
    def process_upload(self, video_file, metadata):
        # 1. Generate unique video ID
        video_id = self.generate_video_id()
        
        # 2. Upload original file to staging storage
        staging_url = self.upload_service.upload_to_staging(video_file, video_id)
        
        # 3. Extract video metadata
        video_info = self.extract_video_metadata(staging_url)
        
        # 4. Create database record
        self.metadata_service.create_video_record(video_id, metadata, video_info)
        
        # 5. Queue transcoding jobs
        self.queue_transcoding_jobs(video_id, staging_url, video_info)
        
        return {"video_id": video_id, "status": "processing"}
    
    def queue_transcoding_jobs(self, video_id, source_url, video_info):
        resolutions = self.determine_target_resolutions(video_info)
        
        for resolution in resolutions:
            job = {
                "video_id": video_id,
                "source_url": source_url,
                "target_resolution": resolution,
                "output_format": "mp4",
                "codec": "h264"
            }
            
            self.transcoding_queue.publish(job)
```

### Transcoding Service

```python
class TranscodingService:
    def __init__(self):
        self.ffmpeg = FFMpegWrapper()
        self.storage = StorageService()
        
    def transcode_video(self, job):
        video_id = job['video_id']
        source_url = job['source_url']
        resolution = job['target_resolution']
        
        try:
            # 1. Download source file
            local_source = self.download_source_file(source_url)
            
            # 2. Transcode to target resolution
            output_file = self.ffmpeg.transcode(
                input_file=local_source,
                resolution=resolution,
                codec=job['codec'],
                bitrate=self.get_target_bitrate(resolution)
            )
            
            # 3. Upload transcoded file to CDN
            cdn_url = self.storage.upload_to_cdn(output_file, video_id, resolution)
            
            # 4. Generate thumbnails
            thumbnails = self.generate_thumbnails(local_source, video_id)
            
            # 5. Update metadata with file URLs
            self.metadata_service.update_video_files(video_id, {
                "resolution": resolution,
                "file_url": cdn_url,
                "file_size": os.path.getsize(output_file),
                "bitrate": self.get_target_bitrate(resolution)
            })
            
            # 6. Cleanup temporary files
            self.cleanup_temp_files([local_source, output_file])
            
        except Exception as e:
            self.handle_transcoding_error(video_id, resolution, str(e))
    
    def get_target_bitrate(self, resolution):
        bitrates = {
            "4k": 20000,      # 20 Mbps
            "1080p": 8000,    # 8 Mbps
            "720p": 4000,     # 4 Mbps
            "480p": 2000,     # 2 Mbps
            "360p": 1000      # 1 Mbps
        }
        return bitrates.get(resolution, 2000)
```

## Content Delivery and Streaming

### CDN Architecture

```python
class CDNManager:
    def __init__(self):
        self.primary_regions = ["us-east", "us-west", "eu-west", "asia-pacific"]
        self.edge_locations = self.load_edge_locations()
        
    def upload_to_cdn(self, video_file, video_id, resolution):
        # 1. Upload to primary storage
        primary_url = self.upload_to_primary_storage(video_file, video_id, resolution)
        
        # 2. Replicate to major regions
        replication_jobs = []
        for region in self.primary_regions:
            job = {
                "source_url": primary_url,
                "target_region": region,
                "video_id": video_id,
                "resolution": resolution
            }
            replication_jobs.append(job)
        
        self.queue_replication_jobs(replication_jobs)
        
        return primary_url
    
    def get_optimal_cdn_url(self, video_id, resolution, user_location):
        # Find nearest CDN edge location
        nearest_edge = self.find_nearest_edge(user_location)
        
        # Check if content is available at edge
        edge_url = f"https://{nearest_edge}/videos/{video_id}/{resolution}.mp4"
        
        if self.check_content_availability(edge_url):
            return edge_url
        else:
            # Fallback to regional CDN
            regional_url = self.get_regional_url(video_id, resolution, user_location)
            
            # Trigger cache warming for future requests
            self.trigger_cache_warming(edge_url, regional_url)
            
            return regional_url
```

### Adaptive Streaming

```python
class AdaptiveStreamingService:
    def generate_hls_manifest(self, video_id, available_resolutions):
        """Generate HLS master playlist for adaptive streaming"""
        
        manifest = "#EXTM3U\n#EXT-X-VERSION:3\n\n"
        
        for resolution in available_resolutions:
            bandwidth = self.get_bandwidth_for_resolution(resolution)
            resolution_str = self.get_resolution_string(resolution)
            
            manifest += f"#EXT-X-STREAM-INF:BANDWIDTH={bandwidth},RESOLUTION={resolution_str}\n"
            manifest += f"{resolution}.m3u8\n"
        
        return manifest
    
    def generate_resolution_playlist(self, video_id, resolution):
        """Generate playlist for specific resolution"""
        
        # Get video segments for this resolution
        segments = self.get_video_segments(video_id, resolution)
        
        playlist = "#EXTM3U\n#EXT-X-VERSION:3\n#EXT-X-TARGETDURATION:10\n\n"
        
        for segment in segments:
            playlist += f"#EXTINF:{segment.duration},\n"
            playlist += f"{segment.url}\n"
        
        playlist += "#EXT-X-ENDLIST\n"
        
        return playlist
    
    def get_bandwidth_for_resolution(self, resolution):
        bandwidths = {
            "4k": 20000000,     # 20 Mbps
            "1080p": 8000000,   # 8 Mbps
            "720p": 4000000,    # 4 Mbps
            "480p": 2000000,    # 2 Mbps
            "360p": 1000000     # 1 Mbps
        }
        return bandwidths.get(resolution, 2000000)
```

## Search and Discovery

### Video Search Service

```python
class VideoSearchService:
    def __init__(self):
        self.elasticsearch = Elasticsearch()
        self.search_analytics = SearchAnalytics()
        
    def index_video(self, video):
        """Index video for search"""
        
        doc = {
            "video_id": video.video_id,
            "title": video.title,
            "description": video.description,
            "tags": video.tags,
            "category": video.category,
            "channel_name": video.channel.name,
            "upload_date": video.upload_date,
            "duration": video.duration,
            "view_count": video.view_count,
            "like_count": video.like_count,
            "engagement_score": self.calculate_engagement_score(video)
        }
        
        self.elasticsearch.index(
            index="videos",
            id=video.video_id,
            body=doc
        )
    
    def search_videos(self, query, filters=None, limit=20, offset=0):
        """Search videos with ranking"""
        
        search_body = {
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": [
                                    "title^3",
                                    "description^2", 
                                    "tags^2",
                                    "channel_name^1.5"
                                ]
                            }
                        }
                    ],
                    "filter": self.build_filters(filters)
                }
            },
            "sort": [
                {"_score": {"order": "desc"}},
                {"engagement_score": {"order": "desc"}},
                {"upload_date": {"order": "desc"}}
            ],
            "size": limit,
            "from": offset
        }
        
        results = self.elasticsearch.search(index="videos", body=search_body)
        
        # Log search for analytics
        self.search_analytics.log_search(query, results['hits']['total']['value'])
        
        return self.format_search_results(results)
    
    def calculate_engagement_score(self, video):
        """Calculate video engagement score for ranking"""
        
        age_in_days = (datetime.now() - video.upload_date).days
        age_factor = 1.0 / (age_in_days + 1)
        
        view_score = math.log(video.view_count + 1)
        like_ratio = video.like_count / max(video.view_count, 1)
        
        engagement_score = (view_score * 0.7 + like_ratio * 100 * 0.3) * age_factor
        
        return engagement_score
```

## Analytics and Monitoring

### Video Analytics

```python
class VideoAnalyticsService:
    def __init__(self):
        self.time_series_db = InfluxDB()
        self.cache = Redis()
        
    def track_video_view(self, video_id, user_id, watch_data):
        """Track video view event"""
        
        event = {
            "video_id": video_id,
            "user_id": user_id,
            "watched_duration": watch_data['watched_duration'],
            "total_duration": watch_data['total_duration'],
            "completion_rate": watch_data['watched_duration'] / watch_data['total_duration'],
            "quality": watch_data['quality'],
            "device_type": watch_data['device_type'],
            "geo_location": watch_data['geo_location'],
            "timestamp": datetime.utcnow()
        }
        
        # Store in time-series database
        self.time_series_db.write_point("video_views", event)
        
        # Update real-time counters
        self.update_realtime_metrics(video_id, event)
    
    def update_realtime_metrics(self, video_id, event):
        """Update real-time view counters"""
        
        # Increment view count
        self.cache.incr(f"video_views:{video_id}")
        
        # Update hourly view count
        hour_key = f"video_views_hourly:{video_id}:{datetime.utcnow().strftime('%Y%m%d%H')}"
        self.cache.incr(hour_key)
        self.cache.expire(hour_key, 86400)  # 24 hours
        
        # Track completion rate
        if event['completion_rate'] > 0.8:  # 80% completion
            self.cache.incr(f"video_completions:{video_id}")
    
    def get_video_analytics(self, video_id, time_range="24h"):
        """Get analytics for a specific video"""
        
        metrics = {
            "total_views": self.cache.get(f"video_views:{video_id}") or 0,
            "hourly_views": self.get_hourly_views(video_id, time_range),
            "completion_rate": self.calculate_completion_rate(video_id),
            "geographic_distribution": self.get_geographic_stats(video_id, time_range),
            "device_breakdown": self.get_device_stats(video_id, time_range),
            "quality_distribution": self.get_quality_stats(video_id, time_range)
        }
        
        return metrics
```

## Scaling Considerations

### Storage Optimization

**Intelligent Storage Tiering**:
```python
class StorageTierManager:
    def __init__(self):
        self.hot_storage = "SSD_TIER"      # Recent, popular content
        self.warm_storage = "HDD_TIER"     # Older, moderate popularity
        self.cold_storage = "GLACIER_TIER" # Archived content
        
    def determine_storage_tier(self, video_id):
        video_stats = self.get_video_stats(video_id)
        
        # Recent videos (< 30 days) → Hot storage
        if video_stats['age_days'] < 30:
            return self.hot_storage
            
        # Popular videos (> 1000 views/day) → Hot storage
        if video_stats['daily_views'] > 1000:
            return self.hot_storage
            
        # Moderate popularity → Warm storage
        if video_stats['daily_views'] > 10:
            return self.warm_storage
            
        # Low popularity → Cold storage
        return self.cold_storage
    
    def migrate_storage_tier(self, video_id, target_tier):
        current_urls = self.get_video_file_urls(video_id)
        
        for resolution, url in current_urls.items():
            # Copy to new storage tier
            new_url = self.copy_to_tier(url, target_tier)
            
            # Update database with new URL
            self.update_video_file_url(video_id, resolution, new_url)
            
            # Delete from old tier (after verification)
            self.schedule_deletion(url, delay="24h")
```

### Global Distribution

**Edge Cache Management**:
```python
class EdgeCacheManager:
    def __init__(self):
        self.popularity_threshold = 1000  # views per hour
        self.cache_regions = ["NA", "EU", "ASIA", "SA", "AFRICA"]
        
    def should_cache_at_edge(self, video_id):
        recent_views = self.get_recent_views(video_id, hours=1)
        return recent_views > self.popularity_threshold
    
    def predict_viral_content(self, video_id):
        """Predict if content will go viral based on early metrics"""
        
        # Get first hour metrics
        first_hour_views = self.get_views_in_timeframe(video_id, "1h")
        first_hour_engagement = self.get_engagement_rate(video_id, "1h")
        
        # Simple viral prediction
        viral_score = first_hour_views * first_hour_engagement
        
        if viral_score > 10000:  # Threshold for viral prediction
            # Pre-cache in all regions
            self.precache_in_all_regions(video_id)
            return True
            
        return False
```

## Video Streaming Design Quiz

Test your understanding of video streaming system design with the interactive quiz that appears after each part of this series.

## Security and Content Protection

### Copyright Protection
- Content ID system for automatic detection
- DMCA takedown process automation
- Watermarking and fingerprinting

### Access Control
- JWT-based authentication
- CDN token authentication
- Geographic content restrictions

### DRM Implementation
- Encrypted video streams
- License server integration
- Device-specific decryption keys

## Key Takeaways

1. **Multi-Resolution Strategy**: Store videos in multiple qualities for adaptive streaming
2. **Global CDN**: Essential for low latency worldwide video delivery
3. **Intelligent Caching**: Predict and pre-cache viral content
4. **Storage Tiering**: Optimize costs with hot/warm/cold storage
5. **Async Processing**: Use message queues for video transcoding workflows

## What's Next?

In Part 6, we'll design a distributed cache system like Redis, which covers fundamental concepts of caching, data consistency, and distributed system coordination.
