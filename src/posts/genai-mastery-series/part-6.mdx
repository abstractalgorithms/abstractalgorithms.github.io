export const metadata = {
  title: "Fine-tuning and Transfer Learning",
  date: "2024-12-01",
  excerpt: "Learn how to adapt pre-trained language models for specific tasks and domains through fine-tuning and transfer learning techniques. Master model customization strategies.",
  author: "Abstract Algorithms",
  tags: ["fine-tuning", "transfer-learning", "model-adaptation", "llm-training", "domain-specific-ai", "model-customization"],
  coverImage: "./assets/part-6.png",
  series: {
    name: "GenAI Mastery",
    order: 6,
    total: 12,
    prev: "/posts/genai-mastery-series/part-5",
    next: "/posts/genai-mastery-series/part-7"
  }
}

# Part 6: Fine-tuning and Transfer Learning

Welcome to the world of model customization! In this part, we'll explore how to adapt pre-trained language models for specific tasks and domains through fine-tuning and transfer learning techniques.

![Fine-tuning Process](./assets/fine-tuning-process.png)
*The fine-tuning pipeline: From general to specialized*

## 🎯 Learning Objectives

By the end of this part, you'll understand:
- Transfer learning principles for language models
- Different fine-tuning approaches and when to use each
- Parameter-efficient fine-tuning techniques (LoRA, adapters)
- Task-specific adaptation strategies
- Evaluation and deployment of fine-tuned models

## 🧠 Understanding Transfer Learning

Transfer learning leverages knowledge from a pre-trained model and adapts it to new tasks. This approach is revolutionary because it allows us to build specialized AI systems without training from scratch.

### Why Transfer Learning Works

**Hierarchical Feature Learning**
- Lower layers: Basic language patterns (syntax, grammar)
- Middle layers: Semantic relationships and concepts  
- Upper layers: Task-specific reasoning and generation

**Knowledge Transfer**
- **Positive Transfer**: Shared knowledge improves target task
- **Negative Transfer**: Source knowledge hurts target performance
- **Zero-shot Transfer**: Direct application without adaptation

![Knowledge Transfer Layers](./assets/knowledge-transfer.png)
*How different layers capture different aspects of language*

## 🎯 Fine-tuning Approaches

### 1. Full Model Fine-tuning

**Process**: Update all model parameters on target task
**Best for**: Tasks with substantial training data
**Resources**: High compute and memory requirements

```python
# Full fine-tuning example with transformers
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_classes
)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
```

### 2. Feature-based Transfer Learning

**Process**: Freeze pre-trained model, train new classifier on top
**Best for**: Small datasets, quick experiments
**Advantage**: Fast training, less overfitting risk

```python
# Feature-based approach
class FeatureBasedClassifier(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.base_model = base_model
        # Freeze base model parameters
        for param in self.base_model.parameters():
            param.requires_grad = False
            
        self.classifier = nn.Sequential(
            nn.Linear(base_model.config.hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, input_ids, attention_mask):
        with torch.no_grad():
            outputs = self.base_model(input_ids, attention_mask)
        
        pooled_output = outputs.pooler_output
        return self.classifier(pooled_output)
```

### 3. Progressive Fine-tuning

**Process**: Gradually unfreeze layers during training
**Best for**: Preventing catastrophic forgetting
**Strategy**: Start with classifier, then unfreeze top layers

```python
def progressive_unfreezing(model, trainer, num_epochs_per_stage=2):
    # Stage 1: Only classifier
    freeze_base_model(model)
    trainer.train(num_epochs=num_epochs_per_stage)
    
    # Stage 2: Unfreeze top 25% of layers
    unfreeze_top_layers(model, fraction=0.25)
    trainer.train(num_epochs=num_epochs_per_stage)
    
    # Stage 3: Unfreeze all layers with lower learning rate
    unfreeze_all_layers(model)
    trainer.args.learning_rate *= 0.1
    trainer.train(num_epochs=num_epochs_per_stage)
```

## ⚡ Parameter-Efficient Fine-tuning

Modern language models have billions of parameters, making full fine-tuning expensive. Parameter-efficient methods achieve comparable performance while updating only a small fraction of parameters.

### Low-Rank Adaptation (LoRA)

**Key Idea**: Decompose weight updates into low-rank matrices
**Parameters**: Only 0.1-3% of original model size
**Performance**: Matches or exceeds full fine-tuning

![LoRA Architecture](./assets/lora-diagram.png)
*LoRA decomposes weight updates into low-rank matrices*

```python
# LoRA implementation
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=32):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        
        # Original frozen weights
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.weight.requires_grad = False
        
        # Low-rank adaptation matrices
        self.lora_A = nn.Parameter(torch.randn(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        self.scaling = alpha / rank
        
    def forward(self, x):
        # Original computation + low-rank adaptation
        result = F.linear(x, self.weight)
        result += F.linear(F.linear(x, self.lora_A.T), self.lora_B.T) * self.scaling
        return result

# Using LoRA with Hugging Face PEFT
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,  # rank
    lora_alpha=32,
    target_modules=["query", "value"],
    lora_dropout=0.1,
)

model = get_peft_model(base_model, lora_config)
print(f"Trainable parameters: {model.num_parameters(only_trainable=True):,}")
```

### Adapters

**Key Idea**: Insert small bottleneck layers between transformer blocks
**Advantage**: Modular - can stack multiple adapters
**Use case**: Multi-task learning, language adaptation

```python
class AdapterLayer(nn.Module):
    def __init__(self, hidden_size, adapter_size=64):
        super().__init__()
        self.down_project = nn.Linear(hidden_size, adapter_size)
        self.up_project = nn.Linear(adapter_size, hidden_size)
        self.activation = nn.ReLU()
        
    def forward(self, x):
        residual = x
        x = self.down_project(x)
        x = self.activation(x)
        x = self.up_project(x)
        return x + residual  # Residual connection

# Integration into transformer block
class TransformerBlockWithAdapter(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention = MultiHeadAttention(config)
        self.adapter1 = AdapterLayer(config.hidden_size)
        self.feed_forward = FeedForward(config)
        self.adapter2 = AdapterLayer(config.hidden_size)
        self.layer_norm1 = nn.LayerNorm(config.hidden_size)
        self.layer_norm2 = nn.LayerNorm(config.hidden_size)
        
    def forward(self, x):
        # Attention + adapter
        attn_output = self.attention(x)
        x = self.layer_norm1(x + attn_output)
        x = self.adapter1(x)
        
        # Feed-forward + adapter
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)
        x = self.adapter2(x)
        
        return x
```

### Prefix Tuning

**Key Idea**: Optimize continuous task-specific vectors (prefixes)
**Advantage**: No architectural changes needed
**Application**: Particularly effective for generation tasks

```python
class PrefixTuning(nn.Module):
    def __init__(self, config, prefix_length=10):
        super().__init__()
        self.prefix_length = prefix_length
        self.num_layers = config.num_layers
        self.num_heads = config.num_attention_heads
        self.head_dim = config.hidden_size // config.num_attention_heads
        
        # Learnable prefix parameters
        self.prefix_tokens = nn.Parameter(torch.randn(
            prefix_length, config.hidden_size
        ))
        
        # Projection layers for each transformer layer
        self.prefix_mlp = nn.ModuleList([
            nn.Sequential(
                nn.Linear(config.hidden_size, config.hidden_size),
                nn.Tanh(),
                nn.Linear(config.hidden_size, 2 * config.hidden_size)  # Key + Value
            ) for _ in range(self.num_layers)
        ])
    
    def get_prefix_states(self, batch_size):
        prefix_states = []
        for layer_mlp in self.prefix_mlp:
            # Project prefix tokens to key-value pairs
            prefix_kv = layer_mlp(self.prefix_tokens)  # [prefix_len, 2*hidden]
            prefix_kv = prefix_kv.unsqueeze(0).expand(batch_size, -1, -1)
            
            # Split into key and value
            prefix_key, prefix_value = torch.chunk(prefix_kv, 2, dim=-1)
            
            # Reshape for multi-head attention
            prefix_key = prefix_key.view(
                batch_size, self.prefix_length, self.num_heads, self.head_dim
            ).transpose(1, 2)
            
            prefix_value = prefix_value.view(
                batch_size, self.prefix_length, self.num_heads, self.head_dim
            ).transpose(1, 2)
            
            prefix_states.append((prefix_key, prefix_value))
        
        return prefix_states
```

## 🎯 Task-Specific Fine-tuning Strategies

### Text Classification

**Setup**: Add classification head to pre-trained model
**Data**: Labeled examples for target categories
**Evaluation**: Accuracy, F1-score, confusion matrix

```python
# Multi-label classification setup
class MultiLabelClassifier(nn.Module):
    def __init__(self, base_model, num_labels):
        super().__init__()
        self.base_model = base_model
        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.base_model(input_ids, attention_mask)
        pooled_output = outputs.pooler_output
        logits = self.classifier(pooled_output)
        
        if labels is not None:
            loss_fn = nn.BCEWithLogitsLoss()
            loss = loss_fn(logits, labels.float())
            return {"loss": loss, "logits": logits}
        
        return {"logits": self.sigmoid(logits)}
```

### Named Entity Recognition (NER)

**Setup**: Token-level classification
**Challenge**: Handling sub-word tokenization
**Metrics**: Precision, recall, F1 at entity level

```python
# NER with sub-word alignment
def align_labels_with_tokens(tokenizer, words, labels):
    tokenized = tokenizer(words, is_split_into_words=True, return_offsets_mapping=True)
    
    aligned_labels = []
    word_ids = tokenized.word_ids()
    
    previous_word_idx = None
    for word_idx in word_ids:
        if word_idx is None:
            aligned_labels.append(-100)  # Special tokens
        elif word_idx != previous_word_idx:
            aligned_labels.append(labels[word_idx])
        else:
            aligned_labels.append(-100)  # Sub-word tokens
        previous_word_idx = word_idx
    
    return aligned_labels

# NER model
class NERModel(nn.Module):
    def __init__(self, base_model, num_labels):
        super().__init__()
        self.base_model = base_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)
        
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.base_model(input_ids, attention_mask)
        sequence_output = self.dropout(outputs.last_hidden_state)
        logits = self.classifier(sequence_output)
        
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
            return {"loss": loss, "logits": logits}
        
        return {"logits": logits}
```

### Question Answering

**Setup**: Predict start and end positions for answers
**Challenge**: Handling unanswerable questions
**Evaluation**: Exact match, F1-score on token level

```python
class QAModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.qa_outputs = nn.Linear(base_model.config.hidden_size, 2)  # start, end
        
    def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):
        outputs = self.base_model(input_ids, attention_mask)
        sequence_output = outputs.last_hidden_state
        
        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1)
        end_logits = end_logits.squeeze(-1)
        
        if start_positions is not None and end_positions is not None:
            loss_fn = nn.CrossEntropyLoss(ignore_index=-1)
            start_loss = loss_fn(start_logits, start_positions)
            end_loss = loss_fn(end_logits, end_positions)
            total_loss = (start_loss + end_loss) / 2
            
            return {
                "loss": total_loss,
                "start_logits": start_logits,
                "end_logits": end_logits
            }
        
        return {"start_logits": start_logits, "end_logits": end_logits}
```

## 📊 Fine-tuning Best Practices

### Learning Rate Strategies

**Discriminative Learning Rates**
- Lower rates for pre-trained layers
- Higher rates for new task-specific layers

```python
def get_optimizer_with_layer_decay(model, base_lr=2e-5, layer_decay=0.9):
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = []
    
    # Group parameters by layer
    for layer_idx in range(model.config.num_hidden_layers):
        layer_lr = base_lr * (layer_decay ** (model.config.num_hidden_layers - layer_idx - 1))
        
        layer_params = {
            "params": [p for n, p in model.named_parameters() 
                      if f"layer.{layer_idx}" in n and not any(nd in n for nd in no_decay)],
            "weight_decay": 0.01,
            "lr": layer_lr
        }
        optimizer_grouped_parameters.append(layer_params)
        
        layer_params_no_decay = {
            "params": [p for n, p in model.named_parameters() 
                      if f"layer.{layer_idx}" in n and any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
            "lr": layer_lr
        }
        optimizer_grouped_parameters.append(layer_params_no_decay)
    
    return AdamW(optimizer_grouped_parameters)
```

### Regularization Techniques

**Gradual Unfreezing**
```python
def gradual_unfreezing_schedule(model, epoch, total_epochs):
    layers_to_unfreeze = int((epoch / total_epochs) * model.config.num_hidden_layers)
    
    for i, layer in enumerate(model.bert.encoder.layer):
        if i >= model.config.num_hidden_layers - layers_to_unfreeze:
            for param in layer.parameters():
                param.requires_grad = True
        else:
            for param in layer.parameters():
                param.requires_grad = False
```

**Warm-up and Decay**
```python
from transformers import get_linear_schedule_with_warmup

def get_scheduler(optimizer, num_training_steps, warmup_ratio=0.1):
    num_warmup_steps = int(warmup_ratio * num_training_steps)
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )
    
    return scheduler
```

## 🧪 Evaluation and Model Selection

### Validation Strategies

**Hold-out Validation**
- Standard train/validation/test split
- Monitor overfitting and early stopping

**Cross-validation**
- K-fold CV for small datasets
- Stratified sampling for imbalanced data

**Domain-specific Evaluation**
- Out-of-distribution testing
- Temporal validation for time-series data

### Model Selection Criteria

```python
class ModelSelector:
    def __init__(self, metric='f1', patience=3):
        self.metric = metric
        self.patience = patience
        self.best_score = 0
        self.patience_counter = 0
        self.best_model_state = None
    
    def update(self, model, validation_score):
        if validation_score > self.best_score:
            self.best_score = validation_score
            self.patience_counter = 0
            self.best_model_state = model.state_dict().copy()
            return False  # Continue training
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.patience  # Early stopping
    
    def load_best_model(self, model):
        model.load_state_dict(self.best_model_state)
        return model
```

## 🧪 Interactive Quiz

### Question 1: Which fine-tuning approach updates the fewest parameters?
A) Full model fine-tuning
B) Feature-based transfer learning
C) LoRA (Low-Rank Adaptation)
D) Progressive fine-tuning

<details>
<summary>Click for answer</summary>

**Answer: C) LoRA (Low-Rank Adaptation)**

LoRA updates only 0.1-3% of the original model parameters by decomposing weight updates into low-rank matrices, making it the most parameter-efficient approach.
</details>

### Question 2: What is the main advantage of gradual unfreezing?
A) Faster training
B) Prevents catastrophic forgetting
C) Requires less memory
D) Improves final accuracy

<details>
<summary>Click for answer</summary>

**Answer: B) Prevents catastrophic forgetting**

Gradual unfreezing helps prevent catastrophic forgetting by allowing the model to adapt slowly to the new task while preserving knowledge from pre-training.
</details>

### Question 3: In discriminative learning rates, which layers typically get higher learning rates?
A) All layers get the same rate
B) Lower layers (closer to input)
C) Upper layers (closer to output)
D) Middle layers only

<details>
<summary>Click for answer</summary>

**Answer: C) Upper layers (closer to output)**

Upper layers are typically more task-specific and can benefit from higher learning rates, while lower layers contain more general features that should be updated more conservatively.
</details>

## 🛠️ Hands-On Exercise: Fine-tuning for Sentiment Analysis

Let's implement a complete fine-tuning pipeline:

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

class SentimentFineTuner:
    def __init__(self, model_name="distilbert-base-uncased"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=2
        )
    
    def preprocess_data(self, texts, labels):
        tokenized = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        return Dataset.from_dict({
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": torch.tensor(labels, dtype=torch.long)
        })
    
    def compute_metrics(self, eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='weighted'
        )
        accuracy = accuracy_score(labels, predictions)
        
        return {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    
    def train(self, train_texts, train_labels, eval_texts, eval_labels):
        train_dataset = self.preprocess_data(train_texts, train_labels)
        eval_dataset = self.preprocess_data(eval_texts, eval_labels)
        
        training_args = TrainingArguments(
            output_dir="./sentiment_model",
            num_train_epochs=3,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=64,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir="./logs",
            logging_steps=10,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            compute_metrics=self.compute_metrics,
        )
        
        trainer.train()
        return trainer

# Example usage
fine_tuner = SentimentFineTuner()

# Sample data (replace with your dataset)
train_texts = ["I love this movie!", "This is terrible.", "Great performance!"]
train_labels = [1, 0, 1]  # 1: positive, 0: negative

eval_texts = ["Amazing film!", "So boring."]
eval_labels = [1, 0]

trainer = fine_tuner.train(train_texts, train_labels, eval_texts, eval_labels)

# Inference
def predict_sentiment(text):
    inputs = fine_tuner.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = fine_tuner.model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    return {"positive": predictions[0][1].item(), "negative": predictions[0][0].item()}

result = predict_sentiment("This movie is fantastic!")
print(result)
```

## 🚀 Advanced Topics

### Multi-task Learning

**Approach**: Train single model on multiple related tasks
**Benefits**: Improved generalization, parameter sharing
**Challenge**: Balancing task-specific losses

```python
class MultiTaskModel(nn.Module):
    def __init__(self, base_model, task_configs):
        super().__init__()
        self.base_model = base_model
        self.task_heads = nn.ModuleDict({
            task_name: nn.Linear(base_model.config.hidden_size, config["num_labels"])
            for task_name, config in task_configs.items()
        })
    
    def forward(self, input_ids, attention_mask, task_name, labels=None):
        outputs = self.base_model(input_ids, attention_mask)
        pooled_output = outputs.pooler_output
        logits = self.task_heads[task_name](pooled_output)
        
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits, labels)
            return {"loss": loss, "logits": logits}
        
        return {"logits": logits}
```

### Domain Adaptation

**Challenge**: Model trained on one domain, applied to another
**Techniques**: Domain adversarial training, gradual domain shift

```python
class DomainAdversarialTraining(nn.Module):
    def __init__(self, feature_extractor, task_classifier, domain_classifier):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.task_classifier = task_classifier
        self.domain_classifier = domain_classifier
        self.gradient_reversal = GradientReversalLayer()
    
    def forward(self, x, domain_labels=None, task_labels=None):
        features = self.feature_extractor(x)
        
        # Task prediction
        task_logits = self.task_classifier(features)
        
        # Domain prediction with gradient reversal
        domain_features = self.gradient_reversal(features)
        domain_logits = self.domain_classifier(domain_features)
        
        return {
            "task_logits": task_logits,
            "domain_logits": domain_logits,
            "features": features
        }
```

## 🔗 What's Next?

In **Part 7**, we'll explore **Retrieval-Augmented Generation (RAG) Systems**, learning how to combine the power of large language models with external knowledge sources for more accurate and up-to-date AI applications.

Topics we'll cover:
- RAG architecture and components
- Dense vs sparse retrieval methods
- Vector databases and similarity search
- RAG evaluation and optimization
- Advanced RAG patterns and techniques

## 📚 Further Reading

**Research Papers:**
- "LoRA: Low-Rank Adaptation of Large Language Models"
- "Parameter-Efficient Transfer Learning for NLP"
- "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"

**Implementation Resources:**
- Hugging Face PEFT library
- Microsoft's LoRA implementation
- Google's T5X framework

**Best Practices:**
- "Fine-Tuning Language Models from Human Preferences"
- "What Makes Good In-Context Examples for GPT-3?"
- "Scaling Laws for Transfer"

---

*Continue with [Part 7: Retrieval-Augmented Generation (RAG) Systems →](/posts/genai-mastery-series/part-7)*
