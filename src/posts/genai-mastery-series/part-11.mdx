export const metadata = {
  title: "Building Production AI Applications",
  date: "2024-12-01",
  excerpt: "Learn how to build, deploy, and maintain AI applications at scale. Master MLOps practices, infrastructure design, monitoring, and production AI system operations.",
  author: "Abstract Algorithms",
  tags: ["production-ai", "mlops", "ai-deployment", "scalability", "monitoring", "ai-infrastructure"],
  coverImage: "./assets/part-11.png",
  series: {
    name: "GenAI Mastery",
    order: 11,
    total: 12,
    prev: "/posts/genai-mastery-series/part-10",
    next: "/posts/genai-mastery-series/part-12"
  }
}

# Part 11: Building Production AI Applications

Welcome to the engineering reality of AI! In this part, we'll explore how to build, deploy, and maintain AI applications at scale. We'll cover MLOps practices, infrastructure design, monitoring strategies, and everything needed to successfully operate AI systems in production.

![Production AI Architecture](./assets/production-ai-architecture.png)
*End-to-end architecture for production AI applications*

## ðŸŽ¯ Learning Objectives

By the end of this part, you'll understand:
- MLOps principles and practices for AI systems
- Scalable infrastructure design for AI applications
- Model deployment strategies and patterns
- Monitoring, observability, and maintenance practices
- Performance optimization and cost management
- Security and compliance in production AI

## ðŸ—ï¸ MLOps: DevOps for AI

MLOps (Machine Learning Operations) extends DevOps principles to machine learning and AI systems, enabling reliable, scalable, and maintainable AI applications.

### MLOps Lifecycle

**Data Management**
- Data versioning and lineage
- Quality monitoring and validation
- Feature engineering pipelines
- Data drift detection

**Model Development**
- Experiment tracking and reproducibility
- Model versioning and registry
- Automated testing and validation
- Collaborative development workflows

**Deployment & Operations**
- Automated deployment pipelines
- A/B testing and gradual rollouts
- Performance monitoring
- Model retraining and updates

![MLOps Pipeline](./assets/mlops-pipeline.png)
*Complete MLOps pipeline from data to deployment*

### Core MLOps Principles

**Automation**
- Continuous Integration/Continuous Deployment (CI/CD)
- Automated testing and validation
- Infrastructure as Code (IaC)
- Self-healing systems

**Reproducibility**
- Version control for code, data, and models
- Environment management and containerization
- Experiment tracking and documentation
- Deterministic training processes

**Collaboration**
- Cross-functional team workflows
- Model governance and approval processes
- Shared environments and resources
- Knowledge sharing and documentation

## ðŸ³ Infrastructure and Deployment

### Containerization with Docker

```dockerfile
# Multi-stage Dockerfile for AI application
FROM python:3.9-slim as base

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Production stage
FROM base as production

# Create non-root user
RUN useradd --create-home --shell /bin/bash appuser
USER appuser

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Kubernetes Deployment

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-model-service
  labels:
    app: ai-model-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-model-service
  template:
    metadata:
      labels:
        app: ai-model-service
    spec:
      containers:
      - name: ai-model
        image: your-registry/ai-model:v1.0.0
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_VERSION
          value: "v1.0.0"
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ai-model-service
spec:
  selector:
    app: ai-model-service
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

### Model Serving Patterns

**Synchronous Serving**
- Real-time API endpoints
- Low latency requirements
- Direct HTTP/gRPC serving

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import asyncio
from typing import List, Dict, Any
import time

app = FastAPI(title="AI Model API", version="1.0.0")

class PredictionRequest(BaseModel):
    features: List[float]
    model_version: str = "latest"

class PredictionResponse(BaseModel):
    prediction: float
    confidence: float
    model_version: str
    inference_time_ms: float

class ModelService:
    def __init__(self):
        self.models = {}
        self.load_models()
    
    def load_models(self):
        """Load multiple model versions"""
        # Load different model versions
        self.models = {
            "v1.0": self.load_model("models/v1.0/model.pt"),
            "v1.1": self.load_model("models/v1.1/model.pt"),
            "latest": self.load_model("models/latest/model.pt")
        }
    
    def load_model(self, model_path: str):
        """Load individual model"""
        try:
            model = torch.load(model_path, map_location='cpu')
            model.eval()
            return model
        except Exception as e:
            print(f"Failed to load model {model_path}: {e}")
            return None
    
    async def predict(self, features: List[float], model_version: str = "latest"):
        """Make prediction with specified model version"""
        if model_version not in self.models:
            raise ValueError(f"Model version {model_version} not found")
        
        model = self.models[model_version]
        if model is None:
            raise ValueError(f"Model version {model_version} failed to load")
        
        start_time = time.time()
        
        # Prepare input
        input_tensor = torch.tensor([features], dtype=torch.float32)
        
        # Make prediction
        with torch.no_grad():
            output = model(input_tensor)
            prediction = output.item()
            
            # Calculate confidence (example using output variance)
            confidence = min(abs(prediction), 1.0)
        
        inference_time = (time.time() - start_time) * 1000
        
        return prediction, confidence, inference_time

# Global model service instance
model_service = ModelService()

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Make prediction endpoint"""
    try:
        prediction, confidence, inference_time = await model_service.predict(
            request.features, 
            request.model_version
        )
        
        return PredictionResponse(
            prediction=prediction,
            confidence=confidence,
            model_version=request.model_version,
            inference_time_ms=inference_time
        )
    
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": time.time()}

@app.get("/models")
async def list_models():
    """List available model versions"""
    return {
        "available_models": list(model_service.models.keys()),
        "default_model": "latest"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Asynchronous/Batch Serving**
- High throughput processing
- Non-real-time requirements
- Queue-based processing

```python
import asyncio
import aioredis
import json
import uuid
from typing import Dict, Any, List
import time

class AsyncModelService:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self.redis = None
        self.models = {}
        
    async def initialize(self):
        """Initialize Redis connection and load models"""
        self.redis = await aioredis.from_url(self.redis_url)
        await self.load_models()
        
    async def load_models(self):
        """Load models for batch processing"""
        # Load models here
        pass
    
    async def submit_batch_job(self, batch_data: List[Dict[str, Any]]) -> str:
        """Submit batch job for processing"""
        job_id = str(uuid.uuid4())
        
        job_data = {
            "job_id": job_id,
            "data": batch_data,
            "status": "pending",
            "submitted_at": time.time()
        }
        
        # Store job in Redis
        await self.redis.hset(f"job:{job_id}", mapping={
            "data": json.dumps(job_data),
            "status": "pending"
        })
        
        # Add to processing queue
        await self.redis.lpush("processing_queue", job_id)
        
        return job_id
    
    async def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """Get job status and results"""
        job_data = await self.redis.hget(f"job:{job_id}", "data")
        
        if job_data:
            return json.loads(job_data)
        else:
            return {"error": "Job not found"}
    
    async def process_batch_jobs(self):
        """Background worker to process batch jobs"""
        while True:
            try:
                # Get job from queue (blocking)
                job_id = await self.redis.brpop("processing_queue", timeout=10)
                
                if job_id:
                    job_id = job_id[1].decode('utf-8')
                    await self._process_job(job_id)
                    
            except Exception as e:
                print(f"Error processing batch job: {e}")
                await asyncio.sleep(1)
    
    async def _process_job(self, job_id: str):
        """Process individual batch job"""
        # Update status to processing
        await self.redis.hset(f"job:{job_id}", "status", "processing")
        
        try:
            # Get job data
            job_data_raw = await self.redis.hget(f"job:{job_id}", "data")
            job_data = json.loads(job_data_raw)
            
            # Process batch
            results = []
            for item in job_data["data"]:
                # Simulate model inference
                result = await self._inference(item)
                results.append(result)
            
            # Update job with results
            job_data["results"] = results
            job_data["status"] = "completed"
            job_data["completed_at"] = time.time()
            
            await self.redis.hset(f"job:{job_id}", mapping={
                "data": json.dumps(job_data),
                "status": "completed"
            })
            
        except Exception as e:
            # Mark job as failed
            await self.redis.hset(f"job:{job_id}", "status", f"failed: {str(e)}")
    
    async def _inference(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """Perform model inference on single item"""
        # Simulate processing time
        await asyncio.sleep(0.1)
        
        return {
            "input": item,
            "prediction": 0.85,  # Placeholder
            "confidence": 0.92,
            "processed_at": time.time()
        }

# FastAPI integration for batch processing
@app.post("/batch/submit")
async def submit_batch(batch_data: List[Dict[str, Any]]):
    """Submit batch processing job"""
    job_id = await async_service.submit_batch_job(batch_data)
    return {"job_id": job_id, "status": "submitted"}

@app.get("/batch/{job_id}")
async def get_batch_status(job_id: str):
    """Get batch job status"""
    return await async_service.get_job_status(job_id)
```

## ðŸ“Š Monitoring and Observability

### Model Performance Monitoring

```python
import logging
import time
import numpy as np
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from collections import defaultdict, deque
import threading

class ModelMonitor:
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        
        # Prometheus metrics
        self.prediction_counter = Counter('model_predictions_total', 'Total predictions made')
        self.prediction_latency = Histogram('model_prediction_duration_seconds', 'Prediction latency')
        self.model_accuracy = Gauge('model_accuracy', 'Current model accuracy')
        self.drift_score = Gauge('model_drift_score', 'Data drift score')
        
        # Internal tracking
        self.predictions = deque(maxlen=window_size)
        self.ground_truth = deque(maxlen=window_size)
        self.feature_stats = defaultdict(lambda: deque(maxlen=window_size))
        self.baseline_stats = {}
        
        # Start metrics server
        start_http_server(8001)
        
        # Start background monitoring
        self._start_background_monitoring()
    
    def log_prediction(self, features: np.ndarray, prediction: float, 
                      ground_truth: float = None, latency: float = None):
        """Log a prediction for monitoring"""
        
        # Update Prometheus metrics
        self.prediction_counter.inc()
        if latency:
            self.prediction_latency.observe(latency)
        
        # Store for analysis
        self.predictions.append(prediction)
        if ground_truth is not None:
            self.ground_truth.append(ground_truth)
        
        # Track feature statistics
        for i, feature_value in enumerate(features):
            self.feature_stats[f'feature_{i}'].append(feature_value)
        
    def calculate_accuracy(self) -> float:
        """Calculate recent accuracy"""
        if len(self.ground_truth) == 0:
            return 0.0
        
        # Convert predictions to binary (example)
        pred_binary = [1 if p > 0.5 else 0 for p in self.predictions[-len(self.ground_truth):]]
        gt_binary = [1 if gt > 0.5 else 0 for gt in self.ground_truth]
        
        correct = sum(p == gt for p, gt in zip(pred_binary, gt_binary))
        accuracy = correct / len(gt_binary)
        
        # Update Prometheus metric
        self.model_accuracy.set(accuracy)
        
        return accuracy
    
    def detect_drift(self) -> Dict[str, float]:
        """Detect data drift using statistical tests"""
        drift_scores = {}
        
        for feature_name, values in self.feature_stats.items():
            if len(values) < 100:  # Need sufficient data
                continue
                
            # Get baseline statistics (would be set during model training)
            if feature_name not in self.baseline_stats:
                continue
                
            baseline_mean = self.baseline_stats[feature_name]['mean']
            baseline_std = self.baseline_stats[feature_name]['std']
            
            # Current statistics
            current_values = list(values)
            current_mean = np.mean(current_values)
            current_std = np.std(current_values)
            
            # Simple drift detection using mean shift
            mean_shift = abs(current_mean - baseline_mean) / baseline_std
            std_change = abs(current_std - baseline_std) / baseline_std
            
            drift_score = max(mean_shift, std_change)
            drift_scores[feature_name] = drift_score
        
        # Overall drift score
        overall_drift = np.mean(list(drift_scores.values())) if drift_scores else 0.0
        self.drift_score.set(overall_drift)
        
        return drift_scores
    
    def _start_background_monitoring(self):
        """Start background thread for periodic monitoring"""
        def monitor_loop():
            while True:
                try:
                    # Calculate and update metrics
                    self.calculate_accuracy()
                    drift_scores = self.detect_drift()
                    
                    # Log alerts if necessary
                    if self.model_accuracy._value._value < 0.8:
                        logging.warning(f"Model accuracy dropped to {self.model_accuracy._value._value:.3f}")
                    
                    for feature, score in drift_scores.items():
                        if score > 2.0:  # Threshold for significant drift
                            logging.warning(f"Significant drift detected in {feature}: {score:.3f}")
                    
                    time.sleep(60)  # Check every minute
                    
                except Exception as e:
                    logging.error(f"Monitoring error: {e}")
                    time.sleep(60)
        
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()

# Global monitor instance
monitor = ModelMonitor()

@app.post("/predict")
async def predict_with_monitoring(request: PredictionRequest):
    """Prediction endpoint with monitoring"""
    start_time = time.time()
    
    try:
        # Make prediction
        prediction, confidence, inference_time = await model_service.predict(
            request.features, 
            request.model_version
        )
        
        # Log for monitoring
        monitor.log_prediction(
            features=np.array(request.features),
            prediction=prediction,
            latency=time.time() - start_time
        )
        
        return PredictionResponse(
            prediction=prediction,
            confidence=confidence,
            model_version=request.model_version,
            inference_time_ms=inference_time
        )
    
    except Exception as e:
        # Log error for monitoring
        logging.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail="Prediction failed")

@app.get("/metrics/accuracy")
async def get_accuracy():
    """Get current model accuracy"""
    return {"accuracy": monitor.calculate_accuracy()}

@app.get("/metrics/drift")
async def get_drift_scores():
    """Get current drift scores"""
    return {"drift_scores": monitor.detect_drift()}
```

### A/B Testing Framework

```python
import hashlib
import random
from typing import Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

class ExperimentStatus(Enum):
    DRAFT = "draft"
    ACTIVE = "active" 
    PAUSED = "paused"
    COMPLETED = "completed"

@dataclass
class ExperimentConfig:
    name: str
    control_model: str
    treatment_model: str
    traffic_split: float  # 0.0 to 1.0
    status: ExperimentStatus
    success_metric: str
    minimum_sample_size: int

class ABTestingFramework:
    def __init__(self):
        self.experiments = {}
        self.user_assignments = {}
        self.experiment_results = defaultdict(lambda: defaultdict(list))
    
    def create_experiment(self, config: ExperimentConfig):
        """Create new A/B test experiment"""
        self.experiments[config.name] = config
        
    def assign_user_to_treatment(self, user_id: str, experiment_name: str) -> str:
        """Deterministically assign user to control or treatment"""
        experiment = self.experiments.get(experiment_name)
        if not experiment or experiment.status != ExperimentStatus.ACTIVE:
            return experiment.control_model
        
        # Use hash for deterministic assignment
        hash_input = f"{user_id}_{experiment_name}".encode()
        hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)
        assignment_value = (hash_value % 100) / 100.0
        
        if assignment_value < experiment.traffic_split:
            assigned_model = experiment.treatment_model
        else:
            assigned_model = experiment.control_model
        
        # Cache assignment
        self.user_assignments[f"{user_id}_{experiment_name}"] = assigned_model
        
        return assigned_model
    
    def log_experiment_result(self, user_id: str, experiment_name: str, 
                            model_used: str, metric_value: float):
        """Log experiment result for analysis"""
        self.experiment_results[experiment_name][model_used].append({
            'user_id': user_id,
            'metric_value': metric_value,
            'timestamp': time.time()
        })
    
    def get_experiment_results(self, experiment_name: str) -> Dict[str, Any]:
        """Get experiment results and statistical analysis"""
        experiment = self.experiments.get(experiment_name)
        if not experiment:
            return {"error": "Experiment not found"}
        
        control_results = self.experiment_results[experiment_name][experiment.control_model]
        treatment_results = self.experiment_results[experiment_name][experiment.treatment_model]
        
        if not control_results or not treatment_results:
            return {"error": "Insufficient data"}
        
        # Calculate basic statistics
        control_values = [r['metric_value'] for r in control_results]
        treatment_values = [r['metric_value'] for r in treatment_results]
        
        control_mean = np.mean(control_values)
        treatment_mean = np.mean(treatment_values)
        
        # Statistical significance test (simplified)
        from scipy import stats
        t_stat, p_value = stats.ttest_ind(treatment_values, control_values)
        
        return {
            'experiment_name': experiment_name,
            'control_model': experiment.control_model,
            'treatment_model': experiment.treatment_model,
            'control_sample_size': len(control_results),
            'treatment_sample_size': len(treatment_results),
            'control_mean': control_mean,
            'treatment_mean': treatment_mean,
            'lift': (treatment_mean - control_mean) / control_mean * 100,
            'p_value': p_value,
            'is_significant': p_value < 0.05,
            'confidence_level': 95
        }

# Global A/B testing framework
ab_framework = ABTestingFramework()

# Create experiment
experiment_config = ExperimentConfig(
    name="model_v2_accuracy_test",
    control_model="v1.0",
    treatment_model="v2.0", 
    traffic_split=0.2,  # 20% to treatment
    status=ExperimentStatus.ACTIVE,
    success_metric="accuracy",
    minimum_sample_size=1000
)
ab_framework.create_experiment(experiment_config)

@app.post("/predict")
async def predict_with_ab_test(request: PredictionRequest, user_id: str = "anonymous"):
    """Prediction with A/B testing"""
    # Determine which model to use
    if "experiment" in request.dict():
        experiment_name = request.experiment
        model_version = ab_framework.assign_user_to_treatment(user_id, experiment_name)
    else:
        model_version = request.model_version
    
    # Make prediction
    prediction, confidence, inference_time = await model_service.predict(
        request.features, 
        model_version
    )
    
    # Log for A/B test if applicable
    if "experiment" in request.dict():
        # Would log actual success metric in real implementation
        ab_framework.log_experiment_result(
            user_id, request.experiment, model_version, confidence
        )
    
    return PredictionResponse(
        prediction=prediction,
        confidence=confidence,
        model_version=model_version,
        inference_time_ms=inference_time
    )

@app.get("/experiments/{experiment_name}/results")
async def get_experiment_results(experiment_name: str):
    """Get A/B test results"""
    return ab_framework.get_experiment_results(experiment_name)
```

## ðŸ”„ Model Lifecycle Management

### Model Registry and Versioning

```python
import mlflow
import mlflow.pytorch
from typing import Dict, Any, List, Optional
import json
import boto3
from pathlib import Path

class ModelRegistry:
    def __init__(self, registry_uri: str = "postgresql://localhost/mlflow"):
        mlflow.set_tracking_uri(registry_uri)
        self.client = mlflow.tracking.MlflowClient()
        
    def register_model(self, model_path: str, model_name: str, 
                      metadata: Dict[str, Any]) -> str:
        """Register a new model version"""
        
        # Log model artifacts
        with mlflow.start_run() as run:
            # Log model
            mlflow.pytorch.log_model(
                pytorch_model=model_path,
                artifact_path="model",
                registered_model_name=model_name
            )
            
            # Log metadata
            mlflow.log_params(metadata.get("parameters", {}))
            mlflow.log_metrics(metadata.get("metrics", {}))
            
            # Log additional artifacts
            if "config" in metadata:
                with open("config.json", "w") as f:
                    json.dump(metadata["config"], f)
                mlflow.log_artifact("config.json")
            
            run_id = run.info.run_id
        
        # Create model version
        model_version = self.client.create_model_version(
            name=model_name,
            source=f"runs:/{run_id}/model",
            description=metadata.get("description", "")
        )
        
        return model_version.version
    
    def promote_model(self, model_name: str, version: str, stage: str):
        """Promote model to different stage (Staging, Production)"""
        self.client.transition_model_version_stage(
            name=model_name,
            version=version,
            stage=stage
        )
    
    def get_model_version(self, model_name: str, stage: str = "Production"):
        """Get model version by stage"""
        latest_version = self.client.get_latest_versions(
            model_name, 
            stages=[stage]
        )
        
        if latest_version:
            return latest_version[0]
        return None
    
    def download_model(self, model_name: str, version: str, 
                      local_path: str) -> str:
        """Download model artifacts"""
        model_uri = f"models:/{model_name}/{version}"
        model_path = mlflow.pytorch.load_model(model_uri)
        return model_path
    
    def list_model_versions(self, model_name: str) -> List[Dict[str, Any]]:
        """List all versions of a model"""
        versions = self.client.search_model_versions(f"name='{model_name}'")
        
        return [
            {
                "version": v.version,
                "stage": v.current_stage,
                "status": v.status,
                "creation_timestamp": v.creation_timestamp,
                "description": v.description
            }
            for v in versions
        ]

class ModelDeploymentManager:
    def __init__(self, registry: ModelRegistry):
        self.registry = registry
        self.active_models = {}
        
    def deploy_model(self, model_name: str, version: str, 
                    deployment_config: Dict[str, Any]):
        """Deploy specific model version"""
        
        # Download model
        model_path = self.registry.download_model(model_name, version, "/tmp/models/")
        
        # Load model into memory
        model = torch.load(model_path)
        model.eval()
        
        # Store in active models
        deployment_key = f"{model_name}:{version}"
        self.active_models[deployment_key] = {
            "model": model,
            "config": deployment_config,
            "deployed_at": time.time()
        }
        
        # Update model registry status
        self.registry.client.set_model_version_tag(
            model_name, version, "deployment_status", "deployed"
        )
        
        return deployment_key
    
    def rollback_model(self, model_name: str, target_version: str):
        """Rollback to previous model version"""
        
        # Undeploy current version
        current_deployment = None
        for key in self.active_models:
            if key.startswith(f"{model_name}:"):
                current_deployment = key
                break
        
        if current_deployment:
            del self.active_models[current_deployment]
        
        # Deploy target version
        return self.deploy_model(model_name, target_version, {})
    
    def gradual_rollout(self, model_name: str, new_version: str, 
                       traffic_percentage: float):
        """Gradually increase traffic to new model version"""
        
        # This would integrate with load balancer/traffic routing
        # For demonstration, we'll simulate with a routing table
        
        old_version_key = None
        for key in self.active_models:
            if key.startswith(f"{model_name}:"):
                old_version_key = key
                break
        
        new_version_key = self.deploy_model(model_name, new_version, {})
        
        # Update routing weights (would integrate with actual load balancer)
        routing_config = {
            "model_name": model_name,
            "routes": {
                old_version_key: 1.0 - traffic_percentage,
                new_version_key: traffic_percentage
            }
        }
        
        return routing_config
    
    def get_model_for_prediction(self, model_name: str, 
                                routing_config: Optional[Dict] = None):
        """Get model instance for prediction based on routing"""
        
        if routing_config:
            # Weighted random selection
            routes = routing_config["routes"]
            weights = list(routes.values())
            models = list(routes.keys())
            
            selected_model = random.choices(models, weights=weights)[0]
            return self.active_models[selected_model]["model"]
        
        # Default: use latest version
        for key in self.active_models:
            if key.startswith(f"{model_name}:"):
                return self.active_models[key]["model"]
        
        return None

# Usage example
registry = ModelRegistry()
deployment_manager = ModelDeploymentManager(registry)

# Register new model version
model_metadata = {
    "parameters": {"learning_rate": 0.001, "batch_size": 32},
    "metrics": {"accuracy": 0.95, "f1_score": 0.93},
    "description": "Improved model with better regularization"
}

version = registry.register_model(
    model_path="path/to/model.pt",
    model_name="sentiment_classifier", 
    metadata=model_metadata
)

# Deploy to staging first
registry.promote_model("sentiment_classifier", version, "Staging")

# After validation, promote to production
registry.promote_model("sentiment_classifier", version, "Production")

# Deploy with gradual rollout
rollout_config = deployment_manager.gradual_rollout(
    "sentiment_classifier", version, traffic_percentage=0.1
)
```

## ðŸš€ Performance Optimization

### Model Optimization Techniques

```python
import torch
import torch.quantization as quantization
from torch.jit import script
import onnx
import onnxruntime as ort
import tensorrt as trt

class ModelOptimizer:
    def __init__(self):
        self.optimization_methods = {
            'quantization': self.quantize_model,
            'pruning': self.prune_model,
            'distillation': self.distill_model,
            'onnx_conversion': self.convert_to_onnx,
            'tensorrt_optimization': self.optimize_with_tensorrt
        }
    
    def quantize_model(self, model: torch.nn.Module, 
                      calibration_data: torch.Tensor) -> torch.nn.Module:
        """Apply dynamic quantization to reduce model size"""
        
        # Prepare model for quantization
        model.eval()
        model.qconfig = quantization.get_default_qconfig('fbgemm')
        model_prepared = quantization.prepare(model)
        
        # Calibrate with representative data
        with torch.no_grad():
            for batch in calibration_data:
                model_prepared(batch)
        
        # Convert to quantized model
        quantized_model = quantization.convert(model_prepared)
        
        return quantized_model
    
    def prune_model(self, model: torch.nn.Module, 
                   pruning_ratio: float = 0.3) -> torch.nn.Module:
        """Apply magnitude-based pruning"""
        import torch.nn.utils.prune as prune
        
        # Apply pruning to linear layers
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=pruning_ratio)
                # Make pruning permanent
                prune.remove(module, 'weight')
        
        return model
    
    def convert_to_onnx(self, model: torch.nn.Module, 
                       input_shape: tuple, output_path: str):
        """Convert PyTorch model to ONNX format"""
        
        # Create dummy input
        dummy_input = torch.randn(input_shape)
        
        # Export to ONNX
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        return output_path
    
    def optimize_with_tensorrt(self, onnx_model_path: str) -> str:
        """Optimize ONNX model with TensorRT"""
        
        # Load ONNX model
        onnx_model = onnx.load(onnx_model_path)
        
        # Create TensorRT engine
        logger = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(logger)
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, logger)
        
        # Parse ONNX model
        success = parser.parse_from_file(onnx_model_path)
        if not success:
            raise Exception("Failed to parse ONNX model")
        
        # Configure optimization
        config = builder.create_builder_config()
        config.max_workspace_size = 1 << 28  # 256MB
        config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision
        
        # Build engine
        engine = builder.build_engine(network, config)
        
        # Serialize engine
        serialized_engine = engine.serialize()
        engine_path = onnx_model_path.replace('.onnx', '.trt')
        
        with open(engine_path, 'wb') as f:
            f.write(serialized_engine)
        
        return engine_path
    
    def benchmark_model(self, model, input_data: torch.Tensor, 
                       num_runs: int = 100) -> Dict[str, float]:
        """Benchmark model performance"""
        
        model.eval()
        torch.backends.cudnn.benchmark = True
        
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = model(input_data)
        
        # Benchmark
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(input_data)
        
        total_time = time.time() - start_time
        avg_latency = total_time / num_runs
        throughput = num_runs / total_time
        
        return {
            "average_latency_ms": avg_latency * 1000,
            "throughput_req_per_sec": throughput,
            "total_time_sec": total_time
        }

# Usage example
optimizer = ModelOptimizer()

# Original model
original_model = load_your_model()
input_shape = (1, 768)  # Example input shape
dummy_input = torch.randn(input_shape)

# Benchmark original model
original_perf = optimizer.benchmark_model(original_model, dummy_input)
print(f"Original model performance: {original_perf}")

# Apply quantization
quantized_model = optimizer.quantize_model(original_model, [dummy_input])
quantized_perf = optimizer.benchmark_model(quantized_model, dummy_input)
print(f"Quantized model performance: {quantized_perf}")

# Convert to ONNX
onnx_path = optimizer.convert_to_onnx(original_model, input_shape, "model.onnx")

# Optimize with TensorRT (if available)
try:
    trt_path = optimizer.optimize_with_tensorrt(onnx_path)
    print(f"TensorRT optimized model saved to: {trt_path}")
except Exception as e:
    print(f"TensorRT optimization failed: {e}")
```

## ðŸ§ª Interactive Quiz

### Question 1: What is the main benefit of containerizing AI applications?
A) Reduced model accuracy
B) Environment consistency and portability
C) Increased training time
D) Higher memory usage

<details>
<summary>Click for answer</summary>

**Answer: B) Environment consistency and portability**

Containerization ensures that AI applications run consistently across different environments (development, staging, production) by packaging all dependencies together.
</details>

### Question 2: In A/B testing for ML models, what determines user assignment to treatment groups?
A) Random assignment each time
B) User preferences
C) Deterministic hash-based assignment
D) Model performance

<details>
<summary>Click for answer</summary>

**Answer: C) Deterministic hash-based assignment**

Hash-based assignment ensures users consistently see the same model variant throughout the experiment while maintaining randomness across the user population.
</details>

### Question 3: What is the primary purpose of model quantization?
A) Improve model accuracy
B) Reduce model size and inference time
C) Increase training speed
D) Add new features

<details>
<summary>Click for answer</summary>

**Answer: B) Reduce model size and inference time**

Quantization reduces the precision of model weights (e.g., from 32-bit to 8-bit), significantly reducing model size and speeding up inference with minimal accuracy loss.
</details>

## ðŸŽ¯ Production Readiness Checklist

### Pre-Production
- [ ] Comprehensive testing (unit, integration, load testing)
- [ ] Security scanning and vulnerability assessment
- [ ] Performance benchmarking and optimization
- [ ] Documentation and runbooks
- [ ] Disaster recovery and backup procedures

### Deployment
- [ ] Staged rollout with monitoring
- [ ] Health checks and readiness probes
- [ ] Rollback procedures tested
- [ ] Monitoring and alerting configured
- [ ] Load balancing and auto-scaling setup

### Post-Production
- [ ] Performance monitoring and SLA tracking
- [ ] Model drift detection and alerting
- [ ] Regular model evaluation and retraining
- [ ] Incident response procedures
- [ ] Cost monitoring and optimization

## ðŸ”— What's Next?

In **Part 12**, we'll explore the **Future of AI and Emerging Trends**, covering cutting-edge developments, research directions, and what's coming next in the rapidly evolving AI landscape.

Topics we'll cover:
- Emerging AI architectures and paradigms
- Multimodal AI and unified models
- AI for scientific discovery and research
- Quantum computing and AI
- The path toward Artificial General Intelligence (AGI)

## ðŸ“š Further Reading

**MLOps Resources:**
- "Machine Learning Engineering" by Andriy Burkov
- "Building Machine Learning Pipelines" by Hannes Hapke
- MLOps.org community resources

**Infrastructure and Deployment:**
- Kubernetes documentation and best practices
- Docker containerization guides
- Cloud provider AI/ML services documentation

**Monitoring and Observability:**
- Prometheus and Grafana documentation
- MLflow tracking and model registry
- Evidently AI for model monitoring

---

*Continue with [Part 12: Future of AI and Emerging Trends â†’](/posts/genai-mastery-series/part-12)*
