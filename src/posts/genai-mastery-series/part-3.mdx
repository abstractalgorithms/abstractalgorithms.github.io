export const metadata = {
  title: "Transformer Architecture & Attention Mechanisms",
  date: "2024-12-01",
  excerpt: "Master the transformer architecture that revolutionized AI. Learn about self-attention, multi-head attention, and how transformers became the foundation for modern language models.",
  author: "Abstract Algorithms",
  tags: ["transformers", "attention-mechanisms", "neural-networks", "nlp", "deep-learning", "architecture"],
  coverImage: "./assets/part-3.png",
  series: {
    name: "GenAI Mastery",
    order: 3,
    total: 12,
    prev: "/posts/genai-mastery-series/part-2",
    next: "/posts/genai-mastery-series/part-4"
  }
}

# Transformer Architecture & Attention Mechanisms

The transformer architecture revolutionized AI and became the foundation for modern language models like GPT, BERT, and ChatGPT. Understanding transformers is essential for mastering generative AI and building sophisticated NLP applications.

![Transformer Architecture Overview](../assets/transformer-overview.png)
*The transformer: The architecture that changed everything*

## üéØ Learning Objectives

By the end of this part, you will:
- Understand the revolutionary attention mechanism
- Master transformer architecture components
- Implement attention from scratch
- Compare different transformer variants
- Apply transformers to real-world problems

## üìö Table of Contents

1. [The Attention Revolution](#attention-revolution)
2. [Self-Attention Mechanism](#self-attention)
3. [Multi-Head Attention](#multi-head)
4. [Transformer Architecture](#transformer-arch)
5. [Positional Encoding](#positional-encoding)
6. [Training Transformers](#training)
7. [Transformer Variants](#variants)
8. [Implementation Guide](#implementation)
9. [Practical Applications](#applications)
10. [Hands-on Project](#project)

---

## üöÄ The Attention Revolution

Before transformers, sequential models like RNNs and LSTMs dominated NLP. The attention mechanism changed everything by allowing models to focus on relevant parts of the input simultaneously.

![Pre-Transformer vs Transformer](../assets/pre-transformer-comparison.png)
*Evolution from sequential processing to parallel attention*

### Problems with Sequential Models

**üêå Sequential Processing**: Had to process tokens one by one
**üìè Long-Range Dependencies**: Struggled with distant relationships
**üîÑ Vanishing Gradients**: Information degraded over long sequences
**‚ö° Slow Training**: Couldn't parallelize effectively

### The Attention Solution

**üëÅÔ∏è Global View**: Can attend to any position in the sequence
**‚ö° Parallelization**: All positions processed simultaneously
**üéØ Selective Focus**: Learns what to pay attention to
**üìà Scalability**: Efficient for long sequences

---

## üîç Self-Attention Mechanism

Self-attention allows each position in a sequence to attend to all positions, creating rich contextual representations.

![Self-Attention Visualization](../assets/self-attention.png)
*How self-attention connects every word to every other word*

### The Attention Formula

The core of attention is surprisingly simple:

```
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V
```

Where:
- **Q (Queries)**: What we're looking for
- **K (Keys)**: What we're comparing against  
- **V (Values)**: What we want to retrieve

### Step-by-Step Attention

```python
import torch
import torch.nn.functional as F
import math

def attention(query, key, value, mask=None):
    """
    Compute scaled dot-product attention
    
    Args:
        query: [batch, seq_len, d_model]
        key: [batch, seq_len, d_model]  
        value: [batch, seq_len, d_model]
        mask: Optional attention mask
    """
    d_k = query.size(-1)
    
    # Step 1: Compute attention scores
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    
    # Step 2: Apply mask (if provided)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Step 3: Apply softmax to get attention weights
    attention_weights = F.softmax(scores, dim=-1)
    
    # Step 4: Apply weights to values
    output = torch.matmul(attention_weights, value)
    
    return output, attention_weights

# Example usage
seq_len, d_model = 10, 64
query = torch.randn(1, seq_len, d_model)
key = torch.randn(1, seq_len, d_model)
value = torch.randn(1, seq_len, d_model)

output, weights = attention(query, key, value)
print(f"Output shape: {output.shape}")  # [1, 10, 64]
print(f"Attention weights shape: {weights.shape}")  # [1, 10, 10]
```

### Understanding Attention Weights

```python
def visualize_attention(sentence, attention_weights):
    """
    Visualize attention weights for a sentence
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    words = sentence.split()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attention_weights.squeeze().detach().numpy(),
        xticklabels=words,
        yticklabels=words,
        cmap='Blues',
        annot=True,
        fmt='.2f'
    )
    plt.title('Attention Weights Visualization')
    plt.xlabel('Keys (attending to)')
    plt.ylabel('Queries (attending from)')
    plt.show()

# Example
sentence = "The cat sat on the mat"
# attention_weights would come from your model
# visualize_attention(sentence, attention_weights)
```

---

## üß† Multi-Head Attention

Multi-head attention allows the model to attend to different types of relationships simultaneously - like having multiple "attention experts."

![Multi-Head Attention](../assets/multi-head-attention.png)
*Multiple attention heads capturing different relationships*

### Why Multiple Heads?

**üéØ Different Relationships**: Each head can focus on different aspects
**üìä Richer Representations**: Combined heads create deeper understanding
**üîÑ Redundancy**: Provides robustness and multiple perspectives

### Implementation

```python
class MultiHeadAttention(torch.nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Linear layers for Q, K, V
        self.w_q = torch.nn.Linear(d_model, d_model)
        self.w_k = torch.nn.Linear(d_model, d_model)
        self.w_v = torch.nn.Linear(d_model, d_model)
        self.w_o = torch.nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 1. Linear transformations and split into heads
        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # 2. Apply attention to each head
        attention_output, attention_weights = self.scaled_dot_product_attention(
            Q, K, V, mask
        )
        
        # 3. Concatenate heads
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # 4. Final linear transformation
        output = self.w_o(attention_output)
        
        return output, attention_weights
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights

# Usage example
d_model, num_heads = 512, 8
seq_len = 20
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)

output, weights = mha(x, x, x)  # Self-attention
print(f"Output shape: {output.shape}")  # [2, 20, 512]
```

---

## üèóÔ∏è Transformer Architecture

The complete transformer combines attention with other essential components to create a powerful sequence-to-sequence model.

![Complete Transformer](../assets/complete-transformer.png)
*Full transformer architecture with all components*

### Key Components

**1. Multi-Head Attention**: The core mechanism
**2. Feed-Forward Networks**: Non-linear transformations
**3. Layer Normalization**: Stabilizes training
**4. Residual Connections**: Enables deep networks
**5. Positional Encoding**: Provides sequence order information

### Transformer Block

```python
class TransformerBlock(torch.nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Multi-head attention
        self.attention = MultiHeadAttention(d_model, num_heads)
        
        # Feed-forward network
        self.feed_forward = torch.nn.Sequential(
            torch.nn.Linear(d_model, d_ff),
            torch.nn.ReLU(),
            torch.nn.Linear(d_ff, d_model)
        )
        
        # Layer normalization
        self.ln1 = torch.nn.LayerNorm(d_model)
        self.ln2 = torch.nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = torch.nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Multi-head attention with residual connection
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.ln1(x + self.dropout(attn_output))
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.ln2(x + self.dropout(ff_output))
        
        return x

# Stack multiple transformer blocks
class Transformer(torch.nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_length):
        super().__init__()
        
        self.d_model = d_model
        self.embedding = torch.nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_length)
        
        self.transformer_blocks = torch.nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])
        
        self.ln_f = torch.nn.LayerNorm(d_model)
        self.head = torch.nn.Linear(d_model, vocab_size)
        
    def forward(self, x, mask=None):
        # Token embeddings + positional encoding
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)
        
        # Pass through transformer blocks
        for block in self.transformer_blocks:
            x = block(x, mask)
        
        # Final layer norm
        x = self.ln_f(x)
        
        # Output projection
        logits = self.head(x)
        
        return logits
```

---

## üìç Positional Encoding

Since attention has no inherent notion of sequence order, we need to inject positional information.

![Positional Encoding](../assets/positional-encoding.png)
*How positional encoding provides sequence order information*

### Sinusoidal Positional Encoding

```python
class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, max_length=5000):
        super().__init__()
        
        pe = torch.zeros(max_length, d_model)
        position = torch.arange(0, max_length).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

# Visualize positional encodings
def plot_positional_encoding(d_model=128, max_length=100):
    import matplotlib.pyplot as plt
    
    pe = PositionalEncoding(d_model, max_length)
    encoding = pe.pe.squeeze().numpy()
    
    plt.figure(figsize=(15, 5))
    plt.imshow(encoding[:max_length, :d_model].T, cmap='RdYlBu', aspect='auto')
    plt.xlabel('Position')
    plt.ylabel('Dimension')
    plt.title('Positional Encoding Visualization')
    plt.colorbar()
    plt.show()
```

### Learned vs Fixed Positional Encodings

```python
# Learned positional embeddings (like GPT)
class LearnedPositionalEmbedding(torch.nn.Module):
    def __init__(self, max_length, d_model):
        super().__init__()
        self.embedding = torch.nn.Embedding(max_length, d_model)
        
    def forward(self, x):
        seq_len = x.size(1)
        positions = torch.arange(seq_len, device=x.device)
        return x + self.embedding(positions)

# Relative positional encoding (like T5)
class RelativePositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, max_relative_position=128):
        super().__init__()
        self.max_relative_position = max_relative_position
        self.embedding = torch.nn.Embedding(
            2 * max_relative_position + 1, d_model
        )
        
    def forward(self, seq_len):
        # Create relative position matrix
        positions = torch.arange(seq_len)
        relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)
        
        # Clip to max range
        relative_positions = torch.clamp(
            relative_positions,
            -self.max_relative_position,
            self.max_relative_position
        )
        
        # Shift to make all positive
        relative_positions += self.max_relative_position
        
        return self.embedding(relative_positions)
```

---

## üß™ Knowledge Check

Test your transformer understanding:

### Question 1: Attention Mechanism
What is the main advantage of attention over recurrent processing?
- A) Smaller model size
- B) Parallel processing and long-range dependencies ‚úÖ
- C) Lower computational complexity
- D) Better gradient flow

**Explanation**: Attention allows parallel processing of all positions and directly connects distant positions, solving key limitations of RNNs.

### Question 2: Multi-Head Attention
Why do we use multiple attention heads instead of just one?
- A) To increase model parameters
- B) To capture different types of relationships ‚úÖ
- C) To reduce overfitting
- D) To speed up training

**Explanation**: Multiple heads allow the model to attend to different types of relationships and information simultaneously.

### Question 3: Positional Encoding
Why is positional encoding necessary in transformers?
- A) To reduce computational cost
- B) Attention mechanism has no inherent sense of order ‚úÖ
- C) To improve gradient flow
- D) To prevent overfitting

**Explanation**: Since attention treats all positions equally, we need to explicitly provide position information.

---

## üéØ Hands-on Project

**Project**: Build a Mini-GPT from Scratch

**Objective**: Implement a simplified version of GPT using transformer architecture for text generation.

### Implementation Steps

**Step 1: Data Preparation**
```python
import torch
import torch.nn.functional as F

class TextDataset:
    def __init__(self, text, block_size=128):
        chars = sorted(list(set(text)))
        self.block_size = block_size
        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}
        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}
        self.vocab_size = len(chars)
        
        # Encode text
        self.data = torch.tensor([self.char_to_idx[ch] for ch in text])
        
    def __len__(self):
        return len(self.data) - self.block_size
    
    def __getitem__(self, idx):
        x = self.data[idx:idx + self.block_size]
        y = self.data[idx + 1:idx + self.block_size + 1]
        return x, y
```

**Step 2: Mini-GPT Model**
```python
class MiniGPT(torch.nn.Module):
    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=6, block_size=128):
        super().__init__()
        
        self.block_size = block_size
        self.token_embedding = torch.nn.Embedding(vocab_size, d_model)
        self.position_embedding = torch.nn.Embedding(block_size, d_model)
        
        self.transformer_blocks = torch.nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_model * 4)
            for _ in range(num_layers)
        ])
        
        self.ln_f = torch.nn.LayerNorm(d_model)
        self.head = torch.nn.Linear(d_model, vocab_size)
        
    def forward(self, x):
        B, T = x.shape
        
        # Token and position embeddings
        tok_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(torch.arange(T, device=x.device))
        x = tok_emb + pos_emb
        
        # Transformer blocks
        for block in self.transformer_blocks:
            x = block(x, mask=self.create_causal_mask(T, x.device))
        
        # Output
        x = self.ln_f(x)
        logits = self.head(x)
        
        return logits
    
    def create_causal_mask(self, size, device):
        # Lower triangular mask for causal attention
        mask = torch.tril(torch.ones(size, size, device=device))
        return mask.unsqueeze(0).unsqueeze(0)
    
    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0):
        for _ in range(max_new_tokens):
            # Crop to block size
            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]
            
            # Forward pass
            logits = self(idx_cond)
            logits = logits[:, -1, :] / temperature
            
            # Sample from distribution
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            
            # Append to sequence
            idx = torch.cat((idx, idx_next), dim=1)
            
        return idx
```

**Step 3: Training Loop**
```python
def train_mini_gpt(model, dataset, epochs=100, lr=3e-4):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        
        for batch_idx, (x, y) in enumerate(dataloader):
            optimizer.zero_grad()
            
            logits = model(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if epoch % 10 == 0:
            avg_loss = total_loss / len(dataloader)
            print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')
            
            # Generate sample text
            sample = generate_sample(model, dataset, length=100)
            print(f'Sample: {sample}\n')

def generate_sample(model, dataset, length=100):
    model.eval()
    with torch.no_grad():
        # Start with a random character
        start_idx = torch.randint(0, dataset.vocab_size, (1, 1))
        generated = model.generate(start_idx, length)
        
        # Decode to text
        text = ''.join([dataset.idx_to_char[i.item()] for i in generated[0]])
        return text
```

### Expected Results
- ‚úÖ Model learns to generate coherent text
- ‚úÖ Understanding of transformer components
- ‚úÖ Hands-on experience with attention mechanisms
- ‚úÖ Foundation for understanding larger models like GPT

---

## üéâ Summary

Congratulations! You've mastered transformer architecture, the foundation of modern AI. You now understand:

‚úÖ **Attention Mechanisms** and their revolutionary impact
‚úÖ **Self-Attention** and multi-head attention
‚úÖ **Complete Transformer Architecture** and its components  
‚úÖ **Positional Encoding** strategies
‚úÖ **Implementation Details** from scratch
‚úÖ **Real-World Applications** and variants

### Key Takeaways

1. **Attention is All You Need**: The core insight that revolutionized AI
2. **Parallelization**: Transformers can process sequences in parallel
3. **Scalability**: Architecture scales well with more data and compute
4. **Flexibility**: Adaptable to many different tasks and domains
5. **Foundation**: Basis for GPT, BERT, and most modern AI systems

### What's Next?

In **Part 4**, we'll explore **Large Language Models** like GPT and BERT, seeing how transformer architecture scales to create powerful AI systems.

---

## üìñ Series Navigation

**Previous**: [Part 2: ML & Deep Learning Foundations ‚Üê](/posts/genai-mastery-series/part-2)
**Next**: [Part 4: Large Language Models ‚Üí](/posts/genai-mastery-series/part-4)

**Jump to**:
- [Part 7: RAG Systems ‚Üí](/posts/genai-mastery-series/part-7)
- [Part 9: Agentic AI ‚Üí](/posts/genai-mastery-series/part-9)

---

*Ready to see how transformers power the largest AI models? Large Language Models await in Part 4!*
